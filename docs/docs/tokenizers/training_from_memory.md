# ä»è®°å¿†ä¸­è®­ç»ƒ

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/tokenizers/training_from_memory>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/tokenizers/training_from_memory>


åœ¨é‡Œé¢
 [å¿«é€Ÿæ¸¸è§ˆ](å¿«é€Ÿæ¸¸è§ˆ)
 ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•æ„å»ºå’Œè®­ç»ƒ
tokenizer ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶ï¼Œä½†æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥ä½¿ç”¨ä»»ä½• Python è¿­ä»£å™¨ã€‚
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å‡ ç§ä¸åŒçš„è®­ç»ƒæ–¹æ³•
åˆ†è¯å™¨ã€‚


å¯¹äºä¸‹é¢åˆ—å‡ºçš„æ‰€æœ‰ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„
 [Tokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/tokenizer#tokenizers.Tokenizer)
 å’Œ
 ã€Œæ•™ç»ƒã€
 ï¼Œæ„å»ºä¸º
ä¸‹åˆ—çš„ï¼š



```
from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers
tokenizer = Tokenizer(models.Unigram())
tokenizer.normalizer = normalizers.NFKC()
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
tokenizer.decoder = decoders.ByteLevel()
trainer = trainers.UnigramTrainer(
    vocab_size=20000,
    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
    special_tokens=["<PAD>", "<BOS>", "<EOS>"],
)
```


è¯¥åˆ†è¯å™¨åŸºäº
 [Unigram](/docs/tokenizers/v0.13.4.rc2/en/api/models#tokenizers.models.Unigram)
 æ¨¡å‹ã€‚å®ƒ
è´Ÿè´£ä½¿ç”¨ NFKC Unicode æ ‡å‡†åŒ–å¯¹è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–
æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨
 [ByteLevel](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)
 é¢„åˆ†è¯å™¨å’Œç›¸åº”çš„è§£ç å™¨ã€‚


æœ‰å…³æ­¤å¤„ä½¿ç”¨çš„ç»„ä»¶çš„æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹
 [æ­¤å¤„]ï¼ˆç»„ä»¶ï¼‰
 ã€‚


## æœ€åŸºæœ¬çš„æ–¹æ³•



æ­£å¦‚æ‚¨å¯èƒ½å·²ç»çŒœåˆ°çš„ï¼Œè®­ç»ƒæˆ‘ä»¬çš„åˆ†è¯å™¨çš„æœ€ç®€å•æ–¹æ³•
æ˜¯é€šè¿‡ä½¿ç”¨
 `åˆ—è¡¨`
 {.è§£é‡Šæ–‡æœ¬è§’è‰²=â€œobjâ€}ï¼š



```
# First few lines of the "Zen of Python" https://www.python.org/dev/peps/pep-0020/
data = [
    "Beautiful is better than ugly."
    "Explicit is better than implicit."
    "Simple is better than complex."
    "Complex is better than complicated."
    "Flat is better than nested."
    "Sparse is better than dense."
    "Readability counts."
]
tokenizer.train_from_iterator(data, trainer=trainer)
```


å®¹æ˜“ï¼Œå¯¹å§ï¼Ÿæ‚¨å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ä»»ä½•ä½œä¸ºè¿­ä»£å™¨å·¥ä½œçš„ä¸œè¥¿ï¼Œæ— è®ºæ˜¯
 `åˆ—è¡¨`
 {.è§£é‡Šæ–‡æœ¬è§’è‰²=â€œobjâ€}ï¼Œ
 `å…ƒç»„`
 {.è§£é‡Šæ–‡æœ¬
è§’è‰²=â€œobjâ€}ï¼Œæˆ–
 `np.Array`
 {.è§£é‡Šæ–‡æœ¬è§’è‰²=â€œobjâ€}ã€‚ä»»ä½•äº‹ç‰©
åªè¦å®ƒæä¾›å­—ç¬¦ä¸²å°±å¯ä»¥å·¥ä½œã€‚


## ä½¿ç”¨ ğŸ¤— æ•°æ®é›†åº“



è®¿é—®ç°æœ‰ä¼—å¤šæ•°æ®é›†ä¹‹ä¸€çš„ç»ä½³æ–¹æ³•
æ˜¯é€šè¿‡ä½¿ç”¨ ğŸ¤— æ•°æ®é›†åº“ã€‚æœ‰å…³å®ƒçš„æ›´å¤šä¿¡æ¯ï¼Œæ‚¨
åº”è¯¥æ£€æŸ¥
 [å®˜æ–¹æ–‡æ¡£
è¿™é‡Œ](https://huggingface.co/docs/datasets/)
 ã€‚


è®©æˆ‘ä»¬ä»åŠ è½½æ•°æ®é›†å¼€å§‹ï¼š



```
import datasets
dataset = datasets.load_dataset("wikitext", "wikitext-103-raw-v1", split="train+test+validation")
```


ä¸‹ä¸€æ­¥æ˜¯åœ¨æ­¤æ•°æ®é›†ä¸Šæ„å»ºè¿­ä»£å™¨ã€‚æœ€ç®€å•çš„æ–¹æ³•
ä¸ºæ­¤ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ç”Ÿæˆå™¨ï¼š



```
def batch\_iterator(batch\_size=1000):
    for i in range(0, len(dataset), batch_size):
        yield dataset[i : i + batch_size]["text"]
```


æ­£å¦‚æ‚¨åœ¨è¿™é‡Œæ‰€çœ‹åˆ°çš„ï¼Œä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥æä¾›
ä¸€æ‰¹ç”¨äºè®­ç»ƒçš„ç¤ºä¾‹ï¼Œè€Œä¸æ˜¯é€ä¸€è¿­ä»£å®ƒä»¬
ä¸€ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥é¢„æœŸæ€§èƒ½ä¸æˆ‘ä»¬çš„æ€§èƒ½éå¸¸ç›¸ä¼¼
ç›´æ¥ä»æ–‡ä»¶è®­ç»ƒæ—¶è·å¾—ã€‚


è¿­ä»£å™¨å‡†å¤‡å°±ç»ªåï¼Œæˆ‘ä»¬åªéœ€è¦å¯åŠ¨è®­ç»ƒå³å¯ã€‚ä¸ºäº†
ä¸ºäº†æ”¹å–„è¿›åº¦æ¡çš„å¤–è§‚ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ€»è®¡
æ•°æ®é›†é•¿åº¦ï¼š



```
tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))
```


å°±æ˜¯è¿™æ ·ï¼


## ä½¿ç”¨ gzip æ–‡ä»¶



ç”±äºPythonä¸­çš„gzipæ–‡ä»¶å¯ä»¥ç”¨ä½œè¿­ä»£å™¨ï¼Œå› æ­¤éå¸¸æ–¹ä¾¿
å¯¹æ­¤ç±»æ–‡ä»¶è¿›è¡Œç®€å•çš„è®­ç»ƒï¼š



```
import gzip
with gzip.open("data/my-file.0.gz", "rt") as f:
    tokenizer.train_from_iterator(f, trainer=trainer)
```


ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ä»å¤šä¸ª gzip æ–‡ä»¶è¿›è¡Œè®­ç»ƒï¼Œé‚£å°±ä¸ä¼šå¤ªå¤šäº†
æ›´éš¾ï¼š



```
files = ["data/my-file.0.gz", "data/my-file.1.gz", "data/my-file.2.gz"]
def gzip\_iterator():
    for path in files:
        with gzip.open(path, "rt") as f:
            for line in f:
                yield line
tokenizer.train_from_iterator(gzip_iterator(), trainer=trainer)
```


ç°åœ¨ä½ å°±æ‹¥æœ‰äº†ï¼