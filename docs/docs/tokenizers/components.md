# 成分

> 译者：[片刻小哥哥](https://github.com/jiangzhonglian)
>
> 项目地址：<https://huggingface.apachecn.org/docs/tokenizers/components>
>
> 原始地址：<https://huggingface.co/docs/tokenizers/components>


构建 Tokenizer 时，您可以将各种类型的组件附加到
此 Tokenizer 以便自定义其行为。此页列出了大多数
提供的组件。


## 标准化器



A
 `标准化器`
 负责按顺序预处理输入字符串
将其标准化为与给定用例相关。一些常见的例子
标准化的算法是 Unicode 标准化算法（NFD、NFKD、
NFC 和 NFKC）、小写等……的特殊性
 `标记器`
 就是它
我们在标准化时跟踪对齐情况。这对于
允许从生成的标记映射回输入文本。


这
 `标准化器`
 是可选的。


|
 姓名
  |
 描述
  |
 例子
  |
| --- | --- | --- |
|
 NFD
  |
 NFD unicode 标准化
  | |
|
 NFKD
  |
 NFKD unicode 标准化
  | |
|
 近场通信
  |
 NFC unicode 标准化
  | |
|
 NFKC
  |
 NFKC unicode 标准化
  | |
|
 小写
  |
 将所有大写字母替换为小写字母
  |
 输入：
 `你好ὈΔYΣΣΕΎΣ`

 输出：
 `你好`
 ὀδυσσεύς`
  |
|
 条
  |
 删除输入指定侧（左侧、右侧或两侧）的所有空白字符
  |
 输入：
 ``````
 你好
 ``````

 输出：
 `“嗨”` |
|
 脱衣口音
  |
 删除 unicode 中的所有重音符号（与 NFD 一起使用以保持一致性）
  |
 输入：
 'é'

 输出：
 `e` |
|
 代替
  |
 替换自定义字符串或正则表达式并用给定内容更改它
  | `替换（“a”，“e”）`
 将会像这样：
 
 输入：
 `“香蕉”`

 输出：
 `“贝内”` |
|
 伯特标准化器
  |
 提供原始 BERT 中使用的 Normalizer 的实现。可以设置的选项有：
 * 干净\_文本
* 句柄\_chinese\_chars
* 去掉\_重音
* 小写
 | |
|
 顺序
  |
 组成多个标准化器，这些标准化器将按提供的顺序运行
  | `序列（[NFKC（），小写（）]）` |


## 预分词器



这
 `预分词器`
 负责根据一组分割输入
规则。此预处理可让您确保底层
 `模型`
 不会跨多个“分裂”构建代币。例如如果
如果您不想在令牌内有空格，那么您可以使用
 `预分词器`
 在这些空白处分裂。


您可以轻松组合多个
 `预分词器`
 一起使用
 `序列`
 （见下文）。这
 `预分词器`
 也允许修改
字符串，就像一个
 `标准化器`
 做。这是必要的，以允许一些
需要在标准化之前进行分割的复杂算法（例如
字节级）


|
 姓名
  |
 描述
  |
 例子
  |
| --- | --- | --- |
|
 字节级
  |
 在将所有字节重新映射到一组可见字符的同时按空格进行拆分。这项技术由 OpenAI 与 GPT-2 一起引入，并具有一些或多或少的不错的特性：
 * 由于它映射到字节，因此使用它的分词器只需要
 **256**
 字符作为初始字母表（一个字节可以拥有的值的数量），而不是 130,000 多个 Unicode 字符。
* 上一点的结果是，绝对没有必要使用这个未知的令牌，因为我们可以用 256 个令牌来表示任何东西（Youhou！！🎉🎉）
* 对于非 ASCII 字符，它完全不可读，但它仍然有效！
 |
 输入：
 ``“你好，我的朋友，你好吗？”

 输出：
 `“你好”，“我的”，“朋友”，“，”，“如何”，“是”，“你”，“？”` |
|
 空白
  |
 在单词边界上分割（使用以下正则表达式：
 `\w+|[^\w\s]+` |
 输入：
 ``“你好！”

 输出：
 `“你好”，“那里”，“！”` |
|
 空白分割
  |
 在任何空白字符上分割
  |
 输入：
 ``“你好！”

 输出：
 `“你好”，“那里！”` |
|
 标点
  |
 将隔离所有标点字符
  |
 输入：
 「喂？」

 输出：
 `“你好”，“？”` |
|
 元空间
  |
 按空格分割并用特殊字符“ ” (U+2581) 替换它们
  |
 输入：
 ``“你好”`

 输出：
 `“你好”，“那里”` |
|
 字符分隔符分割
  |
 根据给定的字符进行分割
  |
 示例为
 `x`
 :
 
 输入：
 `“你好”`

 输出：
 `“你好”，“那里”` |
|
 数字
  |
 将数字与任何其他字符分开。
  |
 输入：
 `“你好123那里”`

 输出：
 `“你好”，“123”，“那里”` |
|
 分裂
  |
 多功能预标记器，可根据提供的模式和提供的行为进行分割。如有必要，可以反转该图案。
 * 模式应该是自定义字符串或正则表达式。
* 行为应该是以下之一：
+ 删除
+ 隔离
+ 与\_合并\_上一个
+ 与\_next 合并
+ 连续的
* invert 应该是一个布尔标志。
 |
 模式示例 =
 
 , 行为 =
 `“孤立”`
 , 反转 =
 ‘假’
 :
 
 输入：
 ``“你好，你好吗？”

 输出：
 `“你好，”，“”，“怎么样”，“”，“是”，“”，“你？”` |
|
 顺序
  |
 让您可以创作多个
 `预分词器`
 将按照给定的顺序运行
  | `序列（[标点符号（），WhitespaceSplit（）]）` |


## 楷模



模型是用于实际标记化的核心算法，因此，
它们是 Tokenizer 的唯一必需组件。


|
 姓名
  |
 描述
  |
| --- | --- |
|
 字级
  |
 这是“经典”标记化算法。它让您可以简单地将单词映射到 ID，无需任何花哨的操作。其优点是非常易于使用和理解，但它需要非常大的词汇量才能获得良好的覆盖范围。使用这个
 `模型`
 需要使用一个
 `预分词器`
 。该模型不会直接做出选择，它只是将输入标记映射到 ID。
  |
|
 BPE
  |
 最流行的子词标记化算法之一。字节对编码的工作原理是从字符开始，同时将最常见的字符合并在一起，从而创建新的标记。然后，它迭代地从语料库中看到的最常见的对构建新的标记。 BPE 能够通过使用多个子词标记来构建它从未见过的单词，因此需要较小的词汇表，并且出现“unk”（未知）标记的可能性较小。
  |
|
 词片
  |
 这是一种与 BPE 非常相似的子词标记化算法，主要由 Google 在 BERT 等模型中使用。它使用贪婪算法，首先尝试构建长单词，当词汇表中不存在整个单词时，将其拆分为多个标记。这与 BPE 不同，BPE 从字符开始，构建尽可能大的代币。它使用了著名的
 `##`
 前缀来识别属于单词一部分的标记（即不是单词的开头）。
  |
|
 一元语法
  |
 Unigram 也是一种子词标记化算法，其工作原理是尝试识别最佳的子词标记集，以最大化给定句子的概率。这与 BPE 的不同之处在于，BPE 不是基于一组顺序应用的规则来确定的。相反，Unigram 将能够计算多种标记化方式，同时选择最可能的一种。
  |


## 后处理器



在整个管道之后，我们有时想插入一些特殊的
标记之前将标记化字符串输入模型，如“[CLS] My
马太棒了[九月]”。这
 `后处理器`
 组件正在做什么
只是。


|
 姓名
  |
 描述
  |
 例子
  |
| --- | --- | --- |
|
 模板处理
  |
 让您轻松模板化后处理、添加特殊标记并指定
 `类型 ID`
 对于每个序列/特殊标记。该模板被赋予两个表示单个序列和一对序列的字符串，以及一组要使用的特殊标记。
  |
 例如，当使用这些值指定模板时：
 * 单身的：
 `“[CLS] $A [九月]”`
* 一对：
 `“[CLS] $A [九月] $B [九月]”`
* 特殊代币：
+ `“[CLS]”`
+ `“[九月]”`


 输入：
 `(“我喜欢这个”，“但不喜欢这个”)`

 输出：
 `“[CLS] 我喜欢这个 [SEP]，但不喜欢这个 [SEP]”` |


## 解码器



解码器知道如何从 Tokenizer 使用的 ID 回到
一段可读的文字。一些
 `标准化器`
 和
 `预分词器`
 使用
例如，需要恢复的特殊字符或标识符。


|
 姓名
  |
 描述
  |
| --- | --- |
|
 字节级
  |
 恢复 ByteLevel PreTokenizer。这个 PreTokenizer 在字节级别进行编码，使用一组可见的 Unicode 字符来表示每个字节，因此我们需要一个解码器来恢复此过程并再次获得可读的内容。
  |
|
 元空间
  |
 恢复元空间预标记器。这个 PreTokenizer 使用特殊的标识符
 ``
 识别空格，因此该解码器有助于解码这些空格。
  |
|
 词片
  |
 恢复 WordPiece 模型。该模型使用特殊标识符
 `##`
 对于连续的子词，因此这个解码器有助于解码这些子词。
  |



