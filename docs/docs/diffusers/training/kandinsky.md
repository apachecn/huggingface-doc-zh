# [](#kandinsky-22)Kandinsky 2.2

> è¯‘è€…ï¼š[ç–¾é£å…”X](https://github.com/jifnegtu)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/diffusers/training/kandinsky>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/diffusers/training/kandinsky>


>è¿™ä¸ªè„šæœ¬æ˜¯å®éªŒæ€§çš„ï¼Œå¾ˆå®¹æ˜“è¿‡åº¦æ‹Ÿåˆå¹¶é‡åˆ°ç¾éš¾æ€§é—å¿˜ç­‰é—®é¢˜ã€‚å°è¯•æ¢ç´¢ä¸åŒçš„è¶…å‚æ•°ï¼Œä»¥è·å¾—æ•°æ®é›†çš„æœ€ä½³ç»“æœã€‚

Kandinsky 2.2 æ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ–‡ç”Ÿå›¾ï¼ˆtext-to-imageï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é€¼çœŸçš„å›¾åƒã€‚è¯¥æ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªå›¾åƒå…ˆéªŒæ¨¡å‹ï¼ˆimage prior modelï¼‰ç”¨äºä»æ–‡æœ¬æç¤ºåˆ›å»ºå›¾åƒåµŒå…¥ï¼Œä»¥åŠä¸€ä¸ªè§£ç å™¨æ¨¡å‹ï¼ˆdecoder modelï¼‰ï¼Œè¯¥æ¨¡å‹åŸºäºå…ˆéªŒæ¨¡å‹çš„åµŒå…¥æ¥ç”Ÿæˆå›¾åƒã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨Diffusersä¸­ä½ ä¼šå‘ç°ç”¨äºè®­ç»ƒKandinsky 2.2çš„ä¸¤ä¸ªç‹¬ç«‹çš„è„šæœ¬ï¼Œä¸€ä¸ªç”¨äºè®­ç»ƒå…ˆéªŒæ¨¡å‹ï¼Œå¦ä¸€ä¸ªç”¨äºè®­ç»ƒè§£ç å™¨æ¨¡å‹ã€‚æ‚¨å¯ä»¥åˆ†åˆ«è®­ç»ƒè¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œä½†è¦è·å¾—æœ€ä½³ç»“æœï¼Œæ‚¨åº”è¯¥åŒæ—¶è®­ç»ƒå…ˆå‰æ¨¡å‹å’Œè§£ç å™¨æ¨¡å‹ã€‚

æ ¹æ®æ‚¨çš„ GPUï¼Œæ‚¨å¯èƒ½éœ€è¦å¯ç”¨ `gradient_checkpointing`ï¼ˆâš ï¸ä»¥å‰çš„æ¨¡å‹ä¸æ”¯æŒï¼ï¼‰ï¼Œ`mixed_precision` å’Œ `gradient_accumulation_steps` æ¥å¸®åŠ©å°†æ¨¡å‹æ”¾å…¥å†…å­˜å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨ [xFormers](../optimization/xformers) å¯ç”¨å†…å­˜æ•ˆç‡é«˜çš„æ³¨æ„åŠ›æ¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨é‡ï¼ˆç‰ˆæœ¬ [v0.0.16](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212) åœ¨æŸäº› GPU ä¸Šæ— æ³•è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤æ‚¨å¯èƒ½éœ€è¦å®‰è£…å¼€å‘ç‰ˆæœ¬ï¼‰ã€‚

æœ¬æŒ‡å—æ¢è®¨äº†[train\_text\_to\_image\_prior.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py)å’Œ[train\_text\_to\_image\_decoder.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_decoder.py)è„šæœ¬ï¼Œä»¥å¸®åŠ©æ‚¨æ›´åŠ ç†Ÿæ‚‰å®ƒï¼Œä»¥åŠå¦‚ä½•æ ¹æ®è‡ªå·±çš„ç”¨ä¾‹è°ƒæ•´å®ƒã€‚

åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä»æºä»£ç å®‰è£…åº“ï¼š

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```
ç„¶åå¯¼èˆªåˆ°åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ï¼Œå¹¶å®‰è£…æ­£åœ¨ä½¿ç”¨çš„è„šæœ¬æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š

```py
cd examples/kandinsky2_2/text_to_image
pip install -r requirements.txt
```
>ğŸ¤— Accelerate æ˜¯ä¸€ä¸ªåº“ï¼Œå¯å¸®åŠ©æ‚¨åœ¨å¤šä¸ª GPU/TPU ä¸Šæˆ–ä»¥æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒã€‚å®ƒå°†æ ¹æ®æ‚¨çš„ç¡¬ä»¶å’Œç¯å¢ƒè‡ªåŠ¨é…ç½®æ‚¨çš„è®­ç»ƒè®¾ç½®ã€‚è¯·æŸ¥çœ‹ ğŸ¤— Accelerate [Quick æ•™ç¨‹](https://huggingface.co/docs/accelerate/quicktour)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚

åˆå§‹åŒ– ğŸ¤— Accelerate ç¯å¢ƒï¼š

```py
accelerate config
```
è¦åœ¨ä¸é€‰æ‹©ä»»ä½•é…ç½®çš„æƒ…å†µä¸‹è®¾ç½®é»˜è®¤ğŸ¤—çš„ Accelerate ç¯å¢ƒï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```py
accelerate config default
```
æˆ–è€…ï¼Œå¦‚æœæ‚¨çš„ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼ shellï¼ˆå¦‚ç¬”è®°æœ¬ï¼‰ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ï¼š

```py
from accelerate.utils import write_basic_config

write_basic_config()
```
æœ€åï¼Œå¦‚æœè¦åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†](create_dataset)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºé€‚ç”¨äºè®­ç»ƒè„šæœ¬çš„æ•°æ®é›†ã€‚

ä»¥ä¸‹éƒ¨åˆ†é‡ç‚¹ä»‹ç»äº†è®­ç»ƒè„šæœ¬ä¸­å¯¹äºäº†è§£å¦‚ä½•ä¿®æ”¹è®­ç»ƒè„šæœ¬å¾ˆé‡è¦çš„éƒ¨åˆ†ï¼Œä½†å¹¶æœªè¯¦ç»†ä»‹ç»è„šæœ¬çš„å„ä¸ªæ–¹é¢ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·éšæ—¶é˜…è¯»è„šæœ¬ï¼Œå¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ï¼Œè¯·å‘Šè¯‰æˆ‘ä»¬ã€‚

## [](#script-parameters)è„šæœ¬å‚æ•°ï¼ˆScript parametersï¼‰

è®­ç»ƒè„šæœ¬æä¾›äº†è®¸å¤šå‚æ•°æ¥å¸®åŠ©æ‚¨è‡ªå®šä¹‰è®­ç»ƒè¿è¡Œã€‚æ‰€æœ‰å‚æ•°åŠå…¶æè¿°éƒ½å¯ä»¥åœ¨ [`parse_argsï¼ˆï¼‰`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L190) å‡½æ•°ä¸­æ‰¾åˆ°ã€‚è®­ç»ƒè„šæœ¬ä¸ºæ¯ä¸ªå‚æ•°æä¾›é»˜è®¤å€¼ï¼Œä¾‹å¦‚è®­ç»ƒæ‰¹å¤§å°å’Œå­¦ä¹ ç‡ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦åœ¨è®­ç»ƒå‘½ä»¤ä¸­è®¾ç½®è‡ªå·±çš„å€¼ã€‚

ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨ fp16 æ ¼å¼ä»¥æ··åˆç²¾åº¦åŠ é€Ÿè®­ç»ƒï¼Œè¯·å°†å‚æ•°`--mixed_precision`æ·»åŠ åˆ°è®­ç»ƒå‘½ä»¤ä¸­ï¼š

```py
accelerate launch train_text_to_image_prior.py \
  --mixed_precision="fp16"
```
å¤§å¤šæ•°å‚æ•°ä¸[æ–‡ç”Ÿå›¾ï¼ˆText-to-imageï¼‰](text2image#script-parameters)è®­ç»ƒæŒ‡å—ä¸­çš„å‚æ•°ç›¸åŒï¼Œå› æ­¤è®©æˆ‘ä»¬ç›´æ¥è¿›å…¥Kandinskyè®­ç»ƒè„šæœ¬çš„æ¼”ç»ƒï¼

### [](#min-snr-weighting)æœ€å°ä¿¡å™ªæ¯”åŠ æƒï¼ˆMin-SNR weightingï¼‰

[Min-SNR](https://huggingface.co/papers/2303.09556) åŠ æƒç­–ç•¥å¯ä»¥é€šè¿‡é‡æ–°å¹³è¡¡æŸå¤±æ¥å®ç°æ›´å¿«çš„æ”¶æ•›æ¥å¸®åŠ©è®­ç»ƒã€‚è®­ç»ƒè„šæœ¬æ”¯æŒé¢„æµ‹ `epsilon`ï¼ˆå™ªå£°ï¼‰æˆ– `v_prediction` ï¼Œä½† Min-SNR ä¸è¿™ä¸¤ç§é¢„æµ‹ç±»å‹å…¼å®¹ã€‚æ­¤åŠ æƒç­–ç•¥ä»…å— PyTorch æ”¯æŒï¼Œåœ¨ Flax è®­ç»ƒè„šæœ¬ä¸­ä¸å¯ç”¨ã€‚

æ·»åŠ å‚æ•°`--snr_gamma`å¹¶å°†å…¶è®¾ç½®ä¸ºå»ºè®®çš„å€¼ 5.0ï¼š

```py
accelerate launch train_text_to_image_prior.py \
  --snr_gamma=5.0
```
## [](#training-script)è®­ç»ƒè„šæœ¬ï¼ˆTraining scriptï¼‰

è®­ç»ƒè„šæœ¬ä¹Ÿç±»ä¼¼äº[æ–‡ç”Ÿå›¾ï¼ˆText-to-imageï¼‰](text2image#training-script)è®­ç»ƒæŒ‡å—ï¼Œä½†å·²å¯¹å…¶è¿›è¡Œä¿®æ”¹ï¼Œä»¥æ”¯æŒè®­ç»ƒå…ˆå‰æ¨¡å‹å’Œè§£ç å™¨æ¨¡å‹ã€‚æœ¬æŒ‡å—é‡ç‚¹ä»‹ç» Kandinsky 2.2 è®­ç»ƒè„šæœ¬æ‰€ç‰¹æœ‰çš„ä»£ç ã€‚


### å…ˆéªŒæ¨¡å‹ï¼ˆprior modelï¼‰
[`mainï¼ˆï¼‰`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L441) å‡½æ•°åŒ…å«ç”¨äºå‡†å¤‡æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹çš„ä»£ç ã€‚

æ‚¨ä¼šç«‹å³æ³¨æ„åˆ°çš„ä¸»è¦åŒºåˆ«ä¹‹ä¸€æ˜¯ï¼Œè®­ç»ƒè„šæœ¬è¿˜åŠ è½½äº†ä¸€ä¸ª [CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/clip#transformers.CLIPImageProcessor)ï¼ˆé™¤äº†è°ƒåº¦ç¨‹åºå’Œåˆ†è¯å™¨ï¼‰ç”¨äºé¢„å¤„ç†å›¾åƒï¼Œä»¥åŠä¸€ä¸ªç”¨äºå¯¹å›¾åƒè¿›è¡Œç¼–ç çš„ [CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/clip#transformers.CLIPVisionModelWithProjection) æ¨¡å‹ï¼š

```py
noise_scheduler = DDPMScheduler(beta_schedule="squaredcos_cap_v2", prediction_type="sample")
image_processor = CLIPImageProcessor.from_pretrained(
    args.pretrained_prior_model_name_or_path, subfolder="image_processor"
)
tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder="tokenizer")

with ContextManagers(deepspeed_zero_init_disabled_context_manager()):
    image_encoder = CLIPVisionModelWithProjection.from_pretrained(
        args.pretrained_prior_model_name_or_path, subfolder="image_encoder", torch_dtype=weight_dtype
    ).eval()
    text_encoder = CLIPTextModelWithProjection.from_pretrained(
        args.pretrained_prior_model_name_or_path, subfolder="text_encoder", torch_dtype=weight_dtype
    ).eval()
```
Kandinsky ä½¿ç”¨ [PriorTransformer](/docs/diffusers/v0.27.2/en/api/models/prior_transformer#diffusers.PriorTransformer) æ¥ç”Ÿæˆå›¾åƒåµŒå…¥ï¼Œå› æ­¤æ‚¨éœ€è¦è®¾ç½®ä¼˜åŒ–å™¨æ¥äº†è§£å…ˆéªŒæ¨¡å¼çš„å‚æ•°ã€‚

```py
prior = PriorTransformer.from_pretrained(args.pretrained_prior_model_name_or_path, subfolder="prior")
prior.train()
optimizer = optimizer_cls(
    prior.parameters(),
    lr=args.learning_rate,
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```
æ¥ä¸‹æ¥ï¼Œå¯¹è¾“å…¥å­—å¹•è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶ç”± [CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/clip#transformers.CLIPImageProcessor) å¯¹å›¾åƒè¿›è¡Œ[é¢„å¤„ç†](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L632)ï¼š

```py
def preprocess_train(examples):
    images = [image.convert("RGB") for image in examples[image_column]]
    examples["clip_pixel_values"] = image_processor(images, return_tensors="pt").pixel_values
    examples["text_input_ids"], examples["text_mask"] = tokenize_captions(examples)
    return examples
```
æœ€åï¼Œ[è®­ç»ƒå¾ªç¯](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L718)å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºæ½œèƒ½å›¾åƒï¼Œå‘å›¾åƒåµŒå…¥æ·»åŠ å™ªå£°ï¼Œå¹¶åšå‡ºé¢„æµ‹ï¼š

```py
model_pred = prior(
    noisy_latents,
    timestep=timesteps,
    proj_embedding=prompt_embeds,
    encoder_hidden_states=text_encoder_hidden_states,
    attention_mask=text_mask,
).predicted_image_embedding
```
å¦‚æœæ‚¨æƒ³äº†è§£æœ‰å…³è®­ç»ƒå¾ªç¯å·¥ä½œåŸç†çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[äº†è§£ç®¡é“ã€æ¨¡å‹å’Œè°ƒåº¦ç¨‹åºï¼ˆ Understanding pipelines, models and schedulersï¼‰](../using-diffusers/write_own_pipeline)æ•™ç¨‹ï¼Œè¯¥æ•™ç¨‹åˆ†è§£äº†å»å™ªè¿‡ç¨‹çš„åŸºæœ¬æ¨¡å¼ã€‚

### è§£ç å™¨æ¨¡å‹ï¼ˆdecoder modelï¼‰
[`mainï¼ˆï¼‰`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_decoder.py#L440) å‡½æ•°åŒ…å«ç”¨äºå‡†å¤‡æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹çš„ä»£ç ã€‚

ä¸ä¹‹å‰çš„æ¨¡å‹ä¸åŒï¼Œè§£ç å™¨åˆå§‹åŒ– [VQModel](/docs/diffusers/v0.27.2/en/api/models/vq#diffusers.VQModel) ä»¥å°†æ½œä¼ç‰©è§£ç ä¸ºå›¾åƒï¼Œå¹¶ä½¿ç”¨ [UNet2DConditionModel](/docs/diffusers/v0.27.2/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)ï¼š

```py
with ContextManagers(deepspeed_zero_init_disabled_context_manager()):
    vae = VQModel.from_pretrained(
        args.pretrained_decoder_model_name_or_path, subfolder="movq", torch_dtype=weight_dtype
    ).eval()
    image_encoder = CLIPVisionModelWithProjection.from_pretrained(
        args.pretrained_prior_model_name_or_path, subfolder="image_encoder", torch_dtype=weight_dtype
    ).eval()
unet = UNet2DConditionModel.from_pretrained(args.pretrained_decoder_model_name_or_path, subfolder="unet")
```
æ¥ä¸‹æ¥ï¼Œè¯¥è„šæœ¬åŒ…æ‹¬å¤šä¸ªå›¾åƒè½¬æ¢å’Œä¸€ä¸ª[é¢„å¤„ç†](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_decoder.py#L622)å‡½æ•°ï¼Œç”¨äºå°†è½¬æ¢åº”ç”¨äºå›¾åƒå¹¶è¿”å›åƒç´ å€¼ï¼š

```py
def preprocess_train(examples):
    images = [image.convert("RGB") for image in examples[image_column]]
    examples["pixel_values"] = [train_transforms(image) for image in images]
    examples["clip_pixel_values"] = image_processor(images, return_tensors="pt").pixel_values
    return examples
```
æœ€åï¼Œ[è®­ç»ƒå¾ªç¯](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_decoder.py#L706)å¤„ç†å°†å›¾åƒè½¬æ¢ä¸ºæ½œä¼å›¾åƒã€æ·»åŠ å™ªå£°å’Œé¢„æµ‹å™ªå£°æ®‹å·®ã€‚

å¦‚æœæ‚¨æƒ³äº†è§£æœ‰å…³è®­ç»ƒå¾ªç¯å·¥ä½œåŸç†çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[äº†è§£ç®¡é“ã€æ¨¡å‹å’Œè°ƒåº¦ç¨‹åºï¼ˆUnderstanding pipelines, models and schedulersï¼‰](../using-diffusers/write_own_pipeline)æ•™ç¨‹ï¼Œè¯¥æ•™ç¨‹åˆ†è§£äº†å»å™ªè¿‡ç¨‹çš„åŸºæœ¬æ¨¡å¼ã€‚

```py
model_pred = unet(noisy_latents, timesteps, None, added_cond_kwargs=added_cond_kwargs).sample[:, :4]
```
## [](#launch-the-script)å¯åŠ¨è„šæœ¬ï¼ˆLaunch the scriptï¼‰

å®Œæˆæ‰€æœ‰æ›´æ”¹æˆ–å¯¹é»˜è®¤é…ç½®æ„Ÿåˆ°æ»¡æ„åï¼Œå°±å¯ä»¥å¯åŠ¨è®­ç»ƒè„šæœ¬äº†ï¼ğŸš€

ä½ å°†åœ¨ [Naruto BLIP captions ](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions)æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä»¥ç”Ÿæˆè‡ªå·±çš„ Naruto è§’è‰²ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥æŒ‰ç…§[åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†ï¼ˆ Create a dataset for trainingï¼‰](create_dataset)æŒ‡å—åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šåˆ›å»ºå’Œè®­ç»ƒã€‚å°†ç¯å¢ƒå˜é‡`DATASET_NAME`è®¾ç½®ä¸º Hub ä¸Šæ•°æ®é›†çš„åç§°ï¼Œæˆ–è€…ï¼Œå¦‚æœè¦å¯¹è‡ªå·±çš„æ–‡ä»¶è¿›è¡Œè®­ç»ƒï¼Œè¯·å°†ç¯å¢ƒå˜é‡`TRAIN_DIR`è®¾ç½®ä¸ºæ•°æ®é›†çš„è·¯å¾„ã€‚

å¦‚æœè¦åœ¨å¤šä¸ª GPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯·å°†å‚æ•°`--multi_gpu``accelerate launch`æ·»åŠ åˆ°å‘½ä»¤ä¸­ã€‚

è¦ä½¿ç”¨æƒé‡å’Œåå·®ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Œè¯·å°†å‚æ•°`--report_to=wandb`æ·»åŠ åˆ°è®­ç»ƒå‘½ä»¤ä¸­ã€‚æ‚¨è¿˜éœ€è¦å°† `--validation_prompt` æ·»åŠ åˆ°è®­ç»ƒå‘½ä»¤ä¸­ä»¥è·Ÿè¸ªç»“æœã€‚è¿™å¯¹äºè°ƒè¯•æ¨¡å‹å’ŒæŸ¥çœ‹ä¸­é—´ç»“æœéå¸¸æœ‰ç”¨ã€‚

### å…ˆéªŒæ¨¡å‹

```py
export DATASET_NAME="lambdalabs/naruto-blip-captions"

accelerate launch --mixed_precision="fp16"  train_text_to_image_prior.py \
  --dataset_name=$DATASET_NAME \
  --resolution=768 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --checkpoints_total_limit=3 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --validation_prompts="A robot naruto, 4k photo" \
  --report_to="wandb" \
  --push_to_hub \
  --output_dir="kandi2-prior-naruto-model"
```

### è§£ç å™¨æ¨¡å‹
```py
export DATASET_NAME="lambdalabs/naruto-blip-captions"

accelerate launch --mixed_precision="fp16"  train_text_to_image_decoder.py \
  --dataset_name=$DATASET_NAME \
  --resolution=768 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --gradient_checkpointing \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --checkpoints_total_limit=3 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --validation_prompts="A robot naruto, 4k photo" \
  --report_to="wandb" \
  --push_to_hub \
  --output_dir="kandi2-decoder-naruto-model"
```
è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–°è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼

### å…ˆéªŒæ¨¡å‹

```py
from diffusers import AutoPipelineForText2Image, DiffusionPipeline
import torch

prior_pipeline = DiffusionPipeline.from_pretrained(output_dir, torch_dtype=torch.float16)
prior_components = {"prior_" + k: v for k,v in prior_pipeline.components.items()}
pipeline = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", **prior_components, torch_dtype=torch.float16)

pipe.enable_model_cpu_offload()
prompt="A robot naruto, 4k photo"
image = pipeline(prompt=prompt, negative_prompt=negative_prompt).images[0]
```
>è¯·éšæ„ä½¿ç”¨æ‚¨è‡ªå·±è®­ç»ƒæœ‰ç´ çš„è§£ç å™¨æ£€æŸ¥ç‚¹æ›¿æ¢`kandinsky-community/kandinsky-2-2-decoder`!
### è§£ç å™¨æ¨¡å‹

```py
from diffusers import AutoPipelineForText2Image, DiffusionPipeline
import torch

prior_pipeline = DiffusionPipeline.from_pretrained(output_dir, torch_dtype=torch.float16)
prior_components = {"prior_" + k: v for k,v in prior_pipeline.components.items()}
pipeline = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", **prior_components, torch_dtype=torch.float16)

pipe.enable_model_cpu_offload()
prompt="A robot naruto, 4k photo"
image = pipeline(prompt=prompt, negative_prompt=negative_prompt).images[0]
```
å¯¹äºè§£ç å™¨æ¨¡å‹ï¼Œè¿˜å¯ä»¥ä»ä¿å­˜çš„æ£€æŸ¥ç‚¹æ‰§è¡Œæ¨æ–­ï¼Œè¿™å¯¹äºæŸ¥çœ‹ä¸­é—´ç»“æœå¾ˆæœ‰ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°†æ£€æŸ¥ç‚¹åŠ è½½åˆ°UNetä¸­:
```py
from diffusers import AutoPipelineForText2Image, UNet2DConditionModel

unet = UNet2DConditionModel.from_pretrained("path/to/saved/model" + "/checkpoint-<N>/unet")

pipeline = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", unet=unet, torch_dtype=torch.float16)
pipeline.enable_model_cpu_offload()

image = pipeline(prompt="A robot naruto, 4k photo").images[0]
```
## [](#next-steps)åç»­æ­¥éª¤ï¼ˆNext stepsï¼‰

æ­å–œæ‚¨è®­ç»ƒäº†Kandinsky 2.2 æ¨¡å‹ï¼è‹¥è¦è¯¦ç»†äº†è§£å¦‚ä½•ä½¿ç”¨æ–°æ¨¡å‹ï¼Œä»¥ä¸‹æŒ‡å—å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼š

+   é˜…è¯»[Kandinsky](../using-diffusers/kandinsky)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•å°†å…¶ç”¨äºå„ç§ä¸åŒçš„ä»»åŠ¡ï¼ˆæ–‡ç”Ÿå›¾ã€å›¾ç”Ÿå›¾ã€ä¿®å¤ã€æ’å€¼ï¼‰ï¼Œä»¥åŠå¦‚ä½•å°†å…¶ä¸ ControlNet ç»“åˆä½¿ç”¨ã€‚
+   æŸ¥çœ‹ [DreamBooth](dreambooth) å’Œ [LoRA](lora) åŸ¹è®­æŒ‡å—ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨å‡ å¼ ç¤ºä¾‹å›¾åƒè®­ç»ƒä¸ªæ€§åŒ–çš„Kandinskyæ¨¡å‹ã€‚è¿™ä¸¤ç§è®­ç»ƒæŠ€å·§ç”šè‡³å¯ä»¥ç»“åˆä½¿ç”¨ï¼