# ä½¿ç”¨å¤šä¸ª GPU è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/diffusers/training/distributed_inference>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/diffusers/training/distributed_inference>


åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ğŸ¤— è·¨å¤šä¸ª GPU è¿è¡Œæ¨ç†
 [åŠ é€Ÿ](https://huggingface.co/docs/accelerate/index)
 æˆ–è€…
 [PyTorch åˆ†å¸ƒå¼](https://pytorch.org/tutorials/beginner/dist_overview.html)
 ï¼Œè¿™å¯¹äºå¹¶è¡Œç”Ÿæˆå¤šä¸ªæç¤ºå¾ˆæœ‰ç”¨ã€‚


æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate å’Œ PyTorch Distributed è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ã€‚


## ğŸ¤— åŠ é€Ÿ



ğŸ¤—
 [åŠ é€Ÿ](https://huggingface.co/docs/accelerate/index)
 æ˜¯ä¸€ä¸ªæ—¨åœ¨è½»æ¾è·¨åˆ†å¸ƒå¼è®¾ç½®è®­ç»ƒæˆ–è¿è¡Œæ¨ç†çš„åº“ã€‚å®ƒç®€åŒ–äº†è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒçš„è¿‡ç¨‹ï¼Œä½¿æ‚¨å¯ä»¥ä¸“æ³¨äº PyTorch ä»£ç ã€‚


é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª Python æ–‡ä»¶å¹¶åˆå§‹åŒ–
 `accelerate.PartialState`
 åˆ›å»ºåˆ†å¸ƒå¼ç¯å¢ƒï¼›æ‚¨çš„è®¾ç½®ä¼šè‡ªåŠ¨æ£€æµ‹åˆ°ï¼Œå› æ­¤æ‚¨æ— éœ€æ˜¾å¼å®šä¹‰
 `ç­‰çº§`
 æˆ–è€…
 `ä¸–ç•Œå¤§å°`
 ã€‚ç§»åŠ¨
 [DiffusionPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/overview#diffusers.DiffusionPipeline)
 åˆ°
 `distributed_state.device`
 ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä¸€ä¸ª GPUã€‚


ç°åœ¨ä½¿ç”¨
 `è¿›ç¨‹ä¹‹é—´çš„åˆ†å‰²`
 ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„å®ç”¨ç¨‹åºå¯ä»¥åœ¨å¤šä¸ªè¿›ç¨‹ä¹‹é—´è‡ªåŠ¨åˆ†é…æç¤ºã€‚



```
import torch
from accelerate import PartialState
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True
)
distributed_state = PartialState()
pipeline.to(distributed_state.device)

with distributed_state.split_between_processes(["a dog", "a cat"]) as prompt:
    result = pipeline(prompt).images[0]
    result.save(f"result\_{distributed\_state.process\_index}.png")
```


ä½¿ç”¨
 `--num_processes`
 å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„ GPU æ•°é‡ï¼Œå¹¶è°ƒç”¨
 â€˜åŠ é€Ÿå‘å°„â€™
 è¿è¡Œè„šæœ¬ï¼š



```
accelerate launch run_distributed.py --num_processes=2
```


è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹
 [ä½¿ç”¨ ğŸ¤— Accelerate è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate)
 æŒ‡å¯¼ã€‚


## PyTorch åˆ†å¸ƒå¼



PyTorch æ”¯æŒ
 [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
 è¿™ä½¿å¾—æ•°æ®å¹¶è¡Œã€‚


é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª Python æ–‡ä»¶å¹¶å¯¼å…¥
 `torch.distributed`
 å’Œ
 `torch.multiprocessing`
 è®¾ç½®åˆ†å¸ƒå¼è¿›ç¨‹ç»„å¹¶åœ¨æ¯ä¸ª GPU ä¸Šç”Ÿæˆç”¨äºæ¨ç†çš„è¿›ç¨‹ã€‚æ‚¨è¿˜åº”è¯¥åˆå§‹åŒ–ä¸€ä¸ª
 [DiffusionPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/overview#diffusers.DiffusionPipeline)
 ï¼š



```
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

from diffusers import DiffusionPipeline

sd = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True
)
```


æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥è¿è¡Œæ¨ç†ï¼›
 [`init_process_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group)
 å¤„ç†åˆ›å»ºå…·æœ‰è¦ä½¿ç”¨çš„åç«¯ç±»å‹çš„åˆ†å¸ƒå¼ç¯å¢ƒï¼Œ
 `ç­‰çº§`
 å½“å‰è¿›ç¨‹çš„ï¼Œä»¥åŠ
 `ä¸–ç•Œå¤§å°`
 æˆ–å‚ä¸çš„è¿›ç¨‹æ•°ã€‚å¦‚æœæ‚¨åœ¨ 2 ä¸ª GPU ä¸Šå¹¶è¡Œè¿è¡Œæ¨ç†ï¼Œé‚£ä¹ˆ
 `ä¸–ç•Œå¤§å°`
 æ˜¯ 2ã€‚


ç§»åŠ¨
 [DiffusionPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/overview#diffusers.DiffusionPipeline)
 åˆ°
 `ç­‰çº§`
 å¹¶ä½¿ç”¨
 `è·å–æ’å`
 ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä¸€ä¸ª GPUï¼Œå…¶ä¸­æ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æç¤ºï¼š



```
def run\_inference(rank, world\_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

    sd.to(rank)

    if torch.distributed.get_rank() == 0:
        prompt = "a dog"
    elif torch.distributed.get_rank() == 1:
        prompt = "a cat"

    image = sd(prompt).images[0]
    image.save(f"./{'\_'.join(prompt)}.png")
```


è¦è¿è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œè¯·è°ƒç”¨
 [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn)
 è¿è¡Œ
 `è¿è¡Œæ¨ç†`
 å‡½æ•°ä¸­å®šä¹‰çš„ GPU æ•°é‡
 `ä¸–ç•Œå¤§å°`
 ï¼š



```
def main():
    world_size = 2
    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)


if __name__ == "\_\_main\_\_":
    main()
```


å®Œæˆæ¨ç†è„šæœ¬åï¼Œä½¿ç”¨
 `--nproc_per_node`
 æŒ‡å®šè¦ä½¿ç”¨å’Œè°ƒç”¨çš„ GPU æ•°é‡çš„å‚æ•°
 `torchrun`
 è¿è¡Œè„šæœ¬ï¼š



```
torchrun run_distributed.py --nproc_per_node=2
```