# 加快推理速度

> 译者：[片刻小哥哥](https://github.com/jiangzhonglian)
>
> 项目地址：<https://huggingface.apachecn.org/docs/diffusers/optimization/fp16>
>
> 原始地址：<https://huggingface.co/docs/diffusers/optimization/fp16>


有多种方法可以优化 🤗 扩散器以提高推理速度。作为一般经验法则，我们建议使用
 [xFormers](xformers)
 或者
 `torch.nn.function.scaled_dot_product_attention`
 PyTorch 2.0 因其内存效率高的注意力而受到青睐。


在许多情况下，优化速度或内存可以提高另一个方面的性能，因此您应该尽可能尝试同时优化这两个方面。本指南重点关注推理速度，但您可以了解有关保留内存的更多信息
 [减少内存使用](内存)
 指导。


下面的结果是根据提示生成单个 512x512 图像获得的
 “一张宇航员在火星上骑马的照片”
 在 Nvidia Titan RTX 上执行 50 个 DDIM 步骤，展示了您可以预期的加速效果。


|  | 	 latency	  | 	 speed-up	  |
| --- | --- | --- |
| 	 original	  | 	 9.50s	  | 	 x1	  |
| 	 fp16	  | 	 3.61s	  | 	 x2.63	  |
| 	 channels last	  | 	 3.30s	  | 	 x2.88	  |
| 	 traced UNet	  | 	 3.21s	  | 	 x2.96	  |
| 	 memory efficient attention	  | 	 2.63s	  | 	 x3.61	  |


## 使用 TensorFloat-32



在 Ampere 和更高版本的 CUDA 设备上，矩阵乘法和卷积可以使用
 [TensorFloat-32 (TF32)](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32- precision-format/)
 模式计算速度更快，但计算精度稍差。默认情况下，PyTorch 启用 TF32 模式进行卷积，但不启用矩阵乘法。除非您的网络需要完整的 float32 精度，否则我们建议启用 TF32 进行矩阵乘法。它可以显着加快计算速度，而数值精度的损失通常可以忽略不计。



```
import torch

torch.backends.cuda.matmul.allow_tf32 = True
```


您可以在以下位置了解有关 TF32 的更多信息
 [混合精度训练](https://huggingface.co/docs/transformers/en/perf_train_gpu_one#tf32)
 指导。


## 半精度权重



为了节省 GPU 内存并获得更快的速度，请尝试直接以半精度或 float16 加载和运行模型权重：



```
import torch
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]
```


不要使用
 [`torch.autocast`](https://pytorch.org/docs/stable/amp.html#torch.autocast)
 在任何管道中，因为它可能导致黑色图像，并且总是比纯 float16 精度慢。