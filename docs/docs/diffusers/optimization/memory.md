> ç¿»è¯‘ä»»åŠ¡

# [](#reduce-memory-usage)å‡å°‘å†…å­˜ä½¿ç”¨é‡(Reduce memory usage)

> è¯‘è€…ï¼š[ç–¾é£å…”X](https://github.com/jifnegtu)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/diffusers/optimization/memory>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/diffusers/optimization/memory>



ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„ä¸€ä¸ªéšœç¢æ˜¯å…¶éœ€è¦å¤§é‡çš„å†…å­˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªæŒ‘æˆ˜ï¼Œæœ‰å‡ ç§å‡å°‘å†…å­˜æ¶ˆè€—çš„æŠ€æœ¯å¯ä¾›ä½¿ç”¨ï¼Œè¿™äº›æŠ€æœ¯ç”šè‡³èƒ½è®©ä¸€äº›æœ€å¤§çš„æ¨¡å‹åœ¨å…è´¹å±‚çº§æˆ–æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œã€‚æœ‰äº›æŠ€æœ¯è¿˜å¯ä»¥ç›¸äº’ç»“åˆä½¿ç”¨ï¼Œè¿›ä¸€æ­¥é™ä½å†…å­˜çš„ä½¿ç”¨é‡ã€‚

>åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨æˆ–æå‡é€Ÿåº¦å®é™…ä¸Šä¼šé—´æ¥ä¿ƒè¿›å¦ä¸€æ–¹é¢çš„æ€§èƒ½æ”¹å–„ï¼Œå› æ­¤ä½ åº”è¯¥å°½å¯èƒ½åœ°å°è¯•åŒæ—¶åœ¨è¿™ä¸¤ä¸ªæ–¹å‘ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚æœ¬æŒ‡å—ä¾§é‡äºæœ€å¤§ç¨‹åº¦åœ°å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥äº†è§£æœ‰å…³å¦‚ä½•[åŠ å¿«æ¨ç†(Speed up inference.)](fp16)é€Ÿåº¦çš„æ›´å¤šä¿¡æ¯ã€‚

ä»¥ä¸‹ç»“æœæ˜¯æ ¹æ®æç¤ºç”Ÿæˆå•ä¸ª 512x512 å›¾åƒè·å¾—çš„ï¼Œè¯¥å›¾åƒæ˜¯å®‡èˆªå‘˜åœ¨ç«æ˜Ÿä¸Šéª‘é©¬çš„ç…§ç‰‡ï¼Œåœ¨ Nvidia Titan RTX ä¸Šä»¥ 50 ä¸ª DDIM æ­¥é•¿è¿è¡Œï¼Œå±•ç¤ºäº†ç”±äºå†…å­˜æ¶ˆè€—å‡å°‘è€Œå¯ä»¥é¢„æœŸçš„åŠ é€Ÿã€‚

|  | latency | speed-up |
| --- | --- | --- |
| original | 9.50s | x1 |
| fp16 | 3.61s | x2.63 |
| channels last | 3.30s | x2.88 |
| traced UNet | 3.21s | x2.96 |
| memory-efficient attention | 2.63s | x3.61 |

## [](#sliced-vae)Sliced VAE

Sliced VAEæŠ€æœ¯å…è®¸ä½ åœ¨æœ‰é™çš„VRAMæ¡ä»¶ä¸‹è§£ç å¤§è§„æ¨¡çš„å›¾åƒæ‰¹æ¬¡ï¼Œæˆ–æ˜¯å¤„ç†åŒ…å«32å¼ ä»¥ä¸Šå›¾åƒçš„æ‰¹æ¬¡ï¼Œé€šè¿‡é€å¼ è§£ç æ½œå˜é‡æ‰¹æ¬¡çš„æ–¹å¼æ¥å®ç°ã€‚å¦‚æœæ‚¨å®‰è£…äº† xFormersï¼Œæ‚¨å¯èƒ½å¸Œæœ›å°†å…¶ä¸ [enable\_xformers\_memory\_efficient\_attentionï¼ˆï¼‰](/docs/diffusers/v0.28.2/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention) ç»“åˆä½¿ç”¨ï¼Œä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

è¦åœ¨æ¨æ–­å‰ä½¿ç”¨Sliced VAEï¼Œä½ éœ€è¦åœ¨ä½ çš„pipelineä¸Šè°ƒç”¨ [enable\_vae\_slicingï¼ˆï¼‰](/docs/diffusers/v0.28.2/en/api/pipelines/latent_consistency_models#diffusers.LatentConsistencyModelPipeline.enable_vae_slicing)ï¼š

```
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_vae_slicing()
#pipe.enable_xformers_memory_efficient_attention()
images = pipe([prompt] * 32).images
```
æ‚¨å¯èƒ½ä¼šçœ‹åˆ°å¤šå›¾åƒæ‰¹å¤„ç†çš„VAEè§£ç æ€§èƒ½ç•¥æœ‰æå‡ï¼Œå¹¶ä¸”å¯¹å•å›¾åƒæ‰¹å¤„ç†çš„æ€§èƒ½åº”è¯¥æ²¡æœ‰å½±å“ã€‚

## [](#tiled-vae)Tiled VAE

Tiled VAEå¤„ç†è¿˜å¯ä»¥åœ¨æœ‰é™çš„VRAMä¸Šå¤„ç†å¤§å‹å›¾åƒï¼ˆä¾‹å¦‚ï¼Œåœ¨8GB VRAMä¸Šç”Ÿæˆ4kå›¾åƒï¼‰ï¼Œæ–¹æ³•æ˜¯å°†å›¾åƒæ‹†åˆ†ä¸ºé‡å çš„å›¾å—ï¼Œè§£ç å›¾å—ï¼Œç„¶åå°†è¾“å‡ºæ··åˆåœ¨ä¸€èµ·ä»¥ç»„æˆæœ€ç»ˆå›¾åƒã€‚å¦‚æœæ‚¨å®‰è£…äº† xFormersï¼Œæ‚¨è¿˜åº”è¯¥ä½¿ç”¨å¸¦æœ‰ [enable\_xformers\_memory\_efficient\_attentionï¼ˆï¼‰ ](/docs/diffusers/v0.28.2/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)çš„Tiled VAEï¼Œä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

è¦ä½¿ç”¨Tiled VAE å¤„ç†ï¼Œè¯·åœ¨æ¨ç†ä¹‹å‰åœ¨ç®¡é“ä¸Šè°ƒç”¨ [enable\_vae\_tilingï¼ˆï¼‰](/docs/diffusers/v0.28.2/en/api/pipelines/latent_consistency_models#diffusers.LatentConsistencyModelPipeline.enable_vae_tiling)ï¼š

```
import torch
from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to("cuda")
prompt = "a beautiful landscape photograph"
pipe.enable_vae_tiling()
#pipe.enable_xformers_memory_efficient_attention()

image = pipe([prompt], width=3840, height=2224, num_inference_steps=20).images[0]
```
è¾“å‡ºå›¾åƒå…·æœ‰ä¸€äº›å›¾å—åˆ°å›¾å—çš„è‰²è°ƒå˜åŒ–ï¼Œå› ä¸ºå›¾å—æ˜¯å•ç‹¬è§£ç çš„ï¼Œä½†æ‚¨ä¸åº”è¯¥çœ‹åˆ°å›¾å—ä¹‹é—´ä»»ä½•å°–é”å’Œæ˜æ˜¾çš„æ¥ç¼ã€‚å¯¹äº 512x512 æˆ–æ›´å°çš„å›¾åƒï¼Œå¹³é“ºå°†å¤„äºå…³é—­çŠ¶æ€ã€‚

## [](#cpu-offloading)CPU å¸è½½ (CPU offloading)

å°†æƒé‡å¸è½½åˆ° CPU å¹¶åœ¨æ‰§è¡Œå‰å‘ä¼ é€’æ—¶ä»…å°†å®ƒä»¬åŠ è½½åˆ° GPU ä¸Šä¹Ÿå¯ä»¥èŠ‚çœå†…å­˜ã€‚é€šå¸¸ï¼Œæ­¤æŠ€æœ¯å¯ä»¥å°†å†…å­˜æ¶ˆè€—å‡å°‘åˆ° 3GB ä»¥ä¸‹ã€‚

è¦æ‰§è¡Œ CPU å¸è½½ï¼Œè¯·è°ƒç”¨ [enable\_sequential\_cpu\_offloadï¼ˆï¼‰](/docs/diffusers/v0.28.2/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_sequential_cpu_offload)ï¼š

```
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_sequential_cpu_offload()
image = pipe(prompt).images[0]
```
CPU å¸è½½é€‚ç”¨äºå­æ¨¡å—ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚è¿™æ˜¯æœ€å°åŒ–å†…å­˜æ¶ˆè€—çš„æœ€ä½³æ–¹æ³•ï¼Œä½†ç”±äºæ‰©æ•£è¿‡ç¨‹çš„è¿­ä»£æ€§è´¨ï¼Œæ¨ç†é€Ÿåº¦è¦æ…¢å¾—å¤šã€‚ç®¡é“çš„ UNet ç»„ä»¶è¿è¡Œæ•°æ¬¡ï¼ˆå¤šè¾¾`num_inference_steps` ï¼‰; æ¯æ¬¡ï¼Œä¸åŒçš„ UNet å­æ¨¡å—éƒ½ä¼šæ ¹æ®éœ€è¦æŒ‰é¡ºåºåŠ è½½å’Œå¸è½½ï¼Œä»è€Œå¯¼è‡´å¤§é‡å†…å­˜ä¼ è¾“ã€‚

>å¦‚æœè¦ä¼˜åŒ–é€Ÿåº¦ï¼Œè¯·è€ƒè™‘ä½¿ç”¨[æ¨¡å‹å¸è½½(model offloading)](#model-offloading)ï¼Œå› ä¸ºå®ƒè¦å¿«å¾—å¤šã€‚æƒè¡¡æ˜¯æ‚¨çš„å†…å­˜èŠ‚çœä¸ä¼šé‚£ä¹ˆå¤§ã€‚

>ä½¿ç”¨ [enable\_sequential\_cpu\_offloadï¼ˆï¼‰](/docs/diffusers/v0.28.2/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_sequential_cpu_offload) æ—¶ï¼Œä¸è¦äº‹å…ˆå°†ç®¡é“ç§»åŠ¨åˆ° CUDAï¼Œå¦åˆ™å†…å­˜æ¶ˆè€—çš„å‡å°‘åªä¼šéå¸¸æœ‰é™ï¼ˆæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤[é—®é¢˜](https://github.com/huggingface/diffusers/issues/1934)ï¼‰ã€‚

>[enable\_sequential\_cpu\_offloadï¼ˆï¼‰](/docs/diffusers/v0.28.2/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_sequential_cpu_offload) æ˜¯ä¸€ä¸ªæœ‰çŠ¶æ€çš„æ“ä½œï¼Œå®ƒä¼šåœ¨æ¨¡å‹ä¸Šå®‰è£…hookï¼ˆé’©å­ï¼‰ã€‚

## [](#model-offloading)æ¨¡å‹å¸è½½ (Model offloading)

>æ¨¡å‹å¸è½½éœ€è¦ ğŸ¤— Accelerate ç‰ˆæœ¬ 0.17.0 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚

[é¡ºåº CPU å¸è½½ (Sequential CPU offloading )](#cpu-offloading)å¯ä¿ç•™å¤§é‡å†…å­˜ï¼Œä½†ä¼šä½¿æ¨ç†é€Ÿåº¦å˜æ…¢ï¼Œå› ä¸ºå­æ¨¡å—ä¼šæ ¹æ®éœ€è¦ç§»åŠ¨åˆ° GPUï¼Œå¹¶ä¸”åœ¨æ–°æ¨¡å—è¿è¡Œæ—¶ä¼šç«‹å³è¿”å›åˆ° CPUã€‚

å®Œæ•´æ¨¡å‹å¸è½½æ˜¯ä¸€ç§å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ° GPU çš„æ›¿ä»£æ–¹æ³•ï¼Œè€Œä¸æ˜¯å¤„ç†æ¯ä¸ªæ¨¡å‹çš„ç»„æˆ*å­æ¨¡å—*ã€‚å¯¹æ¨ç†æ—¶é—´çš„å½±å“å¯ä»¥å¿½ç•¥ä¸è®¡ï¼ˆä¸å°†ç®¡é“ç§»åŠ¨åˆ° `cuda` ç›¸æ¯”ï¼‰ï¼Œå®ƒä»ç„¶æä¾›äº†ä¸€äº›å†…å­˜èŠ‚çœã€‚

åœ¨æ¨¡å‹å¸è½½æœŸé—´ï¼Œåªæœ‰ç®¡é“çš„ä¸€ä¸ªä¸»è¦ç»„ä»¶ï¼ˆé€šå¸¸æ˜¯æ–‡æœ¬ç¼–ç å™¨ã€UNet å’Œ VAEï¼‰ æ”¾ç½®åœ¨ GPU ä¸Šï¼Œè€Œå…¶ä»–äººåˆ™åœ¨ CPU ä¸Šç­‰å¾…ã€‚åƒ UNet è¿™æ ·è¿è¡Œå¤šæ¬¡è¿­ä»£çš„ç»„ä»¶ä¼šä¿ç•™åœ¨ GPU ä¸Šï¼Œç›´åˆ°ä¸å†éœ€è¦å®ƒä»¬ã€‚

é€šè¿‡åœ¨ç®¡é“ä¸Šè°ƒç”¨ [enable\_model\_cpu\_offloadï¼ˆï¼‰](/docs/diffusers/v0.28.2/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_model_cpu_offload) æ¥å¯ç”¨æ¨¡å‹å¸è½½ï¼š

```
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_model_cpu_offload()
image = pipe(prompt).images[0]
```
>ä¸ºäº†åœ¨è°ƒç”¨æ¨¡å‹åæ­£ç¡®å¸è½½æ¨¡å‹ï¼Œéœ€è¦è¿è¡Œæ•´ä¸ªç®¡é“ï¼Œå¹¶æŒ‰ç®¡é“çš„é¢„æœŸé¡ºåºè°ƒç”¨æ¨¡å‹ã€‚å¦‚æœåœ¨å®‰è£…æŒ‚é’©ååœ¨ç®¡é“ä¸Šä¸‹æ–‡ä¹‹å¤–é‡ç”¨æ¨¡å‹ï¼Œè¯·è°¨æ…è¡Œäº‹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[åˆ é™¤é’©å­ ( Removing Hooks)](https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.hooks.remove_hook_from_module)ã€‚

>[enable\_model\_cpu\_offloadï¼ˆï¼‰](/docs/diffusers/v0.28.2/en/api/pipelines/overview#diffusers.DiffusionPipeline.enable_model_cpu_offload) æ˜¯ä¸€ä¸ªæœ‰çŠ¶æ€çš„æ“ä½œï¼Œå®ƒåœ¨æ¨¡å‹ä¸Šå®‰è£…äº†hookï¼ˆé’©å­ï¼‰ï¼Œå¹¶åœ¨pipelineä¸Šä¿æŒäº†ä¸€å®šçš„çŠ¶æ€ã€‚

## [](#channels-last-memory-format)Channels-last å­˜å‚¨æ ¼å¼ (Channels-last memory format)

Channels-lastå†…å­˜æ ¼å¼æ˜¯ä¸€ç§æ›¿ä»£çš„å†…å­˜å¸ƒå±€æ–¹å¼ï¼Œç”¨äºä¿å­˜NCHWå¼ é‡çš„ç»´åº¦é¡ºåºã€‚åœ¨channels-lastæ ¼å¼ä¸‹ï¼Œé€šé“æˆä¸ºäº†æœ€å¯†é›†çš„ç»´åº¦ï¼ˆå³æŒ‰ç…§åƒç´ é¡ºåºå­˜å‚¨å›¾åƒï¼‰ã€‚ç”±äºå¹¶éæ‰€æœ‰è¿ç®—ç¬¦ç›®å‰éƒ½æ”¯æŒchannels-lastæ ¼å¼ï¼Œå› æ­¤åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä½†ä½ ä»ç„¶åº”è¯¥å°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹æ˜¯å¦é€‚ç”¨äºä½ çš„æ¨¡å‹ã€‚

ä¾‹å¦‚ï¼Œè¦è®¾ç½®pipelineä¸­çš„UNetä½¿ç”¨channels-lastæ ¼å¼ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```
print(pipe.unet.conv_out.state_dict()["weight"].stride())  # (2880, 9, 3, 1)
pipe.unet.to(memory_format=torch.channels_last)  # in-place operation
print(
    pipe.unet.conv_out.state_dict()["weight"].stride()
)  # (2880, 1, 960, 320) having a stride of 1 for the 2nd dimension proves that it works
```
## [](#tracing)è·Ÿè¸ª (Tracing)

è¿½è¸ªï¼ˆTracingï¼‰æ˜¯é€šè¿‡æ¨¡å‹è¿è¡Œä¸€ä¸ªç¤ºä¾‹è¾“å…¥å¼ é‡ï¼Œå¹¶æ•æ‰è¯¥è¾“å…¥åœ¨ç©¿è¿‡æ¨¡å‹å„å±‚æ—¶æ‰€æ‰§è¡Œçš„æ“ä½œã€‚è¿”å›çš„å¯æ‰§è¡Œæ–‡ä»¶æˆ–`ScriptFunction`ä¼šé€šè¿‡å³æ—¶ç¼–è¯‘ï¼ˆjust-in-time compilationï¼‰è¿›è¡Œä¼˜åŒ–ã€‚

è¦å¯¹UNetè¿›è¡Œè¿½è¸ªï¼š

```
import time
import torch
from diffusers import StableDiffusionPipeline
import functools

# torch disable grad
torch.set_grad_enabled(False)

# set variables
n_experiments = 2
unet_runs_per_experiment = 50


# load inputs
def generate_inputs():
    sample = torch.randn((2, 4, 64, 64), device="cuda", dtype=torch.float16)
    timestep = torch.rand(1, device="cuda", dtype=torch.float16) * 999
    encoder_hidden_states = torch.randn((2, 77, 768), device="cuda", dtype=torch.float16)
    return sample, timestep, encoder_hidden_states


pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")
unet = pipe.unet
unet.eval()
unet.to(memory_format=torch.channels_last)  # use channels_last memory format
unet.forward = functools.partial(unet.forward, return_dict=False)  # set return_dict=False as default

# warmup
for _ in range(3):
    with torch.inference_mode():
        inputs = generate_inputs()
        orig_output = unet(*inputs)

# trace
print("tracing..")
unet_traced = torch.jit.trace(unet, inputs)
unet_traced.eval()
print("done tracing")


# warmup and optimize graph
for _ in range(5):
    with torch.inference_mode():
        inputs = generate_inputs()
        orig_output = unet_traced(*inputs)


# benchmarking
with torch.inference_mode():
    for _ in range(n_experiments):
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(unet_runs_per_experiment):
            orig_output = unet_traced(*inputs)
        torch.cuda.synchronize()
        print(f"unet traced inference took {time.time() - start_time:.2f} seconds")
    for _ in range(n_experiments):
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(unet_runs_per_experiment):
            orig_output = unet(*inputs)
        torch.cuda.synchronize()
        print(f"unet inference took {time.time() - start_time:.2f} seconds")

# save the model
unet_traced.save("unet_traced.pt")
```
å°†ç®¡é“çš„`unet`å±æ€§æ›¿æ¢ä¸ºè·Ÿè¸ªçš„æ¨¡å‹ï¼š

```
from diffusers import StableDiffusionPipeline
import torch
from dataclasses import dataclass


@dataclass
class UNet2DConditionOutput:
    sample: torch.Tensor


pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")

# use jitted unet
unet_traced = torch.jit.load("unet_traced.pt")


# del pipe.unet
class TracedUNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.in_channels = pipe.unet.config.in_channels
        self.device = pipe.unet.device

    def forward(self, latent_model_input, t, encoder_hidden_states):
        sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]
        return UNet2DConditionOutput(sample=sample)


pipe.unet = TracedUNet()

with torch.inference_mode():
    image = pipe([prompt] * 1, num_inference_steps=50).images[0]
```
## [](#memory-efficient-attention)è®°å¿†-é«˜æ•ˆæ³¨æ„ (Memory-efficient attention)

æœ€è¿‘åœ¨æ³¨æ„åŠ›å—ä¸­ä¼˜åŒ–å¸¦å®½çš„å·¥ä½œæå¤§åœ°åŠ å¿«äº†é€Ÿåº¦å¹¶å‡å°‘äº† GPU å†…å­˜çš„ä½¿ç”¨ã€‚æœ€æ–°çš„å†…å­˜æ•ˆç‡æ³¨æ„åŠ›ç±»å‹æ˜¯ [Flash Attention](https://arxiv.org/abs/2205.14135)ï¼ˆæ‚¨å¯ä»¥åœ¨ [HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention) ä¸ŠæŸ¥çœ‹åŸå§‹ä»£ç ï¼‰ã€‚

>å¦‚æœä½ å®‰è£…çš„PyTorchç‰ˆæœ¬æ˜¯2.0æˆ–æ›´é«˜ï¼Œé‚£ä¹ˆåœ¨å¯ç”¨`xformers`æ—¶ï¼Œä¸åº”è¯¥æœŸæœ›åœ¨æ¨ç†é€Ÿåº¦ä¸Šæœ‰æ‰€æå‡ã€‚

è¦ä½¿ç”¨ Flash Attentionï¼Œè¯·å®‰è£…ä»¥ä¸‹è½¯ä»¶ï¼š

+   PyTorch > 1.12 ç‰ˆ
+   CUDAå¯ç”¨
+   [xFormers](xformers)

ç„¶ååœ¨ç®¡é“ä¸Šè°ƒç”¨ [enable\_xformers\_memory\_efficient\_attentionï¼ˆï¼‰ï¼š](/docs/diffusers/v0.28.2/en/api/models/overview#diffusers.ModelMixin.enable_xformers_memory_efficient_attention)

```
from diffusers import DiffusionPipeline
import torch

pipe = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")

pipe.enable_xformers_memory_efficient_attention()

with torch.inference_mode():
    sample = pipe("a small cat")

# optional: You can disable it via
# pipe.disable_xformers_memory_efficient_attention()
```
ä½¿ç”¨`xformers`æ—¶çš„è¿­ä»£é€Ÿåº¦åº”ä¸[æ­¤å¤„](torch2.0)æ‰€è¿°çš„ PyTorch 2.0 çš„è¿­ä»£é€Ÿåº¦ç›¸åŒ¹é…ã€‚