# æ§åˆ¶ç½‘

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/diffusers/using-diffusers/controlnet>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/diffusers/using-diffusers/controlnet>


ControlNet æ˜¯ä¸€ç§é€šè¿‡ä½¿ç”¨é™„åŠ è¾“å…¥å›¾åƒè°ƒèŠ‚æ¨¡å‹æ¥æ§åˆ¶å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¨¡å‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å¤šç§ç±»å‹çš„è°ƒèŠ‚è¾“å…¥ï¼ˆç²¾æ˜è¾¹ç¼˜ã€ç”¨æˆ·è‰å›¾ã€äººä½“å§¿åŠ¿ã€æ·±åº¦ç­‰ï¼‰æ¥æ§åˆ¶æ‰©æ•£æ¨¡å‹ã€‚è¿™éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä½¿æ‚¨å¯ä»¥æ›´å¥½åœ°æ§åˆ¶å›¾åƒç”Ÿæˆï¼Œä»è€Œæ›´è½»æ¾åœ°ç”Ÿæˆç‰¹å®šå›¾åƒï¼Œè€Œæ— éœ€å°è¯•ä¸åŒçš„æ–‡æœ¬æç¤ºæˆ–å»å™ªå€¼ã€‚


è¯·å‚é˜…ç¬¬ 3.5 èŠ‚
 [ControlNet](https://huggingface.co/papers/2302.05543)
 æœ‰å…³å„ç§è°ƒèŠ‚è¾“å…¥çš„ ControlNet å®ç°åˆ—è¡¨çš„è®ºæ–‡ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®æ‰¾åˆ°å®˜æ–¹ç¨³å®šæ‰©æ•£ ControlNet æ¡ä»¶æ¨¡å‹
 [lllyasviel](https://huggingface.co/lllyasviel)
 çš„é›†çº¿å™¨ç®€ä»‹ç­‰
 [ç¤¾åŒºåŸ¹è®­](https://huggingface.co/models?other=stable-diffusion&other=controlnet)
 é›†çº¿å™¨ä¸Šçš„é‚£äº›ã€‚


å¯¹äº Stable Diffusion XL (SDXL) ControlNet æ¨¡å‹ï¼Œæ‚¨å¯ä»¥åœ¨ ğŸ¤— ä¸Šæ‰¾åˆ°å®ƒä»¬
 [æ‰©æ•£å™¨](https://huggingface.co/diffusers)
 ä¸­å¿ƒç»„ç»‡ï¼Œæˆ–è€…æ‚¨å¯ä»¥æµè§ˆ
 [ç¤¾åŒºåŸ¹è®­](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet)
 é›†çº¿å™¨ä¸Šçš„é‚£äº›ã€‚


ControlNet æ¨¡å‹å…·æœ‰ä¸¤ç»„é€šè¿‡é›¶å·ç§¯å±‚è¿æ¥çš„æƒé‡ï¼ˆæˆ–å—ï¼‰ï¼š


* A
 *é”å®šå‰¯æœ¬*
 ä¿ç•™å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å­¦åˆ°çš„æ‰€æœ‰å†…å®¹
* A
 *å¯è®­ç»ƒå‰¯æœ¬*
 æ¥å—é¢å¤–è°ƒèŠ‚è¾“å…¥çš„è®­ç»ƒ


ç”±äºé”å®šçš„å‰¯æœ¬ä¿ç•™äº†é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå› æ­¤åœ¨æ–°çš„æ¡ä»¶è¾“å…¥ä¸Šè®­ç»ƒå’Œå®ç° ControlNet ä¸å¾®è°ƒä»»ä½•å…¶ä»–æ¨¡å‹ä¸€æ ·å¿«ï¼Œå› ä¸ºæ‚¨ä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚


æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ControlNet è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€ä¿®å¤ç­‰ï¼æœ‰å¤šç§ç±»å‹çš„ ControlNet è°ƒèŠ‚è¾“å…¥å¯ä¾›é€‰æ‹©ï¼Œä½†åœ¨æœ¬æŒ‡å—ä¸­æˆ‘ä»¬å°†åªå…³æ³¨å…¶ä¸­çš„å‡ ç§ã€‚è¯·éšæ„å°è¯•å…¶ä»–è°ƒèŠ‚è¾“å…¥ï¼


åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š



```
# uncomment to install the necessary libraries in Colab
#!pip install diffusers transformers accelerate safetensors opencv-python
```


## æ–‡æœ¬è½¬å›¾åƒ



å¯¹äºæ–‡æœ¬åˆ°å›¾åƒï¼Œæ‚¨é€šå¸¸å°†æ–‡æœ¬æç¤ºä¼ é€’ç»™æ¨¡å‹ã€‚ä½†é€šè¿‡ ControlNetï¼Œæ‚¨å¯ä»¥æŒ‡å®šé¢å¤–çš„è°ƒèŠ‚è¾“å…¥ã€‚è®©æˆ‘ä»¬ç”¨ç²¾æ˜çš„å›¾åƒï¼ˆé»‘è‰²èƒŒæ™¯ä¸Šçš„ç™½è‰²è½®å»“å›¾åƒï¼‰æ¥è°ƒèŠ‚æ¨¡å‹ã€‚è¿™æ ·ï¼ŒControlNetå°±å¯ä»¥ä½¿ç”¨cannyå›¾åƒä½œä¸ºæ§åˆ¶ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆå…·æœ‰ç›¸åŒè½®å»“çš„å›¾åƒã€‚


åŠ è½½å›¾åƒå¹¶ä½¿ç”¨
 [opencv-python](https://github.com/opencv/opencv-python)
 æå–cannyå›¾åƒçš„åº“ï¼š



```
from diffusers import StableDiffusionControlNetPipeline
from diffusers.utils import load_image
from PIL import Image
import cv2
import numpy as np

image = load_image(
    "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input\_image\_vermeer.png"
)

image = np.array(image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
```


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png)

 åŸå§‹å›¾åƒ


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/vermeer_canny_edged.png)

 ç²¾æ˜çš„å›¾åƒ


æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä»¥ canny è¾¹ç¼˜æ£€æµ‹ä¸ºæ¡ä»¶çš„ ControlNet æ¨¡å‹å¹¶å°†å…¶ä¼ é€’ç»™
 [StableDiffusionControlNetPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)
 ã€‚ä½¿ç”¨é€Ÿåº¦è¶Šå¿«
 [UniPCMultistepScheduler](/docs/diffusers/v0.23.0/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
 å¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥åŠ é€Ÿæ¨ç†å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚



```
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
).to("cuda")

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```


ç°åœ¨å°†æ‚¨çš„æç¤ºå’Œç²¾æ˜çš„å›¾åƒä¼ é€’åˆ°ç®¡é“ï¼š



```
output = pipe(
    "the mona lisa", image=canny_image
).images[0]
```


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-text2img.png)


## å›¾åƒåˆ°å›¾åƒ



å¯¹äºå›¾åƒåˆ°å›¾åƒï¼Œæ‚¨é€šå¸¸ä¼šå°†åˆå§‹å›¾åƒå’Œæç¤ºä¼ é€’ç»™ç®¡é“ä»¥ç”Ÿæˆæ–°å›¾åƒã€‚å€ŸåŠ© ControlNetï¼Œæ‚¨å¯ä»¥ä¼ é€’é¢å¤–çš„è°ƒèŠ‚è¾“å…¥æ¥æŒ‡å¯¼æ¨¡å‹ã€‚è®©æˆ‘ä»¬ç”¨æ·±åº¦å›¾ï¼ˆåŒ…å«ç©ºé—´ä¿¡æ¯çš„å›¾åƒï¼‰æ¥è°ƒèŠ‚æ¨¡å‹ã€‚è¿™æ ·ï¼ŒControlNet å°±å¯ä»¥ä½¿ç”¨æ·±åº¦å›¾ä½œä¸ºæ§åˆ¶æ¥æŒ‡å¯¼æ¨¡å‹ç”Ÿæˆä¿ç•™ç©ºé—´ä¿¡æ¯çš„å›¾åƒã€‚


æ‚¨å°†ä½¿ç”¨
 [StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline)
 å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼Œå®ƒä¸åŒäº
 [StableDiffusionControlNetPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline)
 å› ä¸ºå®ƒå…è®¸æ‚¨ä¼ é€’åˆå§‹å›¾åƒä½œä¸ºå›¾åƒç”Ÿæˆè¿‡ç¨‹çš„èµ·ç‚¹ã€‚


åŠ è½½å›¾åƒå¹¶ä½¿ç”¨
 `æ·±åº¦ä¼°è®¡`
[ç®¡é“](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/pipelines#transformers.Pipeline)
 ä»ğŸ¤— Transformers ä¸­æå–å›¾åƒçš„æ·±åº¦å›¾ï¼š



```
import torch
import numpy as np

from transformers import pipeline
from diffusers.utils import load_image

image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg"
).resize((768, 768))


def get\_depth\_map(image, depth\_estimator):
    image = depth_estimator(image)["depth"]
    image = np.array(image)
    image = image[:, :, None]
    image = np.concatenate([image, image, image], axis=2)
    detected_map = torch.from_numpy(image).float() / 255.0
    depth_map = detected_map.permute(2, 0, 1)
    return depth_map

depth_estimator = pipeline("depth-estimation")
depth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to("cuda")
```


æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä»¥æ·±åº¦å›¾ä¸ºæ¡ä»¶çš„ ControlNet æ¨¡å‹å¹¶å°†å…¶ä¼ é€’ç»™
 [StableDiffusionControlNetImg2ImgPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetImg2ImgPipeline)
 ã€‚ä½¿ç”¨é€Ÿåº¦è¶Šå¿«
 [UniPCMultistepScheduler](/docs/diffusers/v0.23.0/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
 å¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥åŠ é€Ÿæ¨ç†å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚



```
from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/control\_v11f1p\_sd15\_depth", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
).to("cuda")

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```


ç°åœ¨å°†æç¤ºã€åˆå§‹å›¾åƒå’Œæ·±åº¦å›¾ä¼ é€’åˆ°ç®¡é“ï¼š



```
output = pipe(
    "lego batman and robin", image=image, control_image=depth_map,
).images[0]
```


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg)

 åŸå§‹å›¾åƒ


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img-2.png)

 ç”Ÿæˆçš„å›¾åƒ


## ä¿®å¤



å¯¹äºä¿®å¤ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªåˆå§‹å›¾åƒã€ä¸€ä¸ªè’™ç‰ˆå›¾åƒå’Œä¸€ä¸ªæè¿°ç”¨ä»€ä¹ˆæ›¿æ¢è’™ç‰ˆçš„æç¤ºã€‚ ControlNet æ¨¡å‹å…è®¸æ‚¨æ·»åŠ å¦ä¸€ä¸ªæ§åˆ¶å›¾åƒæ¥è°ƒèŠ‚æ¨¡å‹ã€‚è®©æˆ‘ä»¬ç”¨ç²¾æ˜çš„å›¾åƒï¼ˆé»‘è‰²èƒŒæ™¯ä¸Šçš„ç™½è‰²è½®å»“å›¾åƒï¼‰æ¥è°ƒèŠ‚æ¨¡å‹ã€‚è¿™æ ·ï¼ŒControlNetå°±å¯ä»¥ä½¿ç”¨cannyå›¾åƒä½œä¸ºæ§åˆ¶ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆå…·æœ‰ç›¸åŒè½®å»“çš„å›¾åƒã€‚


åŠ è½½åˆå§‹å›¾åƒå’Œæ©æ¨¡å›¾åƒï¼š



```
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
import numpy as np
import torch

init_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg"
)
init_image = init_image.resize((512, 512))

mask_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg"
)
mask_image = mask_image.resize((512, 512))
```


åˆ›å»ºä¸€ä¸ªå‡½æ•°ä»¥æ ¹æ®åˆå§‹å›¾åƒå’Œæ©æ¨¡å›¾åƒå‡†å¤‡æ§åˆ¶å›¾åƒã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªå¼ é‡æ¥æ ‡è®°å…¶ä¸­çš„åƒç´ 
 `åˆå§‹åŒ–æ˜ åƒ`
 å¦‚æœç›¸åº”çš„åƒç´ åœ¨
 `é®ç½©å›¾åƒ`
 è¶…è¿‡äº†ä¸€å®šçš„é˜ˆå€¼ã€‚



```
def make\_inpaint\_condition(image, image\_mask):
    image = np.array(image.convert("RGB")).astype(np.float32) / 255.0
    image_mask = np.array(image_mask.convert("L")).astype(np.float32) / 255.0

    assert image.shape[0:1] == image_mask.shape[0:1]
    image[image_mask > 0.5] = 1.0  # set as masked pixel
    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)
    image = torch.from_numpy(image)
    return image

control_image = make_inpaint_condition(init_image, mask_image)
```


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg)

 åŸå§‹å›¾åƒ


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg)

 æ©æ¨¡å›¾åƒ


åŠ è½½ä»¥ä¿®å¤ä¸ºæ¡ä»¶çš„ ControlNet æ¨¡å‹å¹¶å°†å…¶ä¼ é€’ç»™
 [StableDiffusionControlNetInpaintPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetInpaintPipeline)
 ã€‚ä½¿ç”¨é€Ÿåº¦è¶Šå¿«
 [UniPCMultistepScheduler](/docs/diffusers/v0.23.0/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
 å¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥åŠ é€Ÿæ¨ç†å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚



```
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/control\_v11p\_sd15\_inpaint", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
).to("cuda")

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```


ç°åœ¨å°†æç¤ºã€åˆå§‹å›¾åƒã€é®ç½©å›¾åƒå’Œæ§åˆ¶å›¾åƒä¼ é€’åˆ°ç®¡é“ï¼š



```
output = pipe(
    "corgi face with large ears, detailed, pixar, animated, disney",
    num_inference_steps=20,
    eta=1.0,
    image=init_image,
    mask_image=mask_image,
    control_image=control_image,
).images[0]
```


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-result.png)


## çŒœæµ‹æ¨¡å¼



[çŒœæµ‹æ¨¡å¼](https://github.com/lllyasviel/ControlNet/discussions/188)
 æ ¹æœ¬ä¸éœ€è¦å‘ ControlNet æä¾›æç¤ºï¼è¿™è¿«ä½¿ ControlNet ç¼–ç å™¨å°½åŠ›â€œçŒœæµ‹â€è¾“å…¥æ§åˆ¶å›¾çš„å†…å®¹ï¼ˆæ·±åº¦å›¾ã€å§¿æ€ä¼°è®¡ã€canny è¾¹ç¼˜ç­‰ï¼‰ã€‚


çŒœæµ‹æ¨¡å¼æ ¹æ®å—æ·±åº¦ä»¥å›ºå®šæ¯”ç‡è°ƒæ•´ ControlNet è¾“å‡ºæ®‹å·®çš„æ¯”ä¾‹ã€‚æœ€æµ…çš„
 `ä¸‹å—`
 å¯¹åº”äº0.1ï¼Œéšç€å—çš„åŠ æ·±ï¼Œè§„æ¨¡å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œä½¿å¾—
 `ä¸­å—`
 è¾“å‡ºå˜ä¸º1.0ã€‚


çŒœæµ‹æ¨¡å¼å¯¹æç¤ºè°ƒèŠ‚æ²¡æœ‰ä»»ä½•å½±å“ï¼Œå¦‚æœéœ€è¦ï¼Œæ‚¨ä»ç„¶å¯ä»¥æä¾›æç¤ºã€‚


æ”¾
 `guess_mode=True`
 åœ¨ç®¡é“ä¸­ï¼Œå®ƒæ˜¯
 [æ¨è](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode)
 è®¾ç½®
 `æŒ‡å¯¼è§„æ¨¡`
 å€¼ä»‹äº 3.0 å’Œ 5.0 ä¹‹é—´ã€‚



```
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", controlnet=controlnet, use_safetensors=True).to(
    "cuda"
)
image = pipe("", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]
image
```


![](https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0.png)

 å¸¦æç¤ºçš„å¸¸è§„æ¨¡å¼


![](https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0_gm.png)

 æ— æç¤ºçŒœæµ‹æ¨¡å¼


## ControlNet å…·æœ‰ç¨³å®šæ‰©æ•£ XL



ç›®å‰æ²¡æœ‰å¤ªå¤šä¸ Stable Diffusion XL (SDXL) å…¼å®¹çš„ ControlNet æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å·²ç»æ ¹æ®ç²¾æ˜çš„è¾¹ç¼˜æ£€æµ‹å’Œæ·±åº¦å›¾è®­ç»ƒäº†ä¸¤ä¸ªç”¨äº SDXL çš„å…¨å°ºå¯¸ ControlNet æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å°è¯•åˆ›å»ºè¿™äº›å…¼å®¹ SDXL çš„ ControlNet æ¨¡å‹çš„è¾ƒå°ç‰ˆæœ¬ï¼Œä»¥ä¾¿æ›´å®¹æ˜“åœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šè¿è¡Œã€‚æ‚¨å¯ä»¥åœ¨ ğŸ¤— ä¸Šæ‰¾åˆ°è¿™äº›æ£€æŸ¥ç‚¹
 [æ‰©æ•£å™¨](https://huggingface.co/diffusers)
 ä¸­å¿ƒç»„ç»‡ï¼


è®©æˆ‘ä»¬ä½¿ç”¨ä»¥ canny å›¾åƒä¸ºæ¡ä»¶çš„ SDXL ControlNet æ¥ç”Ÿæˆå›¾åƒã€‚é¦–å…ˆåŠ è½½å›¾åƒå¹¶å‡†å¤‡ canny å›¾åƒï¼š



```
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image
from PIL import Image
import cv2
import numpy as np

image = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd\_controlnet/hf-logo.png"
)

image = np.array(image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
canny_image
```


![](https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png)

 åŸå§‹å›¾åƒ


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hf-logo-canny.png)

 ç²¾æ˜çš„å›¾åƒ


åŠ è½½ä»¥ Canny è¾¹ç¼˜æ£€æµ‹ä¸ºæ¡ä»¶çš„ SDXL ControlNet æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™
 [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)
 ã€‚æ‚¨è¿˜å¯ä»¥å¯ç”¨æ¨¡å‹å¸è½½ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚



```
controlnet = ControlNetModel.from_pretrained(
    "diffusers/controlnet-canny-sdxl-1.0",
    torch_dtype=torch.float16,
    use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    controlnet=controlnet,
    vae=vae,
    torch_dtype=torch.float16,
    use_safetensors=True
)
pipe.enable_model_cpu_offload()
```


ç°åœ¨å°†æ‚¨çš„æç¤ºï¼ˆå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨æç¤ºï¼Œè¿˜å¯ä»¥é€‰æ‹©æ˜¯å¦å®šæç¤ºï¼‰å’Œç²¾æ˜çš„å›¾åƒä¼ é€’åˆ°ç®¡é“ï¼š


è¿™
 [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)
 å‚æ•°å†³å®šåˆ†é…ç»™è°ƒèŠ‚è¾“å…¥çš„æƒé‡ã€‚ä¸ºäº†è·å¾—è‰¯å¥½çš„æ³›åŒ–æ•ˆæœï¼Œå»ºè®®ä½¿ç”¨å€¼ 0.5ï¼Œä½†è¯·éšæ„å°è¯•è¿™ä¸ªæ•°å­—ï¼



```
prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = 'low quality, bad quality, sketches'

images = pipe(
    prompt,
    negative_prompt=negative_prompt,
    image=canny_image,
    controlnet_conditioning_scale=0.5,
).images[0]
images
```


![](https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0/resolve/main/out_hug_lab_7.png)


 æ‚¨å¯ä»¥ä½¿ç”¨
 [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)
 åœ¨çŒœæµ‹æ¨¡å¼ä¸‹ä¹Ÿå¯ä»¥é€šè¿‡å°†å‚æ•°è®¾ç½®ä¸º
 'çœŸå®'
 :



```
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image
import numpy as np
import torch

import cv2
from PIL import Image

prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = "low quality, bad quality, sketches"

image = load_image(
    "https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd\_controlnet/hf-logo.png"
)

controlnet = ControlNetModel.from_pretrained(
    "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.enable_model_cpu_offload()

image = np.array(image)
image = cv2.Canny(image, 100, 200)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe(
    prompt, controlnet_conditioning_scale=0.5, image=canny_image, guess_mode=True,
).images[0]
```


### 


 å¤šæ§åˆ¶ç½‘


å°† SDXL æ¨¡å‹æ›¿æ¢ä¸ºç±»ä¼¼æ¨¡å‹
 [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)
 é€šè¿‡ç¨³å®šæ‰©æ•£æ¨¡å‹ä½¿ç”¨å¤šä¸ªè°ƒèŠ‚è¾“å…¥ã€‚


æ‚¨å¯ä»¥æ ¹æ®ä¸åŒçš„å›¾åƒè¾“å…¥ç»„åˆå¤šä¸ª ControlNet æ¡ä»¶æ¥åˆ›å»º
 *å¤šæ§åˆ¶ç½‘ç»œ*
 ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œä»¥ä¸‹æ“ä½œé€šå¸¸ä¼šæœ‰æ‰€å¸®åŠ©ï¼š


1. é®ç›–æ¡ä»¶ï¼Œä½¿å…¶ä¸é‡å ï¼ˆä¾‹å¦‚ï¼Œé®ç›–ç²¾æ˜å›¾åƒä¸­å§¿åŠ¿æ¡ä»¶æ‰€åœ¨çš„åŒºåŸŸï¼‰
2. è¿›è¡Œå®éªŒ
 [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale)
 å‚æ•°æ¥ç¡®å®šä¸ºæ¯ä¸ªè°ƒèŠ‚è¾“å…¥åˆ†é…å¤šå°‘æƒé‡


åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæ‚¨å°†ç»„åˆç²¾æ˜å›¾åƒå’Œäººä½“å§¿åŠ¿ä¼°è®¡å›¾åƒæ¥ç”Ÿæˆæ–°å›¾åƒã€‚


å‡†å¤‡ç²¾æ˜çš„å›¾åƒè°ƒèŠ‚ï¼š



```
from diffusers.utils import load_image
from PIL import Image
import numpy as np
import cv2

canny_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"
)
canny_image = np.array(canny_image)

low_threshold = 100
high_threshold = 200

canny_image = cv2.Canny(canny_image, low_threshold, high_threshold)

# zero out middle columns of image where pose will be overlaid
zero_start = canny_image.shape[1] // 4
zero_end = zero_start + canny_image.shape[1] // 2
canny_image[:, zero_start:zero_end] = 0

canny_image = canny_image[:, :, None]
canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)
canny_image = Image.fromarray(canny_image).resize((1024, 1024))
```


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png)

 åŸå§‹å›¾åƒ


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/landscape_canny_masked.png)

 ç²¾æ˜çš„å›¾åƒ


å‡†å¤‡äººä½“å§¿åŠ¿ä¼°è®¡æ¡ä»¶ï¼š



```
from controlnet_aux import OpenposeDetector
from diffusers.utils import load_image

openpose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")

openpose_image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png"
)
openpose_image = openpose(openpose_image).resize((1024, 1024))
```


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png)

 åŸå§‹å›¾åƒ


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/person_pose.png)

 äººä½“å§¿åŠ¿å›¾åƒ


åŠ è½½ä¸æ¯ä¸ªæ¡ä»¶å¯¹åº”çš„ ControlNet æ¨¡å‹åˆ—è¡¨ï¼Œå¹¶å°†å®ƒä»¬ä¼ é€’ç»™
 [StableDiffusionXLControlNetPipeline](/docs/diffusers/v0.23.0/en/api/pipelines/controlnet_sdxl#diffusers.StableDiffusionXLControlNetPipeline)
 ã€‚ä½¿ç”¨é€Ÿåº¦è¶Šå¿«
 [UniPCMultistepScheduler](/docs/diffusers/v0.23.0/en/api/schedulers/unipc#diffusers.UniPCMultistepScheduler)
 å¹¶å¯ç”¨æ¨¡å‹å¸è½½ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚



```
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, UniPCMultistepScheduler
import torch

controlnets = [
    ControlNetModel.from_pretrained(
        "thibaud/controlnet-openpose-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
    ),
    ControlNetModel.from_pretrained(
        "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
    ),
]

vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnets, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```


ç°åœ¨ï¼Œæ‚¨å¯ä»¥å°†æç¤ºï¼ˆå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨æç¤ºï¼Œåˆ™å¯ä»¥é€‰æ‹©å¦å®šæç¤ºï¼‰ã€ç²¾æ˜å›¾åƒå¹¶å°†å›¾åƒæ‘†åœ¨ç®¡é“ä¸­ï¼š



```
prompt = "a giant standing in a fantasy landscape, best quality"
negative_prompt = "monochrome, lowres, bad anatomy, worst quality, low quality"

generator = torch.manual_seed(1)

images = [openpose_image, canny_image]

images = pipe(
    prompt,
    image=images,
    num_inference_steps=25,
    generator=generator,
    negative_prompt=negative_prompt,
    num_images_per_prompt=3,
    controlnet_conditioning_scale=[1.0, 0.8],
).images[0]
```


![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/multicontrolnet.png)