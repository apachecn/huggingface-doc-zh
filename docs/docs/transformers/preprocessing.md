# é¢„å¤„ç†

> è¯‘è€…ï¼š[BrightLi](https://github.com/brightli)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/transformers/preprocessing>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/transformers/preprocessing>

åœ¨ä½ å¯ä»¥å¯¹æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒä¹‹å‰ï¼Œæ•°æ®éœ€è¦é¢„å¤„ç†ä¸ºæ¨¡å‹æ‰€æœŸæœ›çš„è¾“å…¥æ ¼å¼ã€‚æ— è®ºä½ çš„æ•°æ®æ˜¯æ–‡æœ¬ã€å›¾åƒè¿˜æ˜¯éŸ³é¢‘ï¼Œå®ƒä»¬éƒ½éœ€è¦è¢«è½¬æ¢å¹¶ç»„è£…æˆå¼ é‡çš„æ‰¹æ¬¡ã€‚ğŸ¤— Transformers æä¾›äº†ä¸€ç»„é¢„å¤„ç†ç±»æ¥å¸®åŠ©å‡†å¤‡ä½ çš„æ•°æ®ç”¨äºæ¨¡å‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œä½ å°†å­¦ä¹ ä»¥ä¸‹å†…å®¹ï¼š

* å¯¹äºæ–‡æœ¬ï¼Œä½¿ç”¨[Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç³»åˆ—çš„æ ‡è®°ï¼Œåˆ›å»ºæ ‡è®°çš„æ•°å€¼è¡¨ç¤ºï¼Œå¹¶å°†å®ƒä»¬ç»„è£…æˆå¼ é‡ã€‚
* å¯¹äºè¯­éŸ³å’ŒéŸ³é¢‘ï¼Œä½¿ç”¨[Feature Extractor](https://huggingface.co/docs/transformers/main_classes/feature_extractor)ä»éŸ³é¢‘æ³¢å½¢ä¸­æå–åºåˆ—ç‰¹å¾å¹¶å°†å…¶è½¬æ¢æˆå¼ é‡ã€‚
* å¯¹äºå›¾åƒè¾“å…¥ï¼Œä½¿ç”¨[ImageProcessor](https://huggingface.co/docs/transformers/main_classes/image_processor)å°†å›¾åƒè½¬æ¢æˆå¼ é‡ã€‚
* å¯¹äºå¤šæ¨¡æ€è¾“å…¥ï¼Œä½¿ç”¨[Processor](https://huggingface.co/docs/transformers/main_classes/processors)ç»“åˆä¸€ä¸ª tokenizer å’Œä¸€ä¸ª feature extractor æˆ– image processorã€‚



> AutoProcessor æ€»æ˜¯æœ‰æ•ˆï¼Œå¹¶ä¸”ä¼šè‡ªåŠ¨ä¸ºä½ æ‰€ä½¿ç”¨çš„æ¨¡å‹é€‰æ‹©æ­£ç¡®çš„ç±»ï¼Œæ— è®ºä½ æ˜¯ä½¿ç”¨ tokenizerã€image processorã€feature extractor è¿˜æ˜¯ processorã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œå®‰è£… ğŸ¤— Datasets ä»¥ä¾¿ä½ å¯ä»¥åŠ è½½ä¸€äº›æ•°æ®é›†è¿›è¡Œå®éªŒï¼š

```py
pip install datasets
```

**è‡ªç„¶è¯­è¨€å¤„ç†**


<iframe width="446" height="320" src="https://www.youtube.com/embed/Yffk5aydLzg" title="The tokenization pipeline" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


é¢„å¤„ç†æ–‡æœ¬æ•°æ®çš„ä¸»è¦å·¥å…·æ˜¯[åˆ†è¯å™¨](https://huggingface.co/docs/transformers/main_classes/tokenizer)ã€‚åˆ†è¯å™¨æ ¹æ®ä¸€ç»„è§„åˆ™å°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ ‡è®°ã€‚è¿™äº›æ ‡è®°è¢«è½¬æ¢ä¸ºæ•°å­—ï¼Œç„¶åæ˜¯å¼ é‡ï¼Œæˆä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚æ¨¡å‹æ‰€éœ€çš„ä»»ä½•é¢å¤–è¾“å…¥éƒ½ç”±åˆ†è¯å™¨æ·»åŠ ã€‚


> å¦‚æœæ‚¨è®¡åˆ’ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨ç›¸å…³çš„é¢„è®­ç»ƒåˆ†è¯å™¨å¾ˆé‡è¦ã€‚è¿™ç¡®ä¿äº†æ–‡æœ¬æŒ‰ç…§ä¸é¢„è®­ç»ƒè¯­æ–™åº“ç›¸åŒçš„æ–¹å¼æ‹†åˆ†ï¼Œå¹¶åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨ç›¸åŒçš„å¯¹åº”æ ‡è®°åˆ°ç´¢å¼•ï¼ˆé€šå¸¸ç§°ä¸ºè¯æ±‡è¡¨ï¼‰ã€‚


é€šè¿‡ä½¿ç”¨[AutoTokenizer.from_pretrained()](https://huggingface.co/docs/transformers/v4.41.3/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained)æ–¹æ³•åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨æ¥å¼€å§‹ã€‚è¿™å°†ä¸‹è½½æ¨¡å‹é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„è¯æ±‡è¡¨ï¼š


```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

å°†æ‚¨çš„æ–‡æœ¬ä¼ é€’ç»™åˆ†è¯å™¨ï¼š

```py
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

åˆ†è¯å™¨è¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé‡è¦é¡¹ç›®çš„å­—å…¸ï¼š

* [input_ids](https://huggingface.co/docs/transformers/glossary#input-ids) æ˜¯å¥å­ä¸­æ¯ä¸ªæ ‡è®°å¯¹åº”ç´¢å¼•ã€‚

* [attention_mask](https://huggingface.co/docs/transformers/glossary#attention-mask) æŒ‡ç¤ºæ˜¯å¦åº”è¯¥å…³æ³¨æŸä¸ªæ ‡è®°ã€‚

* [token_type_ids](https://huggingface.co/docs/transformers/glossary#token-type-ids) åœ¨æœ‰å¤šä¸ªåºåˆ—æ—¶ï¼Œæ ‡è¯†æ ‡è®°å±äºå“ªä¸ªåºåˆ—ã€‚

é€šè¿‡è§£ç  input_ids è¿”å›æ‚¨çš„è¾“å…¥ï¼š

```py
tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

å¦‚æ‚¨æ‰€è§ï¼Œåˆ†è¯å™¨åœ¨å¥å­ä¸­æ·»åŠ äº†ä¸¤ä¸ªç‰¹æ®Šæ ‡è®° - CLS å’Œ SEPï¼ˆåˆ†ç±»å™¨å’Œåˆ†éš”ç¬¦ï¼‰ã€‚å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½éœ€è¦ç‰¹æ®Šæ ‡è®°ï¼Œä½†å¦‚æœéœ€è¦ï¼Œåˆ†è¯å™¨ä¼šè‡ªåŠ¨ä¸ºæ‚¨æ·»åŠ å®ƒä»¬ã€‚

å¦‚æœæ‚¨æƒ³è¦é¢„å¤„ç†å‡ ä¸ªå¥å­ï¼Œå°†å®ƒä»¬ä½œä¸ºä¸€ä¸ªåˆ—è¡¨ä¼ é€’ç»™åˆ†è¯å™¨ï¼š

```py
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1]]}
```

**å¡«å……**

å¥å­çš„é•¿åº¦å¹¶ä¸æ€»æ˜¯ç›¸åŒçš„ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ¨¡å‹çš„è¾“å…¥å¼ é‡éœ€è¦å…·æœ‰ç»Ÿä¸€çš„å½¢çŠ¶ã€‚å¡«å……æ˜¯ä¸€ç§ç­–ç•¥ï¼Œé€šè¿‡å‘è¾ƒçŸ­çš„å¥å­æ·»åŠ ç‰¹æ®Šçš„å¡«å……æ ‡è®°æ¥ç¡®ä¿å¼ é‡æ˜¯çŸ©å½¢çš„ã€‚

å°†å¡«å……å‚æ•°è®¾ç½®ä¸ºTrueï¼Œä»¥åœ¨æ‰¹æ¬¡ä¸­å¯¹è¾ƒçŸ­çš„åºåˆ—è¿›è¡Œå¡«å……ï¼Œä½¿å…¶ä¸æœ€é•¿çš„åºåˆ—ç›¸åŒ¹é…ï¼š

```py
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True)
print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

ç¬¬ä¸€å¥å’Œç¬¬ä¸‰å¥ç°åœ¨ç”¨0è¿›è¡Œäº†å¡«å……ï¼Œå› ä¸ºå®ƒä»¬æ¯”è¾ƒçŸ­ã€‚

**æˆªæ–­**


åœ¨å¦ä¸€ç§æƒ…å†µä¸‹ï¼Œæœ‰æ—¶ä¸€ä¸ªåºåˆ—å¯èƒ½è¿‡é•¿ï¼Œæ¨¡å‹æ— æ³•å¤„ç†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦å°†åºåˆ—æˆªæ–­ä¸ºæ›´çŸ­çš„é•¿åº¦ã€‚


å°†æˆªæ–­å‚æ•°è®¾ç½®ä¸ºTrueï¼Œå¯å°†åºåˆ—æˆªæ–­ä¸ºæ¨¡å‹å¯æ¥å—çš„æœ€å¤§é•¿åº¦ï¼š


```py
>>> batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```


> æŸ¥çœ‹[å¡«å……å’Œæˆªæ–­](https://huggingface.co/docs/transformers/pad_truncation)æ¦‚å¿µæŒ‡å—ï¼Œä»¥äº†è§£æ›´å¤šä¸åŒçš„å¡«å……å’Œæˆªæ–­å‚æ•°ã€‚

**æ„å»ºå¼ é‡**

æœ€åï¼Œä½ å¸Œæœ›tokenizerè¿”å›å®é™…è¾“å…¥åˆ°æ¨¡å‹çš„å¼ é‡ã€‚

å°†return_tensorså‚æ•°è®¾ç½®ä¸ºptï¼ˆé’ˆå¯¹PyTorchï¼‰æˆ–tfï¼ˆé’ˆå¯¹TensorFlowï¼‰ï¼š

```py
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
print(encoded_input)
{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
      dtype=int32)>,
 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,
 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}
```

> ä¸åŒçš„ç®¡é“åœ¨å…¶__call__()æ–¹æ³•ä¸­æ”¯æŒtokenizerå‚æ•°çš„æ–¹å¼ä¸åŒã€‚â€œæ–‡æœ¬åˆ°æ–‡æœ¬ç”Ÿæˆâ€ç®¡é“åªæ”¯æŒï¼ˆå³ä¼ é€’ï¼‰truncationå‚æ•°ã€‚â€œæ–‡æœ¬ç”Ÿæˆâ€ç®¡é“æ”¯æŒmax_lengthã€truncationã€paddingå’Œadd_special_tokenså‚æ•°ã€‚åœ¨â€œå¡«å……æ©ç â€ç®¡é“ä¸­ï¼Œå¯ä»¥é€šè¿‡tokenizer_kwargså‚æ•°ï¼ˆå­—å…¸å½¢å¼ï¼‰ä¼ é€’tokenizerå‚æ•°ã€‚

**éŸ³é¢‘**

å¯¹äºéŸ³é¢‘ä»»åŠ¡ï¼Œä½ éœ€è¦ä¸€ä¸ª[ç‰¹å¾æå–å™¨](https://huggingface.co/docs/transformers/main_classes/feature_extractor)æ¥å‡†å¤‡æ¨¡å‹æ‰€éœ€çš„æ•°æ®é›†ã€‚ç‰¹å¾æå–å™¨æ—¨åœ¨ä»åŸå§‹éŸ³é¢‘æ•°æ®ä¸­æå–ç‰¹å¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå¼ é‡ã€‚

åŠ è½½[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… ğŸ¤— [Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨ç‰¹å¾æå–å™¨å¤„ç†éŸ³é¢‘æ•°æ®é›†ï¼š

è®¿é—®éŸ³é¢‘åˆ—çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä»¥æŸ¥çœ‹è¾“å…¥ã€‚è°ƒç”¨éŸ³é¢‘åˆ—ä¼šè‡ªåŠ¨åŠ è½½å¹¶é‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š

```py
>>> dataset[0]["audio"]
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 8000}
```

è¿™å°†è¿”å›ä¸‰ä¸ªé¡¹ç›®ï¼š

* array æ˜¯å°†è¯­éŸ³ä¿¡å·åŠ è½½ï¼ˆå¹¶å¯èƒ½é‡æ–°é‡‡æ ·ï¼‰ä¸º1Dæ•°ç»„ã€‚
  
* path æŒ‡å‘éŸ³é¢‘æ–‡ä»¶çš„ä½ç½®ã€‚
  
* sampling_rate æŒ‡çš„æ˜¯æ¯ç§’æµ‹é‡çš„è¯­éŸ³ä¿¡å·ä¸­çš„æ•°æ®ç‚¹æ•°ã€‚

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œä½ å°†ä½¿ç”¨[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)æ¨¡å‹ã€‚æŸ¥çœ‹æ¨¡å‹å¡ç‰‡ï¼Œä½ ä¼šäº†è§£åˆ°Wav2Vec2æ˜¯åœ¨16kHzé‡‡æ ·çš„è¯­éŸ³éŸ³é¢‘ä¸Šé¢„è®­ç»ƒçš„ã€‚ç¡®ä¿ä½ çš„éŸ³é¢‘æ•°æ®çš„é‡‡æ ·ç‡ä¸ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†çš„é‡‡æ ·ç‡ç›¸åŒ¹é…æ˜¯å¾ˆé‡è¦çš„ã€‚å¦‚æœä½ çš„æ•°æ®çš„é‡‡æ ·ç‡ä¸åŒï¼Œé‚£ä¹ˆä½ éœ€è¦å¯¹ä½ çš„æ•°æ®è¿›è¡Œé‡æ–°é‡‡æ ·ã€‚

1.ä½¿ç”¨ğŸ¤— Datasetsçš„cast_columnæ–¹æ³•å°†é‡‡æ ·ç‡ä¸Šé‡‡æ ·åˆ°16kHzï¼š

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

å†æ¬¡è°ƒç”¨éŸ³é¢‘åˆ—ä»¥é‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š

```py
>>> dataset[0]["audio"]
{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 16000}
```

æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ªç‰¹å¾æå–å™¨æ¥æ ‡å‡†åŒ–å’Œå¡«å……è¾“å…¥ã€‚åœ¨å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œå¡«å……æ—¶ï¼Œå¯¹äºè¾ƒçŸ­çš„åºåˆ—ä¼šæ·»åŠ 0ã€‚åŒæ ·çš„æ–¹æ³•é€‚ç”¨äºéŸ³é¢‘æ•°æ®ã€‚ç‰¹å¾æå–å™¨ä¼šåœ¨æ•°ç»„ä¸­æ·»åŠ 0 - è¢«è§£é‡Šä¸ºé™éŸ³ã€‚

ä½¿ç”¨[AutoFeatureExtractor.from_pretrained()](https://huggingface.co/docs/transformers/v4.41.3/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained)åŠ è½½ç‰¹å¾æå–å™¨ï¼š


```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

å°†éŸ³é¢‘æ•°ç»„ä¼ é€’ç»™ç‰¹å¾æå–å™¨ã€‚æˆ‘ä»¬è¿˜å»ºè®®åœ¨ç‰¹å¾æå–å™¨ä¸­æ·»åŠ sampling_rateå‚æ•°ï¼Œä»¥ä¾¿æ›´å¥½åœ°è°ƒè¯•å¯èƒ½å‘ç”Ÿçš„ä»»ä½•é™é»˜é”™è¯¯ã€‚

```py
>>> audio_input = [dataset[0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,
        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}
```

å°±åƒtokenizerä¸€æ ·ï¼Œä½ å¯ä»¥åº”ç”¨å¡«å……æˆ–æˆªæ–­æ¥å¤„ç†æ‰¹æ¬¡ä¸­çš„å¯å˜åºåˆ—ã€‚çœ‹çœ‹è¿™ä¸¤ä¸ªéŸ³é¢‘æ ·æœ¬çš„åºåˆ—é•¿åº¦ï¼š

```py
>>> dataset[0]["audio"]["array"].shape
(173398,)
>>> dataset[1]["audio"]["array"].shape
(106496,)
```

åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é¢„å¤„ç†æ•°æ®é›†ï¼Œä½¿éŸ³é¢‘æ ·æœ¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚æŒ‡å®šä¸€ä¸ªæœ€å¤§æ ·æœ¬é•¿åº¦ï¼Œç‰¹å¾æå–å™¨å°†å¡«å……æˆ–æˆªæ–­åºåˆ—ä»¥åŒ¹é…å®ƒï¼š

```py
def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16000,
        padding=True,
        max_length=100000,
        truncation=True,
    )
    return inputs
```

å°†preprocess_functionåº”ç”¨äºæ•°æ®é›†ä¸­çš„å‰å‡ ä¸ªç¤ºä¾‹ï¼š

```py
>>> processed_dataset = preprocess_function(dataset[:5])
```

æ ·æœ¬é•¿åº¦ç°åœ¨ç›¸åŒï¼Œå¹¶ä¸”ä¸æŒ‡å®šçš„æœ€å¤§é•¿åº¦åŒ¹é…ã€‚ä½ ç°åœ¨å¯ä»¥å°†å¤„ç†è¿‡çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼

```py
>>> processed_dataset["input_values"][0].shape
(100000,)
>>> processed_dataset["input_values"][1].shape
(100000,)
```

**è®¡ç®—æœºè§†è§‰**

å¯¹äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œä½ éœ€è¦ä¸€ä¸ª[å›¾åƒå¤„ç†å™¨](https://huggingface.co/docs/transformers/main_classes/image_processor)æ¥å‡†å¤‡æ¨¡å‹æ‰€éœ€çš„æ•°æ®é›†ã€‚å›¾åƒé¢„å¤„ç†åŒ…æ‹¬å‡ ä¸ªæ­¥éª¤ï¼Œè¿™äº›æ­¥éª¤å°†å›¾åƒè½¬æ¢ä¸ºæ¨¡å‹æ‰€æœŸæœ›çš„è¾“å…¥ã€‚è¿™äº›æ­¥éª¤åŒ…æ‹¬ä½†ä¸é™äºè°ƒæ•´å¤§å°ã€å½’ä¸€åŒ–ã€é¢œè‰²é€šé“æ ¡æ­£å’Œå°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚

> å›¾åƒé¢„å¤„ç†é€šå¸¸ä¼´éšç€æŸç§å½¢å¼çš„å›¾åƒå¢å¼ºã€‚å›¾åƒé¢„å¤„ç†å’Œå›¾åƒå¢å¼ºéƒ½ä¼šè½¬æ¢å›¾åƒæ•°æ®ï¼Œä½†å®ƒä»¬æœåŠ¡äºä¸åŒçš„ç›®çš„ï¼š
å›¾åƒå¢å¼ºä»¥å¯ä»¥å¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆå’Œæé«˜æ¨¡å‹çš„é²æ£’æ€§çš„æ–¹å¼æ”¹å˜å›¾åƒã€‚ä½ å¯ä»¥åœ¨å¢å¼ºæ•°æ®æ—¶å‘æŒ¥åˆ›æ„ - è°ƒæ•´äº®åº¦å’Œé¢œè‰²ã€è£å‰ªã€æ—‹è½¬ã€è°ƒæ•´å¤§å°ã€ç¼©æ”¾ç­‰ã€‚ç„¶è€Œï¼Œè¦æ³¨æ„ä¸è¦é€šè¿‡ä½ çš„å¢å¼ºæ”¹å˜å›¾åƒçš„æ„ä¹‰ã€‚
å›¾åƒé¢„å¤„ç†ä¿è¯å›¾åƒä¸æ¨¡å‹æœŸæœ›çš„è¾“å…¥æ ¼å¼ç›¸åŒ¹é…ã€‚åœ¨å¾®è°ƒè®¡ç®—æœºè§†è§‰æ¨¡å‹æ—¶ï¼Œå¿…é¡»åƒæ¨¡å‹æœ€åˆè®­ç»ƒæ—¶é‚£æ ·ç²¾ç¡®åœ°è¿›è¡Œå›¾åƒé¢„å¤„ç†ã€‚
ä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„ä»»ä½•åº“è¿›è¡Œå›¾åƒå¢å¼ºã€‚å¯¹äºå›¾åƒé¢„å¤„ç†ï¼Œä½¿ç”¨ä¸æ¨¡å‹ç›¸å…³çš„ImageProcessorã€‚

åŠ è½½[food101](https://huggingface.co/datasets/food101)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… ğŸ¤— [Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨å›¾åƒå¤„ç†å™¨å¤„ç†è®¡ç®—æœºè§†è§‰æ•°æ®é›†ï¼š

> ä½¿ç”¨ğŸ¤— Datasetsçš„splitå‚æ•°ä»…ä»è®­ç»ƒæ‹†åˆ†ä¸­åŠ è½½ä¸€ä¸ªå°æ ·æœ¬ï¼Œå› ä¸ºè¿™ä¸ªæ•°æ®é›†ç›¸å½“å¤§ï¼

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split="train[:100]")
```

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ğŸ¤— Datasetsçš„[Image](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)ç‰¹æ€§æŸ¥çœ‹å›¾åƒï¼š

```py
dataset[0]["image"]
```

<p style="text-align: center;">
<img src='https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png' width=50% />
</p>

ä½¿ç”¨[AutoImageProcessor.from_pretrained()](https://huggingface.co/docs/transformers/v4.41.3/en/model_doc/auto#transformers.AutoImageProcessor.from_pretrained)åŠ è½½å›¾åƒå¤„ç†å™¨ï¼š

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ·»åŠ ä¸€äº›å›¾åƒå¢å¼ºã€‚ä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„ä»»ä½•åº“ï¼Œä½†åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨torchvisionçš„[transforms](https://pytorch.org/vision/stable/transforms.html)æ¨¡å—ã€‚å¦‚æœä½ å¯¹ä½¿ç”¨å¦ä¸€ä¸ªæ•°æ®å¢å¼ºåº“æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨[Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)æˆ–[Korniaç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)ä¸­å­¦ä¹ å¦‚ä½•ä½¿ç”¨ã€‚

1.åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨[Compose](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)å°†å‡ ä¸ªè½¬æ¢é“¾æ¥åœ¨ä¸€èµ· - [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)å’Œ[ColorJitter](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºè°ƒæ•´å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥ä»image_processorè·å–å›¾åƒå¤§å°è¦æ±‚ã€‚å¯¹äºä¸€äº›æ¨¡å‹ï¼Œéœ€è¦ç¡®åˆ‡çš„é«˜åº¦å’Œå®½åº¦ï¼Œè€Œå¯¹äºå…¶ä»–æ¨¡å‹ï¼Œåªå®šä¹‰äº†æœ€çŸ­è¾¹ã€‚

```py
>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose

>>> size = (
    image_processor.size["shortest_edge"]
    if "shortest_edge" in image_processor.size
    else (image_processor.size["height"], image_processor.size["width"])
)

>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])
```

2.æ¨¡å‹æ¥å—[pixel_values](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel.forward.pixel_values)ä½œä¸ºå…¶è¾“å…¥ã€‚ImageProcessorå¯ä»¥è´Ÿè´£å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶ç”Ÿæˆé€‚å½“çš„å¼ é‡ã€‚åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†å›¾åƒå¢å¼ºå’Œå›¾åƒé¢„å¤„ç†ç»“åˆèµ·æ¥ï¼Œä¸ºä¸€æ‰¹å›¾åƒç”Ÿæˆpixel_valuesï¼š

```py
def transforms(examples):
    images = [_transforms(img.convert("RGB")) for img in examples["image"]]
    examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"]
    return examples
```

> åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†do_resizeè®¾ç½®ä¸ºFalseï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨å›¾åƒå¢å¼ºè½¬æ¢ä¸­è°ƒæ•´äº†å›¾åƒçš„å¤§å°ï¼Œå¹¶åˆ©ç”¨äº†é€‚å½“çš„image_processorçš„sizeå±æ€§ã€‚å¦‚æœåœ¨å›¾åƒå¢å¼ºè¿‡ç¨‹ä¸­ä¸è°ƒæ•´å›¾åƒå¤§å°ï¼Œå¯ä»¥çœç•¥æ­¤å‚æ•°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒImageProcessorä¼šå¤„ç†è°ƒæ•´å¤§å°ã€‚
å¦‚æœå¸Œæœ›å°†å›¾åƒå½’ä¸€åŒ–ä½œä¸ºå¢å¼ºè½¬æ¢çš„ä¸€éƒ¨åˆ†ï¼Œä½¿ç”¨image_processor.image_meanå’Œimage_processor.image_stdå€¼ã€‚

3.ç„¶åä½¿ç”¨ğŸ¤— Datasetsçš„set_transformæ–¹æ³•å®æ—¶åº”ç”¨è½¬æ¢ï¼š

```py
>>> dataset.set_transform(transforms)
```

4.ç°åœ¨å½“ä½ è®¿é—®å›¾åƒæ—¶ï¼Œä½ ä¼šæ³¨æ„åˆ°å›¾åƒå¤„ç†å™¨å·²ç»æ·»åŠ äº†pixel_valuesã€‚ç°åœ¨ä½ å¯ä»¥å°†å¤„ç†è¿‡çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼

```py
>>> dataset[0].keys()
```

ä»¥ä¸‹æ˜¯åº”ç”¨è½¬æ¢åå›¾åƒçš„æ ·å­ã€‚å›¾åƒå·²è¢«éšæœºè£å‰ªï¼Œå…¶é¢œè‰²å±æ€§ä¹Ÿæœ‰æ‰€ä¸åŒã€‚

```py
>>> import numpy as np
>>> import matplotlib.pyplot as plt

>>> img = dataset[0]["pixel_values"]
>>> plt.imshow(img.permute(1, 2, 0))
```

<p style="text-align: center;">
<img src='https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png' width=40% />
</p>

> å¯¹äºåƒç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²è¿™æ ·çš„ä»»åŠ¡ï¼ŒImageProcessoræä¾›äº†åå¤„ç†æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å°†æ¨¡å‹çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºæœ‰æ„ä¹‰çš„é¢„æµ‹ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–åˆ†å‰²å›¾ã€‚

**å¡«å……**

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚åœ¨å¾®è°ƒ[DETR](https://huggingface.co/docs/transformers/model_doc/detr)æ—¶ï¼Œæ¨¡å‹ä¼šåœ¨è®­ç»ƒæ—¶åº”ç”¨å°ºåº¦å¢å¼ºã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´æ‰¹å¤„ç†ä¸­çš„å›¾åƒå¤§å°ä¸åŒã€‚æ‚¨å¯ä»¥ä½¿ç”¨[DetrImageProcessor](https://huggingface.co/docs/transformers/v4.41.3/en/model_doc/detr#transformers.DetrImageProcessor)çš„DetrImageProcessor.pad()æ–¹æ³•å¹¶å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„collate_fnæ¥å°†å›¾åƒç»„åˆåœ¨ä¸€èµ·ã€‚

```py
def collate_fn(batch):
    pixel_values = [item["pixel_values"] for item in batch]
    encoding = image_processor.pad(pixel_values, return_tensors="pt")
    labels = [item["labels"] for item in batch]
    batch = {}
    batch["pixel_values"] = encoding["pixel_values"]
    batch["pixel_mask"] = encoding["pixel_mask"]
    batch["labels"] = labels
    return batch
```

**å¤šæ¨¡æ€**

å¯¹äºæ¶‰åŠå¤šæ¨¡æ€è¾“å…¥çš„ä»»åŠ¡ï¼Œæ‚¨éœ€è¦ä¸€ä¸ª[å¤„ç†å™¨](https://huggingface.co/docs/transformers/main_classes/processors)æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®é›†ã€‚ä¸€ä¸ªå¤„ç†å™¨å°†ä¸¤ä¸ªå¤„ç†å¯¹è±¡ï¼ˆå¦‚tokenizerå’Œfeature extractorï¼‰ç»“åˆåœ¨ä¸€èµ·ã€‚

åŠ è½½[LJ Speech](https://huggingface.co/datasets/lj_speech)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨å¤„ç†å™¨è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼š

```py
>>> from datasets import load_dataset

>>> lj_speech = load_dataset("lj_speech", split="train")
```

å¯¹äºASRï¼Œæ‚¨ä¸»è¦å…³æ³¨éŸ³é¢‘å’Œæ–‡æœ¬ï¼Œå› æ­¤å¯ä»¥åˆ é™¤å…¶ä»–åˆ—ï¼š

```py
lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])
```

ç°åœ¨æŸ¥çœ‹éŸ³é¢‘å’Œæ–‡æœ¬åˆ—ï¼š

```py
>>> lj_speech[0]["audio"]
{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,
         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',
 'sampling_rate': 22050}
>>> lj_speech[0]["text"]
'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'
```

è¯·è®°ä½ï¼Œæ‚¨åº”è¯¥å§‹ç»ˆ[é‡æ–°é‡‡æ ·](https://huggingface.co/docs/transformers/preprocessing#audio)éŸ³é¢‘æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼Œä»¥åŒ¹é…ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼

```py
>>> lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))
```

ä½¿ç”¨[AutoProcessor.from_pretrained()](https://huggingface.co/docs/transformers/v4.41.3/en/model_doc/auto#transformers.AutoProcessor.from_pretrained)åŠ è½½å¤„ç†å™¨ï¼š

```py
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

1.åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†åŒ…å«åœ¨æ•°ç»„ä¸­çš„éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºinput_valuesï¼Œå¹¶å°†æ–‡æœ¬æ ‡è®°åŒ–ä¸ºlabelsã€‚è¿™äº›æ˜¯æ¨¡å‹çš„è¾“å…¥ï¼š

```py
>>> def prepare_dataset(example):
    audio = example["audio"]

    example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

    return example
```

2.å°†prepare_datasetå‡½æ•°åº”ç”¨äºä¸€ä¸ªæ ·æœ¬ï¼š

```py
>>> prepare_dataset(lj_speech[0])
```

ç°åœ¨ï¼Œå¤„ç†å™¨å·²ç»æ·»åŠ äº†input_valueså’Œlabelsï¼Œå¹¶ä¸”é‡‡æ ·ç‡ä¹Ÿè¢«æ­£ç¡®åœ°é™ä½åˆ°äº†16kHzã€‚ç°åœ¨ä½ å¯ä»¥å°†å¤„ç†è¿‡çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼