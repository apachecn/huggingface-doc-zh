# åˆ†è¯å™¨

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/transformers/main_classes/tokenizer>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/transformers/main_classes/tokenizer>


åˆ†è¯å™¨è´Ÿè´£å‡†å¤‡æ¨¡å‹çš„è¾“å…¥ã€‚è¯¥åº“åŒ…å«æ‰€æœ‰æ¨¡å‹çš„æ ‡è®°å™¨ã€‚æœ€å¤š
åˆ†è¯å™¨æœ‰ä¸¤ç§å½¢å¼ï¼šå®Œæ•´çš„ python å®ç°å’ŒåŸºäº
Rust åº“
 [ğŸ¤— æ ‡è®°å™¨](https://github.com/huggingface/tokenizers)
 ã€‚ â€œå¿«é€Ÿâ€å®æ–½å…è®¸ï¼š


1. æ˜¾ç€çš„åŠ é€Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨è¿›è¡Œæ‰¹é‡æ ‡è®°åŒ–æ—¶
2. åœ¨åŸå§‹å­—ç¬¦ä¸²ï¼ˆå­—ç¬¦å’Œå•è¯ï¼‰å’Œæ ‡è®°ç©ºé—´ä¹‹é—´è¿›è¡Œæ˜ å°„çš„å…¶ä»–æ–¹æ³•ï¼ˆä¾‹å¦‚è·å–
åŒ…å«ç»™å®šå­—ç¬¦çš„æ ‡è®°çš„ç´¢å¼•æˆ–ä¸ç»™å®šæ ‡è®°ç›¸å¯¹åº”çš„å­—ç¬¦èŒƒå›´ï¼‰ã€‚


åŸºç±»
 [PreTrainedTokenizer](/docs/transformers/v4.35.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
 å’Œ
 [PreTrainedTokenizerFast](/docs/transformers/v4.35.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
 å®ç°åœ¨æ¨¡å‹è¾“å…¥ä¸­ç¼–ç å­—ç¬¦ä¸²è¾“å…¥çš„å¸¸ç”¨æ–¹æ³•ï¼ˆè§ä¸‹æ–‡ï¼‰ä»¥åŠå®ä¾‹åŒ–/ä¿å­˜ python å’Œ
æ¥è‡ªæœ¬åœ°æ–‡ä»¶æˆ–ç›®å½•æˆ–æ¥è‡ªåº“æä¾›çš„é¢„è®­ç»ƒæ ‡è®°ç”Ÿæˆå™¨çš„â€œå¿«é€Ÿâ€æ ‡è®°ç”Ÿæˆå™¨
ï¼ˆä» HuggingFace çš„ AWS S3 å­˜å‚¨åº“ä¸‹è½½ï¼‰ã€‚ä»–ä»¬ä¿©éƒ½ä¾é 
 [PreTrainedTokenizerBase](/docs/transformers/v4.35.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)
 åŒ…å«å¸¸ç”¨æ–¹æ³•ï¼Œä»¥åŠ
 [SpecialTokensMixin](/docs/transformers/v4.35.2/en/internal/tokenization_utils#transformers.SpecialTokensMixin)
 ã€‚


[PreTrainedTokenizer](/docs/transformers/v4.35.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
 å’Œ
 [PreTrainedTokenizerFast](/docs/transformers/v4.35.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
 ä»è€Œå®ç°ä¸»è¦
ä½¿ç”¨æ‰€æœ‰åˆ†è¯å™¨çš„æ–¹æ³•ï¼š


* æ ‡è®°åŒ–ï¼ˆåœ¨å­è¯æ ‡è®°å­—ç¬¦ä¸²ä¸­åˆ†å‰²å­—ç¬¦ä¸²ï¼‰ï¼Œå°†æ ‡è®°å­—ç¬¦ä¸²è½¬æ¢ä¸º id å¹¶è¿”å›ï¼Œä»¥åŠ
ç¼–ç /è§£ç ï¼ˆå³æ ‡è®°åŒ–å¹¶è½¬æ¢ä¸ºæ•´æ•°ï¼‰ã€‚
* ä»¥ç‹¬ç«‹äºåº•å±‚ç»“æ„ï¼ˆBPEã€SentencePiece...ï¼‰çš„æ–¹å¼å‘è¯æ±‡è¡¨æ·»åŠ æ–°æ ‡è®°ã€‚
* ç®¡ç†ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚æ©ç ã€å¥å­å¼€å¤´ç­‰ï¼‰ï¼šæ·»åŠ å®ƒä»¬ï¼Œå°†å®ƒä»¬åˆ†é…ç»™å±æ€§ä¸­
åˆ†è¯å™¨å¯è½»æ¾è®¿é—®å¹¶ç¡®ä¿å®ƒä»¬åœ¨åˆ†è¯è¿‡ç¨‹ä¸­ä¸ä¼šè¢«åˆ†å‰²ã€‚


[BatchEncoding](/docs/transformers/v4.35.2/en/main_classes/tokenizer#transformers.BatchEncoding)
 ä¿å­˜çš„è¾“å‡º
 [PreTrainedTokenizerBase](/docs/transformers/v4.35.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)
 çš„ç¼–ç æ–¹æ³•ï¼ˆ
 `__call__`
 ,
 `ç¼–ç åŠ `
 å’Œ
 `batch_encode_plus`
 ï¼‰å¹¶ä¸”æºè‡ª Python å­—å…¸ã€‚å½“åˆ†è¯å™¨æ˜¯çº¯Pythonæ—¶
tokenizerï¼Œè¿™ä¸ªç±»çš„è¡Œä¸ºå°±åƒä¸€ä¸ªæ ‡å‡†çš„ python å­—å…¸ï¼Œå¹¶ä¿å­˜ç”±ä»¥ä¸‹æ–¹æ³•è®¡ç®—çš„å„ç§æ¨¡å‹è¾“å…¥
è¿™äº›æ–¹æ³•ï¼ˆ
 `è¾“å…¥ID`
 ,
 `æ³¨æ„åŠ›æ©ç `
 â€¦â€¦ï¼‰ã€‚å½“åˆ†è¯å™¨æ˜¯â€œå¿«é€Ÿâ€åˆ†è¯å™¨æ—¶ï¼ˆå³ç”±
æŠ±è„¸
 [tokenizers åº“](https://github.com/huggingface/tokenizers)
 ï¼‰ï¼Œè¿™ä¸ªç±»è¿˜æä¾›äº†
å‡ ç§é«˜çº§å¯¹é½æ–¹æ³•ï¼Œå¯ç”¨äºåœ¨åŸå§‹å­—ç¬¦ä¸²ï¼ˆå­—ç¬¦å’Œå•è¯ï¼‰å’Œå­—ç¬¦ä¸²ä¹‹é—´è¿›è¡Œæ˜ å°„
ä»¤ç‰Œç©ºé—´ï¼ˆä¾‹å¦‚ï¼Œè·å–åŒ…å«ç»™å®šå­—ç¬¦çš„ä»¤ç‰Œçš„ç´¢å¼•æˆ–å¯¹åº”çš„å­—ç¬¦èŒƒå›´
åˆ°ç»™å®šçš„ä»¤ç‰Œï¼‰ã€‚


## é¢„è®­ç»ƒåˆ†è¯å™¨




### 


ç­çº§
 

 å˜å‹å™¨ã€‚
 

 é¢„è®­ç»ƒåˆ†è¯å™¨


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/tokenization_utils.py#L336)



 (
 


 \*\*å¤¸æ ¼




 )
 


 å‚æ•°


* **æ¨¡å‹\_æœ€å¤§\_é•¿åº¦**
 ï¼ˆ
 `int`
 ,
 *é€‰ä¿®çš„*
 )â€”
å˜å‹å™¨æ¨¡å‹è¾“å…¥çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥ä»¤ç‰Œæ•°é‡è®¡ï¼‰ã€‚å½“åˆ†è¯å™¨æ˜¯
æ»¡è½½
 [æ¥è‡ª\_pretrained()](/docs/transformers/v4.35.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)
 ï¼Œè¿™å°†è¢«è®¾ç½®ä¸º
ä¸ºå…³è”æ¨¡å‹å­˜å‚¨çš„å€¼
 `æœ€å¤§æ¨¡å‹è¾“å…¥å¤§å°`
 ï¼ˆå¾€ä¸Šçœ‹ï¼‰ã€‚å¦‚æœæ²¡æœ‰æä¾›å€¼ï¼Œå°†
é»˜è®¤ä¸ºéå¸¸å¤§æ•´æ•°ï¼ˆ
 `int(1e30)`
 ï¼‰ã€‚
* **å¡«å……\_side**
 ï¼ˆ
 `str`
 ,
 *é€‰ä¿®çš„*
 )â€”
æ¨¡å‹åº”åº”ç”¨å¡«å……çš„ä¸€ä¾§ã€‚åº”åœ¨[â€˜å³â€™ã€â€˜å·¦â€™]ä¹‹é—´é€‰æ‹©ã€‚
é»˜è®¤å€¼æ˜¯ä»åŒåçš„ç±»å±æ€§ä¸­é€‰å–çš„ã€‚
* **æˆªæ–­\_side**
 ï¼ˆ
 `str`
 ,
 *é€‰ä¿®çš„*
 )â€”
æ¨¡å‹åº”åº”ç”¨æˆªæ–­çš„ä¸€ä¾§ã€‚åº”åœ¨[â€˜å³â€™ã€â€˜å·¦â€™]ä¹‹é—´é€‰æ‹©ã€‚
é»˜è®¤å€¼æ˜¯ä»åŒåçš„ç±»å±æ€§ä¸­é€‰å–çš„ã€‚
* **èŠå¤©\_æ¨¡æ¿**
 ï¼ˆ
 `str`
 ,
 *é€‰ä¿®çš„*
 )â€”
Jinja æ¨¡æ¿å­—ç¬¦ä¸²ï¼Œå°†ç”¨äºæ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯åˆ—è¡¨ã€‚çœ‹
 <https://huggingface.co/docs/transformers/chat_templated>
 ä»¥è·å¾—å®Œæ•´çš„æè¿°ã€‚
* **æ¨¡å‹\_è¾“å…¥\_åç§°**
 ï¼ˆ
 `åˆ—è¡¨[å­—ç¬¦ä¸²]`
 ,
 *é€‰ä¿®çš„*
 )â€”
æ¨¡å‹å‰å‘ä¼ é€’æ¥å—çš„è¾“å…¥åˆ—è¡¨ï¼ˆä¾‹å¦‚
 `â€œtoken_type_idsâ€`
 æˆ–è€…
 `â€œæ³¨æ„æ©ç â€`
 ï¼‰ã€‚é»˜è®¤å€¼æ˜¯ä»åŒåçš„ç±»å±æ€§ä¸­é€‰å–çš„ã€‚
* **bos\_token**
 ï¼ˆ
 `str`
 æˆ–è€…
 `tokenizers.AddedToken`
 ,
 *é€‰ä¿®çš„*
 )â€”
ä»£è¡¨å¥å­å¼€å¤´çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†å…³è”åˆ°
 `self.bos_token`
 å’Œ
 `self.bos_token_id`
 ã€‚
* **eos\_token**
 ï¼ˆ
 `str`
 æˆ–è€…
 `tokenizers.AddedToken`
 ,
 *é€‰ä¿®çš„*
 )â€”
ä»£è¡¨å¥å­ç»“å°¾çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†å…³è”åˆ°
 `self.eos_token`
 å’Œ
 `self.eos_token_id`
 ã€‚
* **unk\_token**
 ï¼ˆ
 `str`
 æˆ–è€…
 `tokenizers.AddedToken`
 ,
 *é€‰ä¿®çš„*
 )â€”
ä»£è¡¨è¯æ±‡è¡¨å¤–æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†å…³è”åˆ°
 `self.unk_token`
 å’Œ
 `self.unk_token_id`
 ã€‚
* **sep\_token**
 ï¼ˆ
 `str`
 æˆ–è€…
 `tokenizers.AddedToken`
 ,
 *é€‰ä¿®çš„*
 )â€”
åœ¨åŒä¸€è¾“å…¥ä¸­åˆ†éš”ä¸¤ä¸ªä¸åŒå¥å­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚ BERT ä½¿ç”¨ï¼‰ã€‚å°†
å…³è”åˆ°
 `self.sep_token`
 å’Œ
 `self.sep_token_id`
 ã€‚
* **å¡«å……\_token**
 ï¼ˆ
 `str`
 æˆ–è€…
 `tokenizers.AddedToken`
 ,
 *é€‰ä¿®çš„*
 )â€”
ä¸€ç§ç‰¹æ®Šçš„ä»¤ç‰Œï¼Œç”¨äºä½¿ä»¤ç‰Œæ•°ç»„å…·æœ‰ç›¸åŒçš„å¤§å°ä»¥è¿›è¡Œæ‰¹å¤„ç†ã€‚ç„¶åå°†è¢«å¿½ç•¥
æ³¨æ„æœºåˆ¶æˆ–æŸå¤±è®¡ç®—ã€‚å°†å…³è”åˆ°
 `self.pad_token`
 å’Œ
 `self.pad_token_id`
 ã€‚
* **cls\_token**
 ï¼ˆ
 `str`
 æˆ–è€…
 `tokenizers.AddedToken`
 ,
 *é€‰ä¿®çš„*
 )â€”
è¡¨ç¤ºè¾“å…¥ç±»åˆ«çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚ BERT ä½¿ç”¨ï¼‰ã€‚å°†å…³è”åˆ°
 `self.cls_token`
 å’Œ
 `self.cls_token_id`
 ã€‚
* **æ©ç \_token**
 ï¼ˆ
 `str`
 æˆ–è€…
 `tokenizers.AddedToken`
 ,
 *é€‰ä¿®çš„*
 )â€”
ä»£è¡¨æ©ç æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ï¼ˆç”±æ©ç è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒç›®æ ‡ä½¿ç”¨ï¼Œä¾‹å¦‚
ä¼¯ç‰¹ï¼‰ã€‚å°†å…³è”åˆ°
 `self.mask_token`
 å’Œ
 `self.mask_token_id`
 ã€‚
* **é¢å¤–\_ç‰¹æ®Š\_ä»¤ç‰Œ**
 ï¼ˆå…ƒç»„æˆ–åˆ—è¡¨
 `str`
 æˆ–è€…
 `tokenizers.AddedToken`
 ,
 *é€‰ä¿®çš„*
 )â€”
é™„åŠ ç‰¹æ®Šæ ‡è®°çš„å…ƒç»„æˆ–åˆ—è¡¨ã€‚å°†å®ƒä»¬æ·»åŠ åˆ°æ­¤å¤„ä»¥ç¡®ä¿åœ¨è§£ç æ—¶è·³è¿‡å®ƒä»¬
 `skip_special_tokens`
 è®¾ç½®ä¸º Trueã€‚å¦‚æœå®ƒä»¬ä¸æ˜¯è¯æ±‡è¡¨çš„ä¸€éƒ¨åˆ†ï¼Œåˆ™ä¼šåœ¨æœ«å°¾æ·»åŠ 
çš„è¯æ±‡ã€‚
* **æ¸…ç†\_up\_æ ‡è®°åŒ–\_spaces**
 ï¼ˆ
 `å¸ƒå°”`
 ,
 *é€‰ä¿®çš„*
 ï¼Œé»˜è®¤ä¸º
 'çœŸå®'
 )â€”
æ¨¡å‹æ˜¯å¦åº”è¯¥æ¸…é™¤åœ¨åˆ†å‰²è¾“å…¥æ–‡æœ¬æ—¶æ·»åŠ çš„ç©ºæ ¼
æ ‡è®°åŒ–è¿‡ç¨‹ã€‚
* **åˆ†å‰²\_ç‰¹æ®Š\_ä»£å¸**
 ï¼ˆ
 `å¸ƒå°”`
 ,
 *é€‰ä¿®çš„*
 ï¼Œé»˜è®¤ä¸º
 â€˜å‡â€™
 )â€”
åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ˜¯å¦åº”åˆ†å‰²ç‰¹æ®Šæ ‡è®°ã€‚é»˜è®¤è¡Œä¸ºæ˜¯
ä¸åˆ†å‰²ç‰¹æ®Šä»¤ç‰Œã€‚è¿™æ„å‘³ç€å¦‚æœ
 `<s>`
 æ˜¯ä¸ª
 `bos_token`
 ï¼Œ ç„¶å
 `tokenizer.tokenize("<s>") = ['<s>`
 ]ã€‚å¦åˆ™ï¼Œå¦‚æœ
 `split_special_tokens=True`
 ï¼Œ ç„¶å
 `tokenizer.tokenize("<s>")`
 å°†ç»™äºˆ
 `['<', 's', '>']`
 ã€‚è¯¥è®ºç‚¹ä»…æ”¯æŒ
 â€˜æ…¢â€™
 æš‚æ—¶ä½¿ç”¨æ ‡è®°å™¨ã€‚


 æ‰€æœ‰æ…¢æ ‡è®°å™¨çš„åŸºç±»ã€‚


ç»§æ‰¿è‡ª
 [PreTrainedTokenizerBase](/docs/transformers/v4.35.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)
 ã€‚


å¤„ç†æ‰€æœ‰ç”¨äºæ ‡è®°åŒ–å’Œç‰¹æ®Šæ ‡è®°çš„å…±äº«æ–¹æ³•ä»¥åŠä¸‹è½½/ç¼“å­˜/åŠ è½½æ–¹æ³•
é¢„è®­ç»ƒæ ‡è®°å™¨ä»¥åŠå°†æ ‡è®°æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚


è¯¥ç±»è¿˜ä»¥ç»Ÿä¸€çš„æ–¹å¼åŒ…å«åœ¨æ‰€æœ‰æ ‡è®°å™¨ä¹‹ä¸Šæ·»åŠ çš„æ ‡è®°ï¼Œå› æ­¤æˆ‘ä»¬ä¸å¿…å¤„ç†
å„ç§åº•å±‚è¯å…¸ç»“æ„ï¼ˆBPEã€sentpieceâ€¦ï¼‰çš„ç‰¹å®šè¯æ±‡å¢å¼ºæ–¹æ³•ã€‚


ç±»å±æ€§ï¼ˆè¢«æ´¾ç”Ÿç±»è¦†ç›–ï¼‰


* **è¯æ±‡\_æ–‡ä»¶\_åç§°**
 ï¼ˆ
 `å­—å…¸[str, str]`
 ) â€” ä¸€æœ¬å­—å…¸ï¼Œå…¶ä¸­çš„é”®ä¸º
 `__init__`
 æ¯ä¸ªçš„å…³é”®å­—åç§°
æ¨¡å‹æ‰€éœ€çš„è¯æ±‡æ–‡ä»¶ï¼Œä»¥åŠå…³è”å€¼ï¼Œç”¨äºä¿å­˜å…³è”æ–‡ä»¶çš„æ–‡ä»¶å
ï¼ˆç»†ç»³ï¼‰ã€‚
* **é¢„è®­ç»ƒ\_vocab\_files\_map**
 ï¼ˆ
 `å­—å…¸[strï¼Œå­—å…¸[strï¼Œstr]]`
 ) â€” å­—å…¸ä¸­çš„å­—å…¸ï¼Œå…¶ä¸­
é«˜çº§æŒ‰é”®æ˜¯
 `__init__`
 æ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªè¯æ±‡æ–‡ä»¶çš„å…³é”®å­—åç§°ï¼Œ
ä½çº§æ˜¯
 `å¿«æ·åç§°`
 é¢„è®­ç»ƒæ¨¡å‹çš„ç›¸å…³å€¼æ˜¯
 `ç½‘å€`
 åˆ°
å…³è”çš„é¢„è®­ç»ƒè¯æ±‡æ–‡ä»¶ã€‚
* **æœ€å¤§\_æ¨¡å‹\_è¾“å…¥\_å°ºå¯¸**
 ï¼ˆ
 `å­—å…¸[strï¼Œå¯é€‰[int]]`
 ) â€” ä¸€æœ¬å­—å…¸ï¼Œå…¶ä¸­çš„é”®ä¸º
 `å¿«æ·åç§°`
 é¢„è®­ç»ƒæ¨¡å‹çš„æ•°é‡ï¼Œä»¥åŠè¯¥æ¨¡å‹çš„åºåˆ—è¾“å…¥çš„æœ€å¤§é•¿åº¦ä½œä¸ºå…³è”å€¼ï¼Œ
æˆ–è€…
 `æ— `
 å¦‚æœæ¨¡å‹æ²¡æœ‰æœ€å¤§è¾“å…¥å¤§å°ã€‚
* **é¢„è®­ç»ƒ\_init\_configuration**
 ï¼ˆ
 `å­—å…¸[strï¼Œå­—å…¸[strï¼Œä»»æ„]]`
 ) â€” ä¸€æœ¬å­—å…¸ï¼Œå…¶ä¸­çš„é”®ä¸º
 `å¿«æ·åç§°`
 é¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®ï¼Œä»¥åŠä½œä¸ºå…³è”å€¼çš„ç‰¹å®šå‚æ•°çš„å­—å…¸
ä¼ é€’åˆ°
 `__init__`
 åŠ è½½åˆ†è¯å™¨æ—¶æ­¤é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†è¯å™¨ç±»çš„æ–¹æ³•
ä¸
 [æ¥è‡ª\_pretrained()](/docs/transformers/v4.35.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)
 æ–¹æ³•ã€‚
* **æ¨¡å‹\_è¾“å…¥\_åç§°**
 ï¼ˆ
 `åˆ—è¡¨[str]`
 ) â€” æ¨¡å‹å‰å‘ä¼ é€’ä¸­é¢„æœŸçš„è¾“å…¥åˆ—è¡¨ã€‚
* **å¡«å……\_side**
 ï¼ˆ
 `str`
 ) â€” æ¨¡å‹åº”åº”ç”¨å¡«å……çš„ä¸€ä¾§çš„é»˜è®¤å€¼ã€‚
åº”è¯¥
 ``å¯¹'``
 æˆ–è€…
 ``å·¦'`
 ã€‚
* **æˆªæ–­\_side**
 ï¼ˆ
 `str`
 ) â€” æ¨¡å‹åº”æˆªæ–­ä¸€ä¾§çš„é»˜è®¤å€¼
åº”ç”¨ã€‚åº”è¯¥
 ``å¯¹'``
 æˆ–è€…
 ``å·¦'`
 ã€‚



#### 


\_\_ç§°å‘¼\_\_


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/tokenization_utils_base.py#L2724)



 (
 


 æ–‡æœ¬
 
 : æ‰“å­—.Union[str, æ‰“å­—.List[str], æ‰“å­—.List[æ‰“å­—.List[str]]] = æ— 


æ–‡æœ¬\_å¯¹
 
 : æ‰“å­—.Union[str, æ‰“å­—.List[str], æ‰“å­—.List[æ‰“å­—.List[str]], NoneType] = æ— 


æ–‡æœ¬\_ç›®æ ‡
 
 : æ‰“å­—.Union[str, æ‰“å­—.List[str], æ‰“å­—.List[æ‰“å­—.List[str]]] = æ— 


æ–‡æœ¬\_å¯¹\_ç›®æ ‡
 
 : æ‰“å­—.Union[str, æ‰“å­—.List[str], æ‰“å­—.List[æ‰“å­—.List[str]], NoneType] = æ— 


æ·»åŠ \_ç‰¹æ®Š\_ä»¤ç‰Œ
 
 ï¼šå¸ƒå°”=çœŸ


å¡«å……
 
 ï¼šæ‰“å­—.Union[boolï¼Œstrï¼Œtransformers.utils.generic.PaddingStrategy] = False


æˆªæ–­
 
 ï¼štyping.Union[boolï¼Œstrï¼Œtransformers.tokenization\_utils\_base.TruncationStrategy] = None


æœ€é•¿é•¿åº¦
 
 : æ‰“å­—.Optional[int] = None


è·¨æ­¥
 
 ï¼šæ•´æ•°=0


è¢«\_åˆ†å‰²\_æˆ\_å•è¯
 
 ï¼šå¸ƒå°”=å‡


å¡«å……åˆ°å¤šä¸ª
 
 : æ‰“å­—.Optional[int] = None


è¿”å›\_å¼ é‡
 
 ï¼šæ‰“å­—.Union[strï¼Œtransformers.utils.generic.TensorTypeï¼ŒNoneType] =æ— 


è¿”å›\_token\_type\_ids
 
 : æ‰“å­—.Optional[bool] = None


è¿”å›\_attention\_mask
 
 : æ‰“å­—.Optional[bool] = None


è¿”å›\_overflowing\_tokens
 
 ï¼šå¸ƒå°”=å‡


è¿”å›\_ç‰¹æ®Š\_ä»¤ç‰Œ\_æ©ç 
 
 ï¼šå¸ƒå°”=å‡


è¿”å›\_offsets\_mapping
 
 ï¼šå¸ƒå°”=å‡


è¿”å›\_length
 
 ï¼šå¸ƒå°”=å‡


å†—é•¿çš„
 
 ï¼šå¸ƒå°”=çœŸ


\*\*å¤¸æ ¼


ï¼‰
 

 â†’


å¯¼å‡ºå¸¸é‡å…ƒæ•°æ®='æœªå®šä¹‰';
 

[BatchEncoding](/docs/transformers/v4.35.2/en/main_classes/tokenizer#transformers.BatchEncoding)


 å‚æ•°




* **text** 
 (
 `str` 
 ,
 `List[str]` 
 ,
 `List[List[str]]` 
 ,
 *optional* 
 ) â€”
The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
 `is_split_into_words=True` 
 (to lift the ambiguity with a batch of sequences).
* **text\_pair** 
 (
 `str` 
 ,
 `List[str]` 
 ,
 `List[List[str]]` 
 ,
 *optional* 
 ) â€”
The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
 `is_split_into_words=True` 
 (to lift the ambiguity with a batch of sequences).
* **text\_target** 
 (
 `str` 
 ,
 `List[str]` 
 ,
 `List[List[str]]` 
 ,
 *optional* 
 ) â€”
The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a
list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),
you must set
 `is_split_into_words=True` 
 (to lift the ambiguity with a batch of sequences).
* **text\_pair\_target** 
 (
 `str` 
 ,
 `List[str]` 
 ,
 `List[List[str]]` 
 ,
 *optional* 
 ) â€”
The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a
list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),
you must set
 `is_split_into_words=True` 
 (to lift the ambiguity with a batch of sequences).
* **add\_special\_tokens** 
 (
 `bool` 
 ,
 *optional* 
 , defaults to
 `True` 
 ) â€”
Whether or not to add special tokens when encoding the sequences. This will use the underlying
 `PretrainedTokenizerBase.build_inputs_with_special_tokens` 
 function, which defines which tokens are
automatically added to the input ids. This is usefull if you want to add
 `bos` 
 or
 `eos` 
 tokens
automatically.
* **padding** 
 (
 `bool` 
 ,
 `str` 
 or
 [PaddingStrategy](/docs/transformers/v4.35.2/en/internal/file_utils#transformers.utils.PaddingStrategy) 
 ,
 *optional* 
 , defaults to
 `False` 
 ) â€”
Activates and controls padding. Accepts the following values:
 
	+ `True` 
	 or
	 `'longest'` 
	 : Pad to the longest sequence in the batch (or no padding if only a single
	sequence if provided).
	+ `'max_length'` 
	 : Pad to a maximum length specified with the argument
	 `max_length` 
	 or to the maximum
	acceptable input length for the model if that argument is not provided.
	+ `False` 
	 or
	 `'do_not_pad'` 
	 (default): No padding (i.e., can output a batch with sequences of different
	lengths).
* **truncation** 
 (
 `bool` 
 ,
 `str` 
 or
 [TruncationStrategy](/docs/transformers/v4.35.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy) 
 ,
 *optional* 
 , defaults to
 `False` 
 ) â€”
Activates and controls truncation. Accepts the following values:
 
	+ `True` 
	 or
	 `'longest_first'` 
	 : Truncate to a maximum length specified with the argument
	 `max_length` 
	 or
	to the maximum acceptable input length for the model if that argument is not provided. This will
	truncate token by token, removing a token from the longest sequence in the pair if a pair of
	sequences (or a batch of pairs) is provided.
	+ `'only_first'` 
	 : Truncate to a maximum length specified with the argument
	 `max_length` 
	 or to the
	maximum acceptable input length for the model if that argument is not provided. This will only
	truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.
	+ `'only_second'` 
	 : Truncate to a maximum length specified with the argument
	 `max_length` 
	 or to the
	maximum acceptable input length for the model if that argument is not provided. This will only
	truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.
	+ `False` 
	 or
	 `'do_not_truncate'` 
	 (default): No truncation (i.e., can output batch with sequence lengths
	greater than the model maximum admissible input size).
* **max\_length** 
 (
 `int` 
 ,
 *optional* 
 ) â€”
Controls the maximum length to use by one of the truncation/padding parameters.
 
 If left unset or set to
 `None` 
 , this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.
* **stride** 
 (
 `int` 
 ,
 *optional* 
 , defaults to 0) â€”
If set to a number along with
 `max_length` 
 , the overflowing tokens returned when
 `return_overflowing_tokens=True` 
 will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.
* **is\_split\_into\_words** 
 (
 `bool` 
 ,
 *optional* 
 , defaults to
 `False` 
 ) â€”
Whether or not the input is already pre-tokenized (e.g., split into words). If set to
 `True` 
 , the
tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)
which it will tokenize. This is useful for NER or token classification.
* **pad\_to\_multiple\_of** 
 (
 `int` 
 ,
 *optional* 
 ) â€”
If set will pad the sequence to a multiple of the provided value. Requires
 `padding` 
 to be activated.
This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
 `>= 7.5` 
 (Volta).
* **return\_tensors** 
 (
 `str` 
 or
 [TensorType](/docs/transformers/v4.35.2/en/internal/file_utils#transformers.TensorType) 
 ,
 *optional* 
 ) â€”
If set, will return tensors instead of list of python integers. Acceptable values are:
 
	+ `'tf'` 
	 : Return TensorFlow
	 `tf.constant` 
	 objects.
	+ `'pt'` 
	 : Return PyTorch
	 `torch.Tensor` 
	 objects.
	+ `'np'` 
	 : Return Numpy
	 `np.ndarray` 
	 objects.
* **return\_token\_type\_ids** 
 (
 `bool` 
 ,
 *optional* 
 ) â€”
Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizerâ€™s default, defined by the
 `return_outputs` 
 attribute.
 
[What are token type IDs?](../glossary#token-type-ids)
* **return\_attention\_mask** 
 (
 `bool` 
 ,
 *optional* 
 ) â€”
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizerâ€™s default, defined by the
 `return_outputs` 
 attribute.
 
[What are attention masks?](../glossary#attention-mask)
* **return\_overflowing\_tokens** 
 (
 `bool` 
 ,
 *optional* 
 , defaults to
 `False` 
 ) â€”
Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch
of pairs) is provided with
 `truncation_strategy = longest_first` 
 or
 `True` 
 , an error is raised instead
of returning overflowing tokens.
* **return\_special\_tokens\_mask** 
 (
 `bool` 
 ,
 *optional* 
 , defaults to
 `False` 
 ) â€”
Whether or not to return special tokens mask information.
* **return\_offsets\_mapping** 
 (
 `bool` 
 ,
 *optional* 
 , defaults to
 `False` 
 ) â€”
Whether or not to return
 `(char_start, char_end)` 
 for each token.
 
 This is only available on fast tokenizers inheriting from
 [PreTrainedTokenizerFast](/docs/transformers/v4.35.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) 
 , if using
Pythonâ€™s tokenizer, this method will raise
 `NotImplementedError` 
.
* **return\_length** 
 (
 `bool` 
 ,
 *optional* 
 , defaults to
 `False` 
 ) â€”
Whether or not to return the lengths of the encoded inputs.
* **verbose** 
 (
 `bool` 
 ,
 *optional* 
 , defaults to
 `True` 
 ) â€”
Whether or not to print more information and warnings.
\*\*kwargs â€” passed to the
 `self.tokenize()` 
 method


é€€è´§


å¯¼å‡ºå¸¸é‡å…ƒæ•°æ®='æœªå®šä¹‰';
 

[BatchEncoding](/docs/transformers/v4.35.2/en/main_classes/tokenizer#transformers.BatchEncoding)


å¯¼å‡ºå¸¸é‡å…ƒæ•°æ®='æœªå®šä¹‰';


A
 [BatchEncoding](/docs/transformers/v4.35.2/en/main_classes/tokenizer#transformers.BatchEncoding)
 åŒ…å«ä»¥ä¸‹å­—æ®µï¼š


* **è¾“å…¥\_ids**
 â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„ä»¤ç‰Œ ID åˆ—è¡¨ã€‚


[ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ](../glossary#input-ids)
* **ä»¤ç‰Œ\_type\_ids**
 â€” è¦é¦ˆé€åˆ°æ¨¡å‹çš„ä»¤ç‰Œç±»å‹ ID åˆ—è¡¨ï¼ˆå½“
 `return_token_type_ids=True`
 æˆ–è€…
å¦‚æœ
 *â€œä»¤ç‰Œ\_type\_idsâ€*
 æ˜¯åœ¨
 `self.model_input_names`
 ï¼‰ã€‚


[ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹ IDï¼Ÿ](../glossary#token-type-ids)
* **æ³¨æ„\_mask**
 â€” æŒ‡å®šæ¨¡å‹åº”å…³æ³¨å“ªäº›æ ‡è®°çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“
 `return_attention_mask=True`
 æˆ–è€…å¦‚æœ
 *â€œæ³¨æ„\_maskâ€*
 æ˜¯åœ¨
 `self.model_input_names`
 ï¼‰ã€‚


[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›è’™ç‰ˆï¼Ÿ](../glossary#attention-mask)
* **æº¢å‡º\_tokens**
 â€” æº¢å‡ºæ ‡è®°åºåˆ—åˆ—è¡¨ï¼ˆå½“
 `æœ€å¤§é•¿åº¦`
 è¢«æŒ‡å®šå¹¶ä¸”
 `return_overflowing_tokens=True`
 ï¼‰ã€‚
* **num\_truncated\_tokens**
 â€” è¢«æˆªæ–­çš„ä»¤ç‰Œæ•°é‡ï¼ˆå½“
 `æœ€å¤§é•¿åº¦`
 è¢«æŒ‡å®šå¹¶ä¸”
 `return_overflowing_tokens=True`
 ï¼‰ã€‚
* **ç‰¹æ®Š\_ä»¤ç‰Œ\_æ©ç **
 â€” 0 å’Œ 1 çš„åˆ—è¡¨ï¼Œå…¶ä¸­ 1 æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼Œ0 æŒ‡å®š
å¸¸è§„åºåˆ—æ ‡è®°ï¼ˆå½“
 `add_special_tokens=True`
 å’Œ
 `return_special_tokens_mask=True`
 ï¼‰ã€‚
* **é•¿åº¦**
 â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“
 `return_length=True`
 ï¼‰


å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€å¯¹æˆ–å¤šå¯¹åºåˆ—è¿›è¡Œæ ‡è®°å’Œå‡†å¤‡æ¨¡å‹çš„ä¸»è¦æ–¹æ³•
åºåˆ—ã€‚




#### 


åº”ç”¨\_èŠå¤©\_æ¨¡æ¿


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/tokenization_utils_base.py#L1679)



 (
 


 å¯¹è¯
 
 : Typing.Union[typing.List[typing.Dict[str, str]], ForwardRef('å¯¹è¯')]


èŠå¤©\_æ¨¡æ¿
 
 : æ‰“å­—.å¯é€‰[str] =æ— 


æ·»åŠ \_ä»£\_æç¤º
 
 ï¼šå¸ƒå°”=å‡