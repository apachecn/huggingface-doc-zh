# å› æœè¯­è¨€å»ºæ¨¡çš„åŠæ—¶è°ƒæ•´

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/task_guides/clm-prompt-tuning>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/task_guides/clm-prompt-tuning>


![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/assets/colab-badge.svg)


![åœ¨ Studio Lab ä¸­æ‰“å¼€](https://studiolab.sagemaker.aws/studiolab.svg)


æç¤ºé€šè¿‡æ·»åŠ ä¸€äº›ç‰¹å®šäºä»»åŠ¡çš„è¾“å…¥æ–‡æœ¬æ¥å¸®åŠ©æŒ‡å¯¼è¯­è¨€æ¨¡å‹è¡Œä¸ºã€‚æç¤ºè°ƒæ•´æ˜¯ä¸€ç§é™„åŠ æ–¹æ³•ï¼Œä»…ç”¨äºè®­ç»ƒå’Œæ›´æ–°æ–°æ·»åŠ çš„æç¤ºæ ‡è®°åˆ°é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™æ ·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæƒé‡è¢«å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒå’Œæ›´æ–°ä¸€ç»„è¾ƒå°çš„æç¤ºå‚æ•°ï¼Œè€Œä¸æ˜¯å®Œå…¨å¾®è°ƒå•ç‹¬çš„æ¨¡å‹ã€‚éšç€æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤§ï¼ŒåŠæ—¶è°ƒæ•´ä¼šæ›´åŠ æœ‰æ•ˆï¼Œå¹¶ä¸”éšç€æ¨¡å‹å‚æ•°çš„æ‰©å±•ï¼Œç»“æœä¼šæ›´å¥½ã€‚


ğŸ’¡é˜…è¯»
 [å‚æ•°é«˜æ•ˆæç¤ºè°ƒæ•´çš„è§„æ¨¡åŠ›é‡](https://arxiv.org/abs/2104.08691)
 äº†è§£æœ‰å…³æç¤ºè°ƒæ•´çš„æ›´å¤šä¿¡æ¯ã€‚


æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åº”ç”¨æç¤ºè°ƒæ•´æ¥è®­ç»ƒ
 [`bloomz-560m`](https://huggingface.co/bigscience/bloomz-560m)
 æ¨¡å‹ä¸Šçš„
 `twitter_complaints`
 çš„å­é›†
 [RAFT](https://huggingface.co/datasets/ought/raft)
 æ•°æ®é›†ã€‚


åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ï¼š



```
!pip install -q peft transformers datasets
```


## è®¾ç½®



é¦–å…ˆå®šä¹‰æ¨¡å‹å’Œåˆ†è¯å™¨ã€è¦è®­ç»ƒçš„æ•°æ®é›†å’Œæ•°æ®é›†åˆ—ã€ä¸€äº›è®­ç»ƒè¶…å‚æ•°ä»¥åŠ
 [PromptTuningConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.PromptTuningConfig)
 ã€‚è¿™
 [PromptTuningConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.PromptTuningConfig)
 åŒ…å«æœ‰å…³ä»»åŠ¡ç±»å‹ã€åˆå§‹åŒ–æç¤ºåµŒå…¥çš„æ–‡æœ¬ã€è™šæ‹Ÿæ ‡è®°çš„æ•°é‡ä»¥åŠè¦ä½¿ç”¨çš„æ ‡è®°ç”Ÿæˆå™¨çš„ä¿¡æ¯ï¼š



```
from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup
from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType
import torch
from datasets import load_dataset
import os
from torch.utils.data import DataLoader
from tqdm import tqdm

device = "cuda"
model_name_or_path = "bigscience/bloomz-560m"
tokenizer_name_or_path = "bigscience/bloomz-560m"
peft_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    prompt_tuning_init=PromptTuningInit.TEXT,
    num_virtual_tokens=8,
    prompt_tuning_init_text="Classify if the tweet is a complaint or not:",
    tokenizer_name_or_path=model_name_or_path,
)

dataset_name = "twitter\_complaints"
checkpoint_name = f"{dataset\_name}\_{model\_name\_or\_path}\_{peft\_config.peft\_type}\_{peft\_config.task\_type}\_v1.pt".replace(
    "/", "\_"
)
text_column = "Tweet text"
label_column = "text\_label"
max_length = 64
lr = 3e-2
num_epochs = 50
batch_size = 8
```


## åŠ è½½æ•°æ®é›†



å¯¹äºæœ¬æŒ‡å—ï¼Œæ‚¨å°†åŠ è½½
 `twitter_complaints`
 çš„å­é›†
 [RAFT](https://huggingface.co/datasets/ought/raft)
 æ•°æ®é›†ã€‚è¯¥å­é›†åŒ…å«æ ‡è®°ä¸º
 `æŠ•è¯‰`
 æˆ–è€…
 `æ²¡æœ‰æŠ±æ€¨`
 ï¼š



```
dataset = load_dataset("ought/raft", dataset_name)
dataset["train"][0]
{"Tweet text": "@HMRCcustomers No this is my first job", "ID": 0, "Label": 2}
```


ä¸ºäº†ä½¿
 `æ ‡ç­¾`
 åˆ—æ›´å…·å¯è¯»æ€§ï¼Œæ›¿æ¢
 `æ ‡ç­¾`
 å€¼ä¸ç›¸åº”çš„æ ‡ç­¾æ–‡æœ¬å¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨
 `æ–‡æœ¬æ ‡ç­¾`
 æŸ±å­ã€‚æ‚¨å¯ä»¥ä½¿ç”¨
 `åœ°å›¾`
 å‡½æ•°ä¸€æ­¥å°†è¿™ä¸€æ›´æ”¹åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ï¼š



```
classes = [k.replace("\_", " ") for k in dataset["train"].features["Label"].names]
dataset = dataset.map(
    lambda x: {"text\_label": [classes[label] for label in x["Label"]]},
    batched=True,
    num_proc=1,
)
dataset["train"][0]
{"Tweet text": "@HMRCcustomers No this is my first job", "ID": 0, "Label": 2, "text\_label": "no complaint"}
```


## é¢„å¤„ç†æ•°æ®é›†



æ¥ä¸‹æ¥ï¼Œæ‚¨å°†è®¾ç½®ä¸€ä¸ªåˆ†è¯å™¨ï¼›é…ç½®é€‚å½“çš„å¡«å……æ ‡è®°ä»¥ç”¨äºå¡«å……åºåˆ—ï¼Œå¹¶ç¡®å®šæ ‡è®°åŒ–æ ‡ç­¾çš„æœ€å¤§é•¿åº¦ï¼š



```
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id
target_max_length = max([len(tokenizer(class_label)["input\_ids"]) for class_label in classes])
print(target_max_length)
3
```


åˆ›å»ºä¸€ä¸ª
 `é¢„å¤„ç†å‡½æ•°`
 åˆ°ï¼š


1. å¯¹è¾“å…¥æ–‡æœ¬å’Œæ ‡ç­¾è¿›è¡Œæ ‡è®°ã€‚
2. å¯¹äºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªç¤ºä¾‹ï¼Œä½¿ç”¨åˆ†è¯å™¨å¡«å……æ ‡ç­¾
 `pad_token_id`
 ã€‚
3. å°†è¾“å…¥æ–‡æœ¬å’Œæ ‡ç­¾è¿æ¥åˆ°
 `æ¨¡å‹è¾“å…¥`
 ã€‚
4. åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„æ³¨æ„åŠ›è’™ç‰ˆ
 `æ ‡ç­¾`
 å’Œ
 `æ¨¡å‹è¾“å…¥`
 ã€‚
5. å†æ¬¡å¾ªç¯éå†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªç¤ºä¾‹ï¼Œå°†è¾“å…¥ idã€æ ‡ç­¾å’Œæ³¨æ„æ©ç å¡«å……åˆ°
 `æœ€å¤§é•¿åº¦`
 å¹¶å°†å®ƒä»¬è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚



```
def preprocess\_function(examples):
    batch_size = len(examples[text_column])
    inputs = [f"{text\_column} : {x} Label : " for x in examples[text_column]]
    targets = [str(x) for x in examples[label_column]]
    model_inputs = tokenizer(inputs)
    labels = tokenizer(targets)
    for i in range(batch_size):
        sample_input_ids = model_inputs["input\_ids"][i]
        label_input_ids = labels["input\_ids"][i] + [tokenizer.pad_token_id]
        # print(i, sample\_input\_ids, label\_input\_ids)
        model_inputs["input\_ids"][i] = sample_input_ids + label_input_ids
        labels["input\_ids"][i] = [-100] * len(sample_input_ids) + label_input_ids
        model_inputs["attention\_mask"][i] = [1] * len(model_inputs["input\_ids"][i])
    # print(model\_inputs)
    for i in range(batch_size):
        sample_input_ids = model_inputs["input\_ids"][i]
        label_input_ids = labels["input\_ids"][i]
        model_inputs["input\_ids"][i] = [tokenizer.pad_token_id] * (
            max_length - len(sample_input_ids)
        ) + sample_input_ids
        model_inputs["attention\_mask"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[
            "attention\_mask"
        ][i]
        labels["input\_ids"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids
        model_inputs["input\_ids"][i] = torch.tensor(model_inputs["input\_ids"][i][:max_length])
        model_inputs["attention\_mask"][i] = torch.tensor(model_inputs["attention\_mask"][i][:max_length])
        labels["input\_ids"][i] = torch.tensor(labels["input\_ids"][i][:max_length])
    model_inputs["labels"] = labels["input\_ids"]
    return model_inputs
```


ä½¿ç”¨
 `åœ°å›¾`
 å‡½æ•°æ¥åº”ç”¨
 `é¢„å¤„ç†å‡½æ•°`
 åˆ°æ•´ä¸ªæ•°æ®é›†ã€‚æ‚¨å¯ä»¥åˆ é™¤æœªå¤„ç†çš„åˆ—ï¼Œå› ä¸ºæ¨¡å‹ä¸éœ€è¦å®ƒä»¬ï¼š



```
processed_datasets = dataset.map(
    preprocess_function,
    batched=True,
    num_proc=1,
    remove_columns=dataset["train"].column_names,
    load_from_cache_file=False,
    desc="Running tokenizer on dataset",
)
```


åˆ›å»ºä¸€ä¸ª
 [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
 æ¥è‡ª
 `ç«è½¦`
 å’Œ
 `è¯„ä¼°`
 æ•°æ®é›†ã€‚æ”¾
 `pin_memory=True`
 å¦‚æœæ•°æ®é›†ä¸­çš„æ ·æœ¬ä½äº CPU ä¸Šï¼Œåˆ™å¯ä»¥åœ¨è®­ç»ƒæœŸé—´åŠ å¿«æ•°æ®ä¼ è¾“åˆ° GPU çš„é€Ÿåº¦ã€‚



```
train_dataset = processed_datasets["train"]
eval_dataset = processed_datasets["test"]


train_dataloader = DataLoader(
    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True
)
eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)
```


## ç«è½¦



æ‚¨å‡ ä¹å·²å‡†å¤‡å¥½è®¾ç½®æ¨¡å‹å¹¶å¼€å§‹è®­ç»ƒï¼


åˆå§‹åŒ–åŸºæœ¬æ¨¡å‹
 [AutoModelForCausalLM](https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/auto#transformers.AutoModelForCausalLM)
 ï¼Œå¹¶é€šè¿‡å®ƒå’Œ
 `peft_config`
 åˆ°
 `get_peft_model()`
 å‡½æ•°æ¥åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚æ‚¨å¯ä»¥æ‰“å°æ–°çš„
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 çš„å¯è®­ç»ƒå‚æ•°ï¼Œçœ‹çœ‹å®ƒæ¯”è®­ç»ƒåŸå§‹æ¨¡å‹çš„å®Œæ•´å‚æ•°æ•ˆç‡é«˜å¤šå°‘ï¼



```
model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)
print(model.print_trainable_parameters())
"trainable params: 8192 || all params: 559222784 || trainable%: 0.0014648902430985358"
```


è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š



```
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
lr_scheduler = get_linear_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=(len(train_dataloader) * num_epochs),
)
```


å°†æ¨¡å‹ç§»è‡³ GPUï¼Œç„¶åç¼–å†™è®­ç»ƒå¾ªç¯å¼€å§‹è®­ç»ƒï¼



```
model = model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for step, batch in enumerate(tqdm(train_dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        total_loss += loss.detach().float()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()

    model.eval()
    eval_loss = 0
    eval_preds = []
    for step, batch in enumerate(tqdm(eval_dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        loss = outputs.loss
        eval_loss += loss.detach().float()
        eval_preds.extend(
            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)
        )

    eval_epoch_loss = eval_loss / len(eval_dataloader)
    eval_ppl = torch.exp(eval_epoch_loss)
    train_epoch_loss = total_loss / len(train_dataloader)
    train_ppl = torch.exp(train_epoch_loss)
    print(f"{epoch=}: {train\_ppl=} {train\_epoch\_loss=} {eval\_ppl=} {eval\_epoch\_loss=}")
```


## åˆ†äº«æ¨¡å‹



å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨å¯ä»¥åœ¨ Hub ä¸Šå­˜å‚¨å’Œå…±äº«æ‚¨çš„æ¨¡å‹ã€‚ç™»å½•æ‚¨çš„ Hugging Face å¸æˆ·å¹¶åœ¨å‡ºç°æç¤ºæ—¶è¾“å…¥æ‚¨çš„ä»¤ç‰Œï¼š



```
from huggingface_hub import notebook_login

notebook_login()
```


ä½¿ç”¨
 [push\_to\_hub](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
 å°†æ¨¡å‹ä¸Šä¼ åˆ° Hub ä¸Šçš„æ¨¡å‹å­˜å‚¨åº“çš„å‡½æ•°ï¼š



```
peft_model_id = "your-name/bloomz-560m\_PROMPT\_TUNING\_CAUSAL\_LM"
model.push_to_hub("your-name/bloomz-560m\_PROMPT\_TUNING\_CAUSAL\_LM", use_auth_token=True)
```


æ¨¡å‹ä¸Šä¼ åï¼Œæ‚¨ä¼šçœ‹åˆ°æ¨¡å‹æ–‡ä»¶å¤§å°åªæœ‰ 33.5kBï¼ ğŸ¤


## æ¨ç†



è®©æˆ‘ä»¬åœ¨ç¤ºä¾‹è¾“å…¥ä¸Šå°è¯•è¯¥æ¨¡å‹ä»¥è¿›è¡Œæ¨ç†ã€‚å¦‚æœæ‚¨æŸ¥çœ‹å°†æ¨¡å‹ä¸Šä¼ åˆ°çš„å­˜å‚¨åº“ï¼Œæ‚¨å°†çœ‹åˆ°
 `adapter_config.json`
 æ–‡ä»¶ã€‚å°†æ­¤æ–‡ä»¶åŠ è½½åˆ°
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 æŒ‡å®š
 `peft_type`
 å’Œ
 `ä»»åŠ¡ç±»å‹`
 ã€‚ç„¶åå°±å¯ä»¥åŠ è½½æç¤ºè°ƒæ•´çš„æ¨¡å‹æƒé‡ï¼Œå¹¶å°†é…ç½®æ”¾å…¥
 [æ¥è‡ª_pretrained()](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel.from_pretrained)
 æ¥åˆ›å»º
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ï¼š



```
from peft import PeftModel, PeftConfig

peft_model_id = "stevhliu/bloomz-560m\_PROMPT\_TUNING\_CAUSAL\_LM"

config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(model, peft_model_id)
```


è·å–ä¸€æ¡æ¨æ–‡å¹¶å°†å…¶æ ‡è®°åŒ–ï¼š



```
inputs = tokenizer(
    f'{text\_column} : {"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?"} Label : ',
    return_tensors="pt",
)
```


å°†æ¨¡å‹æ”¾åœ¨ GPU ä¸Šå¹¶
 *äº§ç”Ÿ*
 é¢„æµ‹æ ‡ç­¾ï¼š



```
model.to(device)

with torch.no_grad():
    inputs = {k: v.to(device) for k, v in inputs.items()}
    outputs = model.generate(
        input_ids=inputs["input\_ids"], attention_mask=inputs["attention\_mask"], max_new_tokens=10, eos_token_id=3
    )
    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))
[
    "Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label : complaint"
]
```