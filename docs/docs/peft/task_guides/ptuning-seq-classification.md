# ç”¨äºåºåˆ—åˆ†ç±»çš„ P-tuning

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/task_guides/ptuning-seq-classification>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/task_guides/ptuning-seq-classification>


ä¸ºä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬æœ‰å¤ªå¤šå‚æ•°ã€‚è¦è§£å†³æ­¤é—®é¢˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨
 *æç¤º*
 åœ¨ä¸å®Œå…¨å¾®è°ƒæ¨¡å‹çš„æƒ…å†µä¸‹å°†æ¨¡å‹å¼•å¯¼è‡³ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚é€šå¸¸ï¼Œè¿™äº›æç¤ºæ˜¯æ‰‹å·¥åˆ¶ä½œçš„ï¼Œè¿™å¯èƒ½ä¸åˆ‡å®é™…ï¼Œå› ä¸ºæ‚¨éœ€è¦éå¸¸å¤§çš„éªŒè¯é›†æ‰èƒ½æ‰¾åˆ°æœ€ä½³æç¤ºã€‚
 *Pè°ƒ*
 æ˜¯ä¸€ç§åœ¨è¿ç»­ç©ºé—´ä¸­è‡ªåŠ¨æœç´¢å’Œä¼˜åŒ–æ›´å¥½æç¤ºçš„æ–¹æ³•ã€‚


ğŸ’¡é˜…è¯»
 [GPT ä¹Ÿèƒ½ç†è§£](https://arxiv.org/abs/2103.10385)
 äº†è§£æœ‰å…³ p è°ƒæ•´çš„æ›´å¤šä¿¡æ¯ã€‚


æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•è®­ç»ƒ
 [`roberta-large`](https://huggingface.co/roberta-large)
 æ¨¡å‹ï¼ˆä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½• GPTã€OPT æˆ– BLOOM æ¨¡å‹ï¼‰ï¼Œå¹¶åœ¨
 `mrpc`
 é…ç½®çš„
 [èƒ¶æ°´](https://huggingface.co/datasets/glue)
 åŸºå‡†ã€‚


åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ï¼š



```
!pip install -q peft transformers datasets evaluate
```


## è®¾ç½®



é¦–å…ˆï¼Œå¯¼å…¥ ğŸ¤— Transformers ä»¥åˆ›å»ºåŸºæœ¬æ¨¡å‹ï¼ŒğŸ¤— Datasets æ¥åŠ è½½æ•°æ®é›†ï¼ŒğŸ¤— Evaluate æ¥åŠ è½½è¯„ä¼°æŒ‡æ ‡ï¼ŒğŸ¤— PEFT æ¥åˆ›å»º
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 å¹¶è®¾ç½® p-tuning çš„é…ç½®ã€‚


å®šä¹‰æ¨¡å‹ã€æ•°æ®é›†å’Œä¸€äº›åŸºæœ¬è®­ç»ƒè¶…å‚æ•°ï¼š



```
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)
from peft import (
    get_peft_config,
    get_peft_model,
    get_peft_model_state_dict,
    set_peft_model_state_dict,
    PeftType,
    PromptEncoderConfig,
)
from datasets import load_dataset
import evaluate
import torch

model_name_or_path = "roberta-large"
task = "mrpc"
num_epochs = 20
lr = 1e-3
batch_size = 32
```


## åŠ è½½æ•°æ®é›†å’ŒæŒ‡æ ‡



æ¥ä¸‹æ¥ï¼ŒåŠ è½½
 `mrpc`
 é…ç½® - æ ¹æ®è¯­ä¹‰æ˜¯å¦ç­‰ä»·æ ‡è®°çš„å¥å­å¯¹è¯­æ–™åº“ - æ¥è‡ª
 [èƒ¶æ°´](https://huggingface.co/datasets/glue)
 åŸºå‡†ï¼š



```
dataset = load_dataset("glue", task)
dataset["train"][0]
{
    "sentence1": 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
    "sentence2": 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
    "label": 1,
    "idx": 0,
}
```


ä»ğŸ¤—è¯„ä¼°ä¸­ï¼ŒåŠ è½½ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚è¯„ä¼°æ¨¡å—è¿”å›ä¸è¯¥ç‰¹å®šä»»åŠ¡ç›¸å…³çš„å‡†ç¡®æ€§å’Œ F1 åˆ†æ•°ã€‚



```
metric = evaluate.load("glue", task)
```


ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨
 `å…¬åˆ¶`
 ç¼–å†™ä¸€ä¸ªè®¡ç®—å‡†ç¡®åº¦å’Œ F1 åˆ†æ•°çš„å‡½æ•°ã€‚è¿™
 `è®¡ç®—æŒ‡æ ‡`
 å‡½æ•°æ ¹æ®æ¨¡å‹é¢„æµ‹å’Œæ ‡ç­¾è®¡ç®—åˆ†æ•°ï¼š



```
import numpy as np


def compute\_metrics(eval\_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)
```


## é¢„å¤„ç†æ•°æ®é›†



åˆå§‹åŒ–æ ‡è®°ç”Ÿæˆå™¨å¹¶é…ç½®è¦ä½¿ç”¨çš„å¡«å……æ ‡è®°ã€‚å¦‚æœæ‚¨ä½¿ç”¨ GPTã€OPT æˆ– BLOOM æ¨¡å‹ï¼Œåˆ™åº”è¯¥è®¾ç½®
 `å¡«å……ä¾§`
 å‘å·¦è½¬;å¦åˆ™å®ƒå°†è¢«è®¾ç½®åœ¨å³ä¾§ã€‚å¯¹å¥å­å¯¹è¿›è¡Œåˆ†è¯å¹¶å°†å…¶æˆªæ–­è‡³æœ€å¤§é•¿åº¦ã€‚



```
if any(k in model_name_or_path for k in ("gpt", "opt", "bloom")):
    padding_side = "left"
else:
    padding_side = "right"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)
if getattr(tokenizer, "pad\_token\_id") is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id


def tokenize\_function(examples):
    # max\_length=None => use the model max length (it's actually the default)
    outputs = tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, max_length=None)
    return outputs
```


ä½¿ç”¨
 `åœ°å›¾`
 æ¥åº”ç”¨
 `æ ‡è®°åŒ–å‡½æ•°`
 åˆ°æ•°æ®é›†ï¼Œå¹¶åˆ é™¤æœªå¤„ç†çš„åˆ—ï¼Œå› ä¸ºæ¨¡å‹ä¸éœ€è¦è¿™äº›åˆ—ã€‚æ‚¨è¿˜åº”è¯¥é‡å‘½å
 `æ ‡ç­¾`
 åˆ—è‡³
 `æ ‡ç­¾`
 å› ä¸ºè¿™æ˜¯ ğŸ¤— Transformers åº“ä¸­æ¨¡å‹æ ‡ç­¾çš„é¢„æœŸåç§°ã€‚



```
tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["idx", "sentence1", "sentence2"],
)

tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
```


åˆ›å»ºä¸€ä¸ªæ•´ç†å™¨å‡½æ•°
 [DataCollatâ€‹â€‹orWithPadding](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/data_collatâ€‹â€‹or#transformers.DataCollatâ€‹â€‹orWithPadding)
 å°†æ‰¹æ¬¡ä¸­çš„ç¤ºä¾‹å¡«å……åˆ°
 â€˜æœ€é•¿â€™
 æ‰¹æ¬¡ä¸­çš„é¡ºåºï¼š



```
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")
```


## ç«è½¦



P-tuning ä½¿ç”¨æç¤ºç¼–ç å™¨æ¥ä¼˜åŒ–æç¤ºå‚æ•°ï¼Œå› æ­¤æ‚¨éœ€è¦åˆå§‹åŒ–
 [PromptEncoderConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.PromptEncoderConfig)
 æœ‰å‡ ä¸ªå‚æ•°ï¼š


* `ä»»åŠ¡ç±»å‹`
 ï¼šæ‚¨æ­£åœ¨è®­ç»ƒçš„ä»»åŠ¡ç±»å‹ï¼Œåœ¨æœ¬ä¾‹ä¸­æ˜¯åºåˆ—åˆ†ç±»æˆ–
 `SEQ_CLS`
* `num_virtual_tokens`
 ï¼šè¦ä½¿ç”¨çš„è™šæ‹Ÿä»¤ç‰Œçš„æ•°é‡ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œæç¤º
* `ç¼–ç å™¨éšè—å¤§å°`
 ï¼šç”¨äºä¼˜åŒ–æç¤ºå‚æ•°çš„ç¼–ç å™¨çš„éšè—å¤§å°



```
peft_config = PromptEncoderConfig(task_type="SEQ\_CLS", num_virtual_tokens=20, encoder_hidden_size=128)
```


åˆ›å»ºåŸºåœ°
 â€œç½—ä¼¯å¡”Â·å¤§å·â€
 æ¨¡å‹æ¥è‡ª
 [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification)
 ï¼Œç„¶ååŒ…è£…åŸºç¡€æ¨¡å‹å¹¶
 `peft_config`
 å’Œ
 `get_peft_model()`
 åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚å¦‚æœæ‚¨æƒ³çŸ¥é“ä¸æ‰€æœ‰æ¨¡å‹å‚æ•°çš„è®­ç»ƒç›¸æ¯”ï¼Œæ‚¨å®é™…è®­ç»ƒäº†å¤šå°‘å‚æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰“å°å‡ºæ¥
 [print\_trainable\_parameters()](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel.print_trainable_parameters)
 :



```
model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
"trainable params: 1351938 || all params: 355662082 || trainable%: 0.38011867680626127"
```


ä» ğŸ¤— Transformers åº“ä¸­ï¼Œè®¾ç½®
 [TrainingArguments](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.TrainingArguments)
 åŒ…å«è¦å°†æ¨¡å‹ä¿å­˜åˆ°çš„ä½ç½®ã€è®­ç»ƒè¶…å‚æ•°ã€å¦‚ä½•è¯„ä¼°æ¨¡å‹ä»¥åŠä½•æ—¶ä¿å­˜æ£€æŸ¥ç‚¹çš„ç±»ï¼š



```
training_args = TrainingArguments(
    output_dir="your-name/roberta-large-peft-p-tuning",
    learning_rate=1e-3,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)
```


ç„¶åé€šè¿‡æ¨¡å‹ï¼Œ
 ã€Šè®­ç»ƒè®ºæ®ã€‹
 ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œè¯„ä¼°å‡½æ•°
 [Trainer](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Trainer)
 è¯¾ç¨‹ï¼Œå®ƒå°†ä¸ºæ‚¨å¤„ç†æ•´ä¸ªåŸ¹è®­å¾ªç¯ã€‚å‡†å¤‡å¥½åï¼Œè¯·è‡´ç”µ
 [ç«è½¦](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Trainer.train)
 å¼€å§‹è®­ç»ƒï¼



```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```


## åˆ†äº«æ¨¡å‹



å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨å¯ä»¥åœ¨ Hub ä¸Šå­˜å‚¨å’Œå…±äº«æ‚¨çš„æ¨¡å‹ã€‚ç™»å½•æ‚¨çš„ Hugging Face å¸æˆ·å¹¶åœ¨å‡ºç°æç¤ºæ—¶è¾“å…¥æ‚¨çš„ä»¤ç‰Œï¼š



```
from huggingface_hub import notebook_login

notebook_login()
```


ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ¨¡å‹ä¸Šä¼ åˆ° Hub ä¸Šçš„ç‰¹å®šæ¨¡å‹å­˜å‚¨åº“
 [push\_to\_hub](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
 åŠŸèƒ½ï¼š



```
model.push_to_hub("your-name/roberta-large-peft-p-tuning", use_auth_token=True)
```


## æ¨ç†



æ¨¡å‹ä¸Šä¼ åˆ° Hub åï¼Œä»»ä½•äººéƒ½å¯ä»¥è½»æ¾ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†ã€‚åŠ è½½é…ç½®å’Œæ¨¡å‹ï¼š



```
import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForSequenceClassification, AutoTokenizer

peft_model_id = "smangrul/roberta-large-peft-p-tuning"
config = PeftConfig.from_pretrained(peft_model_id)
inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(inference_model, peft_model_id)
```


è·å–ä¸€äº›æ–‡æœ¬å¹¶å°†å…¶æ ‡è®°åŒ–ï¼š



```
classes = ["not equivalent", "equivalent"]

sentence1 = "Coast redwood trees are the tallest trees on the planet and can grow over 300 feet tall."
sentence2 = "The coast redwood trees, which can attain a height of over 300 feet, are the tallest trees on earth."

inputs = tokenizer(sentence1, sentence2, truncation=True, padding="longest", return_tensors="pt")
```


å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ä»¥å¯¹å¥å­è¿›è¡Œåˆ†ç±»ï¼š



```
with torch.no_grad():
    outputs = model(**inputs).logits
    print(outputs)

paraphrased_text = torch.softmax(outputs, dim=1).tolist()[0]
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrased\_text[i] \* 100))}%")
"not equivalent: 4%"
"equivalent: 96%"
```