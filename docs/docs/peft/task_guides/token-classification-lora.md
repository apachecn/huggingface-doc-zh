# LoRA ç”¨äºä»£å¸åˆ†ç±»

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/task_guides/token-classification-lora>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/task_guides/token-classification-lora>


ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§é‡å‚æ•°åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å…·æœ‰ä½ç§©è¡¨ç¤ºçš„å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚æƒé‡çŸ©é˜µè¢«åˆ†è§£ä¸ºç»è¿‡è®­ç»ƒå’Œæ›´æ–°çš„ä½ç§©çŸ©é˜µã€‚æ‰€æœ‰é¢„è®­ç»ƒçš„æ¨¡å‹å‚æ•°ä¿æŒå†»ç»“ã€‚è®­ç»ƒåï¼Œä½ç§©çŸ©é˜µè¢«æ·»åŠ å›åŸå§‹æƒé‡ã€‚è¿™ä½¿å¾—å­˜å‚¨å’Œè®­ç»ƒ LoRA æ¨¡å‹æ›´åŠ é«˜æ•ˆï¼Œå› ä¸ºå‚æ•°æ˜æ˜¾å‡å°‘ã€‚


ğŸ’¡é˜…è¯»
 [LoRAï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„ä½ç§©é€‚åº”](https://arxiv.org/abs/2106.09685)
 äº†è§£æœ‰å…³ LoRA çš„æ›´å¤šä¿¡æ¯ã€‚


æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•è®­ç»ƒ
 [`roberta-large`](https://huggingface.co/roberta-large)
 å¸¦æœ‰ LoRA çš„æ¨¡å‹
 [BioNLP2004](https://huggingface.co/datasets/tner/bionlp2004)
 ç”¨äºæ ‡è®°åˆ†ç±»çš„æ•°æ®é›†ã€‚


åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ï¼š



```
!pip install -q peft transformers datasets evaluate seqeval
```


## è®¾ç½®



è®©æˆ‘ä»¬é¦–å…ˆå¯¼å…¥æ‚¨éœ€è¦çš„æ‰€æœ‰å¿…è¦çš„åº“ï¼š


* ğŸ¤— ç”¨äºè£…è½½åº•åº§çš„å˜å‹å™¨
 â€œç½—ä¼¯å¡”Â·å¤§å·â€
 æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œä»¥åŠå¤„ç†è®­ç»ƒå¾ªç¯
* ğŸ¤— ç”¨äºåŠ è½½å’Œå‡†å¤‡çš„æ•°æ®é›†
 `bionlp2004`
 è®­ç»ƒæ•°æ®é›†
* ğŸ¤— Evaluate ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½
* ğŸ¤— PEFT ç”¨äºè®¾ç½® LoRA é…ç½®å¹¶åˆ›å»º PEFT æ¨¡å‹



```
from datasets import load_dataset
from transformers import (
    AutoModelForTokenClassification,
    AutoTokenizer,
    DataCollatorForTokenClassification,
    TrainingArguments,
    Trainer,
)
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
import evaluate
import torch
import numpy as np

model_checkpoint = "roberta-large"
lr = 1e-3
batch_size = 16
num_epochs = 10
```


## åŠ è½½æ•°æ®é›†å’ŒæŒ‡æ ‡



è¿™
 [BioNLP2004](https://huggingface.co/datasets/tner/bionlp2004)
 æ•°æ®é›†åŒ…æ‹¬ DNAã€RNA å’Œè›‹ç™½è´¨ç­‰ç”Ÿç‰©ç»“æ„çš„æ ‡è®°å’Œæ ‡ç­¾ã€‚åŠ è½½æ•°æ®é›†ï¼š



```
bionlp = load_dataset("tner/bionlp2004")
bionlp["train"][0]
{
    "tokens": [
        "Since",
        "HUVECs",
        "released",
        "superoxide",
        "anions",
        "in",
        "response",
        "to",
        "TNF",
        ",",
        "and",
        "H2O2",
        "induces",
        "VCAM-1",
        ",",
        "PDTC",
        "may",
        "act",
        "as",
        "a",
        "radical",
        "scavenger",
        ".",
    ],
    "tags": [0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0],
}
```


è¿™
 `æ ‡ç­¾`
 å€¼åœ¨æ ‡ç­¾ id ä¸­å®šä¹‰
 [å­—å…¸](https://huggingface.co/datasets/tner/bionlp2004#label-id)
 ã€‚æ¯ä¸ªæ ‡ç­¾å‰é¢çš„å­—æ¯è¡¨ç¤ºæ ‡è®°ä½ç½®ï¼š
 'B'
 æ˜¯å®ä½“çš„ç¬¬ä¸€ä¸ªä»¤ç‰Œï¼Œ
 â€˜æˆ‘â€™
 ç”¨äºå®ä½“å†…éƒ¨çš„ä»¤ç‰Œï¼Œå¹¶ä¸”
 `0`
 ç”¨äºä¸å±äºå®ä½“çš„ä»¤ç‰Œã€‚



```
{
    "O": 0,
    "B-DNA": 1,
    "I-DNA": 2,
    "B-protein": 3,
    "I-protein": 4,
    "B-cell\_type": 5,
    "I-cell\_type": 6,
    "B-cell\_line": 7,
    "I-cell\_line": 8,
    "B-RNA": 9,
    "I-RNA": 10,
}
```


ç„¶ååŠ è½½
 [`seqeval`](https://huggingface.co/spaces/evaluate-metric/seqeval)
 æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ç”¨äºè¯„ä¼°åºåˆ—æ ‡è®°ä»»åŠ¡çš„å¤šä¸ªæŒ‡æ ‡ï¼ˆç²¾ç¡®åº¦ã€å‡†ç¡®æ€§ã€F1 å’Œå¬å›ç‡ï¼‰ã€‚



```
seqeval = evaluate.load("seqeval")
```


ç°åœ¨ï¼Œæ‚¨å¯ä»¥ç¼–å†™ä¸€ä¸ªè¯„ä¼°å‡½æ•°æ¥è®¡ç®—æ¨¡å‹é¢„æµ‹å’Œæ ‡ç­¾çš„æŒ‡æ ‡ï¼Œå¹¶è¿”å›ç²¾åº¦ã€å¬å›ç‡ã€F1 å’Œå‡†ç¡®åº¦åˆ†æ•°ï¼š



```
label_list = [
    "O",
    "B-DNA",
    "I-DNA",
    "B-protein",
    "I-protein",
    "B-cell\_type",
    "I-cell\_type",
    "B-cell\_line",
    "I-cell\_line",
    "B-RNA",
    "I-RNA",
]


def compute\_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall\_precision"],
        "recall": results["overall\_recall"],
        "f1": results["overall\_f1"],
        "accuracy": results["overall\_accuracy"],
    }
```


## é¢„å¤„ç†æ•°æ®é›†



åˆå§‹åŒ–åˆ†è¯å™¨å¹¶ç¡®ä¿è®¾ç½®
 `is_split_into_words=True`
 å› ä¸ºæ–‡æœ¬åºåˆ—å·²ç»è¢«åˆ†å‰²æˆå•è¯ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ„å‘³ç€å®ƒå·²ç»è¢«æ ‡è®°åŒ–äº†ï¼ˆå°½ç®¡å®ƒå¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·ï¼ï¼‰ï¼Œå¹¶ä¸”æ‚¨éœ€è¦è¿›ä¸€æ­¥å°†å•è¯æ ‡è®°ä¸ºå­è¯ã€‚



```
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)
```


æ‚¨è¿˜éœ€è¦ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥ï¼š


1. ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ¯ä¸ªæ ‡è®°æ˜ å°„åˆ°å„è‡ªçš„å•è¯
 [word\_ids](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/tokenizer#transformers.BatchEncoding.word_ids)
 æ–¹æ³•ã€‚
2. å¿½ç•¥ç‰¹æ®Šæ ‡è®°ï¼Œå°†å…¶è®¾ç½®ä¸º
 `-100`
 ã€‚
3. æ ‡è®°ç»™å®šå®ä½“çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚



```
def tokenize\_and\_align\_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs
```


ä½¿ç”¨
 `åœ°å›¾`
 æ¥åº”ç”¨
 `tokenize_and_align_labels`
 æ•°æ®é›†çš„å‡½æ•°ï¼š



```
tokenized_bionlp = bionlp.map(tokenize_and_align_labels, batched=True)
```


æœ€åï¼Œåˆ›å»ºä¸€ä¸ªæ•°æ®æ•´ç†å™¨å°†ç¤ºä¾‹å¡«å……åˆ°æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ï¼š



```
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```


## ç«è½¦



ç°åœ¨æ‚¨å·²å‡†å¤‡å¥½åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚é¦–å…ˆåŠ è½½åº•åº§
 â€œç½—ä¼¯å¡”Â·å¤§å·â€
 æ¨¡å‹ã€é¢„æœŸæ ‡ç­¾çš„æ•°é‡ä»¥åŠ
 `id2æ ‡ç­¾`
 å’Œ
 `æ ‡ç­¾2id`
 å­—å…¸ï¼š



```
id2label = {
    0: "O",
    1: "B-DNA",
    2: "I-DNA",
    3: "B-protein",
    4: "I-protein",
    5: "B-cell\_type",
    6: "I-cell\_type",
    7: "B-cell\_line",
    8: "I-cell\_line",
    9: "B-RNA",
    10: "I-RNA",
}
label2id = {
    "O": 0,
    "B-DNA": 1,
    "I-DNA": 2,
    "B-protein": 3,
    "I-protein": 4,
    "B-cell\_type": 5,
    "I-cell\_type": 6,
    "B-cell\_line": 7,
    "I-cell\_line": 8,
    "B-RNA": 9,
    "I-RNA": 10,
}

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint, num_labels=11, id2label=id2label, label2id=label2id
)
```


å®šä¹‰
 [LoraConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraConfig)
 å’Œï¼š


* `ä»»åŠ¡ç±»å‹`
 ï¼Œä»¤ç‰Œåˆ†ç±»ï¼ˆ
 `ä»»åŠ¡ç±»å‹.TOKEN_CLS`
 ï¼‰
* `r`
 ï¼Œä½ç§©çŸ©é˜µçš„ç»´æ•°
* `lora_alpha`
 , æƒé‡çŸ©é˜µçš„æ¯”ä¾‹å› å­
* `lora_dropout`
 , LoRA å±‚çš„ä¸¢å¤±æ¦‚ç‡
*`åè§`
 ï¼Œ è®¾ç½®
 `å…¨éƒ¨`
 è®­ç»ƒæ‰€æœ‰åç½®å‚æ•°


ğŸ’¡ æƒé‡çŸ©é˜µæŒ‰æ¯”ä¾‹ç¼©æ”¾
 `lora_alpha/r`
 ï¼Œä»¥åŠæ›´é«˜çš„
 `lora_alpha`
 value ä¸º LoRA æ¿€æ´»åˆ†é…æ›´å¤šæƒé‡ã€‚ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®è®¾ç½®
 `åè§`
 åˆ°
 `æ— `
 é¦–å…ˆï¼Œç„¶å
 `ä»…åŠ³æ‹‰`
 ï¼Œåœ¨å°è¯•ä¹‹å‰
 `å…¨éƒ¨`
 ã€‚



```
peft_config = LoraConfig(
    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias="all"
)
```


é€šè¿‡åŸºæœ¬æ¨¡å‹å¹¶
 `peft_config`
 åˆ°
 `get_peft_model()`
 å‡½æ•°æ¥åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹ä¸€ä¸‹è®­ç»ƒæ•ˆç‡æœ‰å¤šå°‘
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ä¸é€šè¿‡æ‰“å°å‡ºå¯è®­ç»ƒå‚æ•°æ¥å®Œå…¨è®­ç»ƒåŸºç¡€æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼š



```
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
"trainable params: 1855499 || all params: 355894283 || trainable%: 0.5213624069370061"
```


ä» ğŸ¤— Transformers åº“ä¸­ï¼Œåˆ›å»ºä¸€ä¸ª
 [TrainingArguments](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.TrainingArguments)
 class å¹¶æŒ‡å®šè¦å°†æ¨¡å‹ä¿å­˜åˆ°çš„ä½ç½®ã€è®­ç»ƒè¶…å‚æ•°ã€å¦‚ä½•è¯„ä¼°æ¨¡å‹ä»¥åŠä½•æ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼š



```
training_args = TrainingArguments(
    output_dir="roberta-large-lora-token-classification",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)
```


é€šè¿‡æ¨¡å‹ï¼Œ
 ã€Šè®­ç»ƒè®ºæ®ã€‹
 ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œè¯„ä¼°å‡½æ•°
 [åŸ¹è®­å¸ˆ](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Trainer)
 ç­çº§ã€‚è¿™
 ã€Œæ•™ç»ƒã€
 ä¸ºæ‚¨å¤„ç†è®­ç»ƒå¾ªç¯ï¼Œå½“æ‚¨å‡†å¤‡å¥½æ—¶ï¼Œè¯·è‡´ç”µ
 [ç«è½¦](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Trainer.train)
 å¼€å§‹ï¼



```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_bionlp["train"],
    eval_dataset=tokenized_bionlp["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```


## åˆ†äº«æ¨¡å‹



è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦åœ¨ Hub ä¸Šå­˜å‚¨å’Œå…±äº«æ‚¨çš„æ¨¡å‹ã€‚ç™»å½•æ‚¨çš„ Hugging Face å¸æˆ·å¹¶åœ¨å‡ºç°æç¤ºæ—¶è¾“å…¥æ‚¨çš„ä»¤ç‰Œï¼š



```
from huggingface_hub import notebook_login

notebook_login()
```


ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ¨¡å‹ä¸Šä¼ åˆ° Hub ä¸Šçš„ç‰¹å®šæ¨¡å‹å­˜å‚¨åº“
 [push\_to\_hub](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
 æ–¹æ³•ï¼š



```
model.push_to_hub("your-name/roberta-large-lora-token-classification")
```


## æ¨ç†



è¦ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè¯·åŠ è½½é…ç½®å’Œæ¨¡å‹ï¼š



```
peft_model_id = "stevhliu/roberta-large-lora-token-classification"
config = PeftConfig.from_pretrained(peft_model_id)
inference_model = AutoModelForTokenClassification.from_pretrained(
    config.base_model_name_or_path, num_labels=11, id2label=id2label, label2id=label2id
)
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(inference_model, peft_model_id)
```


è·å–ä¸€äº›è¦æ ‡è®°åŒ–çš„æ–‡æœ¬ï¼š



```
text = "The activation of IL-2 gene expression and NF-kappa B through CD28 requires reactive oxygen production by 5-lipoxygenase."
inputs = tokenizer(text, return_tensors="pt")
```


å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶æ‰“å°å‡ºæ¯ä¸ªæ ‡è®°çš„æ¨¡å‹é¢„æµ‹ï¼š



```
with torch.no_grad():
    logits = model(**inputs).logits

tokens = inputs.tokens()
predictions = torch.argmax(logits, dim=2)

for token, prediction in zip(tokens, predictions[0].numpy()):
    print((token, model.config.id2label[prediction]))
("<s>", "O")
("The", "O")
("Ä activation", "O")
("Ä of", "O")
("Ä IL", "B-DNA")
("-", "O")
("2", "I-DNA")
("Ä gene", "O")
("Ä expression", "O")
("Ä and", "O")
("Ä NF", "B-protein")
("-", "O")
("k", "I-protein")
("appa", "I-protein")
("Ä B", "I-protein")
("Ä through", "O")
("Ä CD", "B-protein")
("28", "I-protein")
("Ä requires", "O")
("Ä reactive", "O")
("Ä oxygen", "O")
("Ä production", "O")
("Ä by", "O")
("Ä 5", "B-protein")
("-", "O")
("lip", "I-protein")
("oxy", "I-protein")
("gen", "I-protein")
("ase", "I-protein")
(".", "O")
("</s>", "O")
```