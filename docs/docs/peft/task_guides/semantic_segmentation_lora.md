# ä½¿ç”¨ LoRA è¿›è¡Œè¯­ä¹‰åˆ†å‰²

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/task_guides/semantic_segmentation_lora>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/task_guides/semantic_segmentation_lora>


æœ¬æŒ‡å—æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LoRAï¼ˆä¸€ç§ä½ç§©è¿‘ä¼¼æŠ€æœ¯ï¼‰æ¥å¾®è°ƒ SegFormer æ¨¡å‹å˜ä½“ä»¥è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚
é€šè¿‡ä½¿ç”¨ğŸ¤— PEFT çš„ LoRAï¼Œæˆ‘ä»¬å¯ä»¥å°† SegFormer æ¨¡å‹ä¸­çš„å¯è®­ç»ƒå‚æ•°æ•°é‡å‡å°‘åˆ°åŸå§‹å¯è®­ç»ƒå‚æ•°çš„ 14%ã€‚


LoRA é€šè¿‡å‘æ¨¡å‹çš„ç‰¹å®šå—ï¼ˆä¾‹å¦‚æ³¨æ„åŠ›ï¼‰æ·»åŠ ä½ç§©â€œæ›´æ–°çŸ©é˜µâ€æ¥å®ç°è¿™ç§å‡å°‘
å—ã€‚å¾®è°ƒæ—¶ï¼Œä»…è®­ç»ƒè¿™äº›çŸ©é˜µï¼Œè€ŒåŸå§‹æ¨¡å‹å‚æ•°ä¿æŒä¸å˜ã€‚
åœ¨æ¨ç†æ—¶ï¼Œæ›´æ–°çŸ©é˜µä¸åŸå§‹æ¨¡å‹å‚æ•°åˆå¹¶ä»¥äº§ç”Ÿæœ€ç»ˆçš„åˆ†ç±»ç»“æœã€‚


æœ‰å…³ LoRA çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…
 [LoRA åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2106.09685)
 ã€‚


## å®‰è£…ä¾èµ–é¡¹



å®‰è£…æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„åº“ï¼š



```
!pip install transformers accelerate evaluate datasets peft -q
```


## éªŒè¯ä»¥å…±äº«æ‚¨çš„æ¨¡å‹



è¦åœ¨åŸ¹è®­ç»“æŸæ—¶ä¸ç¤¾åŒºåˆ†äº«å¾®è°ƒåçš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨æ‚¨çš„ ğŸ¤— ä»¤ç‰Œè¿›è¡Œèº«ä»½éªŒè¯ã€‚
æ‚¨å¯ä»¥ä»æ‚¨çš„
 [è´¦æˆ·è®¾ç½®](https://huggingface.co/settings/token)
 ã€‚



```
from huggingface_hub import notebook_login

notebook_login()
```


## åŠ è½½æ•°æ®é›†



ä¸ºäº†ç¡®ä¿è¯¥ç¤ºä¾‹åœ¨åˆç†çš„æ—¶é—´èŒƒå›´å†…è¿è¡Œï¼Œè¿™é‡Œæˆ‘ä»¬é™åˆ¶è®­ç»ƒçš„å®ä¾‹æ•°é‡
çš„é›†åˆ
 [SceneParse150 æ•°æ®é›†](https://huggingface.co/datasets/scene_parse_150)
 è‡³ 150ã€‚



```
from datasets import load_dataset

ds = load_dataset("scene\_parse\_150", split="train[:150]")
```


æ¥ä¸‹æ¥ï¼Œå°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚



```
ds = ds.train_test_split(test_size=0.1)
train_ds = ds["train"]
test_ds = ds["test"]
```


## å‡†å¤‡æ ‡ç­¾å›¾



åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œå°†æ ‡ç­¾ id æ˜ å°„åˆ°æ ‡ç­¾ç±»ï¼Œè¿™åœ¨ç¨åè®¾ç½®æ¨¡å‹æ—¶ä¼šå¾ˆæœ‰ç”¨ï¼š


* `æ ‡ç­¾2id`
 ï¼šå°†æ•°æ®é›†çš„è¯­ä¹‰ç±»åˆ«æ˜ å°„åˆ°æ•´æ•° idsã€‚
* `id2æ ‡ç­¾`
 ï¼šå°†æ•´æ•° id æ˜ å°„å›è¯­ä¹‰ç±»ã€‚



```
import json
from huggingface_hub import cached_download, hf_hub_url

repo_id = "huggingface/label-files"
filename = "ade20k-id2label.json"
id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
id2label = {int(k): v for k, v in id2label.items()}
label2id = {v: k for k, v in id2label.items()}
num_labels = len(id2label)
```


## å‡†å¤‡ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„æ•°æ®é›†



æ¥ä¸‹æ¥ï¼ŒåŠ è½½ SegFormer å›¾åƒå¤„ç†å™¨æ¥å‡†å¤‡æ¨¡å‹çš„å›¾åƒå’Œæ³¨é‡Šã€‚è¯¥æ•°æ®é›†ä½¿ç”¨
é›¶ç´¢å¼•ä½œä¸ºèƒŒæ™¯ç±»ï¼Œå› æ­¤è¯·ç¡®ä¿è®¾ç½®
 `do_reduce_labels=True`
 ä»æ‰€æœ‰æ ‡ç­¾ä¸­å‡å» 1ï¼Œå› ä¸º
èƒŒæ™¯ç±»ä¸åœ¨150ä¸ªç±»ä¹‹ä¸­ã€‚



```
from transformers import AutoImageProcessor

checkpoint = "nvidia/mit-b0"
image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)
```


æ·»åŠ ä¸€ä¸ªå‡½æ•°ï¼Œå°†æ•°æ®å¢å¼ºåº”ç”¨äºå›¾åƒï¼Œä½¿æ¨¡å‹å¯¹äºè¿‡åº¦æ‹Ÿåˆå…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨
 [ColorJitter](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html)
 å‡½æ•°æ¥è‡ª
 [torchvision](https://pytorch.org/vision/stable/index.html)
 éšæœºæ”¹å˜å›¾åƒçš„é¢œè‰²å±æ€§ã€‚



```
from torchvision.transforms import ColorJitter

jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```


æ·»åŠ å¤„ç†ç°åº¦å›¾åƒçš„å‡½æ•°ï¼Œå¹¶ç¡®ä¿æ¯ä¸ªè¾“å…¥å›¾åƒå…·æœ‰ä¸‰ä¸ªé¢œè‰²é€šé“ï¼Œæ— è®º
åŸæ¥æ˜¯ç°åº¦è¿˜æ˜¯RGBã€‚è¯¥å‡½æ•°å°† RGB å›¾åƒæŒ‰åŸæ ·è½¬æ¢ä¸ºæ•°ç»„ï¼Œå¯¹äºç°åº¦å›¾åƒ
ä»…å…·æœ‰ä¸€ä¸ªé¢œè‰²é€šé“æ—¶ï¼Œè¯¥å‡½æ•°ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†åŒä¸€é€šé“å¤åˆ¶ä¸‰æ¬¡
 `np.tile()`
 è½¬æ¢å‰
å°†å›¾åƒæ”¾å…¥æ•°ç»„ä¸­ã€‚



```
import numpy as np


def handle\_grayscale\_image(image):
    np_image = np.array(image)
    if np_image.ndim == 2:
        tiled_image = np.tile(np.expand_dims(np_image, -1), 3)
        return Image.fromarray(tiled_image)
    else:
        return Image.fromarray(np_image)
```


æœ€åï¼Œå°†æ‰€æœ‰å†…å®¹ç»„åˆåˆ°ä¸¤ä¸ªå‡½æ•°ä¸­ï¼Œç”¨äºè½¬æ¢è®­ç»ƒå’ŒéªŒè¯æ•°æ®ã€‚ä¸¤ä¸ªå‡½æ•°
ç±»ä¼¼ï¼Œåªæ˜¯æ•°æ®å¢å¼ºä»…åº”ç”¨äºè®­ç»ƒæ•°æ®ã€‚



```
from PIL import Image


def train\_transforms(example\_batch):
    images = [jitter(handle_grayscale_image(x)) for x in example_batch["image"]]
    labels = [x for x in example_batch["annotation"]]
    è¾“å…¥= image_processorï¼ˆå›¾åƒï¼Œæ ‡ç­¾ï¼‰
    è¿”å›è¾“å…¥


def val\_transforms(example\_batch):
    images = [handle_grayscale_image(x) for x in example_batch["image"]]
    labels = [x for x in example_batch["annotation"]]
    inputs = image_processor(images, labels)
    return inputs
```


è¦å°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ï¼Œè¯·ä½¿ç”¨ ğŸ¤— æ•°æ®é›†
 `è®¾ç½®å˜æ¢`
 åŠŸèƒ½ï¼š



```
train_ds.set_transform(train_transforms)
test_ds.set_transform(val_transforms)
```


## åˆ›å»ºè¯„ä»·å‡½æ•°



åœ¨è®­ç»ƒæœŸé—´åŒ…å«ä¸€ä¸ªæŒ‡æ ‡æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥åŠ è½½è¯„ä¼°
æ–¹æ³•ä¸
 [ğŸ¤— è¯„ä¼°](https://huggingface.co/docs/evaluate/index)
 å›¾ä¹¦é¦†ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼Œè¯·ä½¿ç”¨
è¿™
 [å¹³å‡äº¤å¹¶é›† (IoU)](https://huggingface.co/spaces/evaluate-metric/accuracy)
 æŒ‡æ ‡ï¼ˆå‚è§ ğŸ¤— è¯„ä¼°
 [å¿«é€Ÿæµè§ˆ](https://huggingface.co/docs/evaluate/a_quick_tour)
 äº†è§£æœ‰å…³å¦‚ä½•åŠ è½½å’Œè®¡ç®—æŒ‡æ ‡çš„æ›´å¤šä¿¡æ¯ï¼‰ï¼š



```
import torch
from torch import nn
import evaluate

metric = evaluate.load("mean\_iou")


def compute\_metrics(eval\_pred):
    with torch.no_grad():
        logits, labels = eval_pred
        logits_tensor = torch.from_numpy(logits)
        logits_tensor = nn.functional.interpolate(
            logits_tensor,
            size=labels.shape[-2:],
            mode="bilinear",
            align_corners=False,
        ).argmax(dim=1)

        pred_labels = logits_tensor.detach().cpu().numpy()
        # currently using \_compute instead of compute
        # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576
        metrics = metric._compute(
            predictions=pred_labels,
            references=labels,
            num_labels=len(id2label),
            ignore_index=0,
            reduce_labels=image_processor.do_reduce_labels,
        )

        per_category_accuracy = metrics.pop("per\_category\_accuracy").tolist()
        per_category_iou = metrics.pop("per\_category\_iou").tolist()

        metrics.update({f"accuracy\_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
        metrics.update({f"iou\_{id2label[i]}": v for i, v in enumerate(per_category_iou)})

        return metrics
```


## åŠ è½½åŸºç¡€æ¨¡å‹



åœ¨åŠ è½½åŸºæœ¬æ¨¡å‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥æ£€æŸ¥æ¨¡å‹å…·æœ‰çš„å‚æ•°æ€»æ•°
å…¶ä¸­æœ‰å¤šå°‘æ˜¯å¯è®­ç»ƒçš„ã€‚



```
def print\_trainable\_parameters(model):
    """
 Prints the number of trainable parameters in the model.
 """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable\_params} || all params: {all\_param} || trainable%: {100 \* trainable\_params / all\_param:.2f}"
    )
```


é€‰æ‹©åŸºæœ¬æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬ä½¿ç”¨
 [SegFormer B0 å˜ä½“](https://huggingface.co/nvidia/mit-b0)
 ã€‚
é™¤æ£€æŸ¥ç«™å¤–ï¼Œè¿˜éœ€é€šè¿‡
 `æ ‡ç­¾2id`
 å’Œ
 `id2æ ‡ç­¾`
 å­—å…¸è®©
 `è¯­ä¹‰åˆ†å‰²è‡ªåŠ¨æ¨¡å‹`
 ç­çº§çŸ¥é“æˆ‘ä»¬æ˜¯
å¯¹è‡ªå®šä¹‰åŸºæœ¬æ¨¡å‹æ„Ÿå…´è¶£ï¼Œå…¶ä¸­åº”ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†ä¸­çš„ç±»éšæœºåˆå§‹åŒ–è§£ç å™¨å¤´ã€‚



```
from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

model = AutoModelForSemanticSegmentation.from_pretrained(
    checkpoint, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True
)
print_trainable_parameters(model)
```


æ­¤æ—¶æ‚¨å¯ä»¥æ£€æŸ¥
 `æ‰“å°å¯è®­ç»ƒå‚æ•°`
 è¾…åŠ©å‡½æ•°ï¼Œæ‰€æœ‰ 100% å‚æ•°éƒ½åœ¨åŸºç¡€ä¸­
æ¨¡å‹ï¼ˆåˆå
 `æ¨¡å‹`
 ï¼‰æ˜¯å¯è®­ç»ƒçš„ã€‚


## å°†åŸºç¡€æ¨¡å‹åŒ…è£…ä¸º PeftModel ä»¥è¿›è¡Œ LoRA è®­ç»ƒ



è¦åˆ©ç”¨ LoRa æ–¹æ³•ï¼Œæ‚¨éœ€è¦å°†åŸºæœ¬æ¨¡å‹åŒ…è£…ä¸º
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ã€‚è¿™æ¶‰åŠä¸¤ä¸ªæ­¥éª¤ï¼š


1. å®šä¹‰ LoRa é…ç½®
 `LoraConfig`
2. åŒ…è£…åŸç¨¿
 `æ¨¡å‹`
 å’Œ
 `get_peft_model()`
 ä½¿ç”¨ä¸Šé¢æ­¥éª¤ä¸­å®šä¹‰çš„é…ç½®ã€‚



```
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=["query", "value"],
    lora_dropout=0.1,
    bias="lora\_only",
    modules_to_save=["decode\_head"],
)
lora_model = get_peft_model(model, config)
print_trainable_parameters(lora_model)
```


è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹
 `LoraConfig`
 ã€‚ä¸ºäº†å¯ç”¨ LoRA æŠ€æœ¯ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨å…¶ä¸­å®šä¹‰ç›®æ ‡æ¨¡å—
 `LoraConfig`
 ä»¥ä¾¿
 `ä½©å¤«ç‰¹æ¨¡å‹`
 å¯ä»¥æ›´æ–°å¿…è¦çš„çŸ©é˜µã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›ç„å‡†
 `æŸ¥è¯¢`
 å’Œ
 `ä»·å€¼`
 çŸ©é˜µåœ¨
åŸºæœ¬æ¨¡å‹çš„æ³¨æ„åŠ›å—ã€‚è¿™äº›çŸ©é˜µç”±å®ƒä»¬å„è‡ªçš„åç§°â€œæŸ¥è¯¢â€å’Œâ€œå€¼â€æ¥æ ‡è¯†ã€‚
å› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨
 `ç›®æ ‡æ¨¡å—`
 çš„è®ºè¯
 `LoraConfig`
 ã€‚


åœ¨æˆ‘ä»¬åŒ…è£…æˆ‘ä»¬çš„åŸºç¡€æ¨¡å‹ä¹‹å
 `æ¨¡å‹`
 å’Œ
 `ä½©å¤«ç‰¹æ¨¡å‹`
 è¿åŒé…ç½®ï¼Œæˆ‘ä»¬å¾—åˆ°
ä¸€ç§æ–°æ¨¡å‹ï¼Œå…¶ä¸­ä»… LoRA å‚æ•°å¯è®­ç»ƒï¼ˆæ‰€è°“çš„â€œæ›´æ–°çŸ©é˜µâ€ï¼‰ï¼Œè€Œé¢„è®­ç»ƒå‚æ•°
è¢«å†·å†»ä¿å­˜ã€‚è¿™äº›ä¹ŸåŒ…æ‹¬éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å™¨å‚æ•°çš„å‚æ•°ã€‚è¿™ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„
åœ¨æˆ‘ä»¬çš„è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒåŸºæœ¬æ¨¡å‹æ—¶ã€‚ä¸ºäº†ç¡®ä¿åˆ†ç±»å™¨å‚æ•°ä¹Ÿå¾—åˆ°è®­ç»ƒï¼Œæˆ‘ä»¬
æŒ‡å®š
 `è¦ä¿å­˜çš„æ¨¡å—`
 ã€‚è¿™ä¹Ÿç¡®ä¿äº†è¿™äº›æ¨¡å—ä¸ LoRA å¯è®­ç»ƒå‚æ•°ä¸€èµ·åºåˆ—åŒ–
å½“ä½¿ç”¨åƒè¿™æ ·çš„å®ç”¨ç¨‹åºæ—¶
 `save_pretrained()`
 å’Œ
 `push_to_hub()`
 ã€‚


é™¤äº†æŒ‡å®š
 `ç›®æ ‡æ¨¡å—`
 ä¹‹å†…
 `LoraConfig`
 ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æŒ‡å®š
 `è¦ä¿å­˜çš„æ¨¡å—`
 ã€‚ä»€ä¹ˆæ—¶å€™
æˆ‘ä»¬ç”¨ä»¥ä¸‹å†…å®¹åŒ…è£…æˆ‘ä»¬çš„åŸºæœ¬æ¨¡å‹
 `ä½©å¤«ç‰¹æ¨¡å‹`
 å¹¶é€šè¿‡é…ç½®ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ–°æ¨¡å‹ï¼Œå…¶ä¸­åªæœ‰LoRAå‚æ•°
æ˜¯å¯è®­ç»ƒçš„ï¼Œè€Œé¢„è®­ç»ƒå‚æ•°å’Œéšæœºåˆå§‹åŒ–çš„åˆ†ç±»å™¨å‚æ•°ä¿æŒå†»ç»“ã€‚
ç„¶è€Œï¼Œæˆ‘ä»¬ç¡®å®æƒ³è®­ç»ƒåˆ†ç±»å™¨å‚æ•°ã€‚é€šè¿‡æŒ‡å®š
 `è¦ä¿å­˜çš„æ¨¡å—`
 è®ºè¯ï¼Œæˆ‘ä»¬ç¡®ä¿
åˆ†ç±»å™¨å‚æ•°ä¹Ÿæ˜¯å¯è®­ç»ƒçš„ï¼Œå½“æˆ‘ä»¬
ä½¿ç”¨å®ç”¨å‡½æ•°ï¼Œä¾‹å¦‚
 `save_pretrained()`
 å’Œ
 `push_to_hub()`
 ã€‚


è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹å…¶ä½™çš„å‚æ•°ï¼š


* `r`
 ï¼šLoRA æ›´æ–°çŸ©é˜µä½¿ç”¨çš„ç»´åº¦ã€‚
* `é˜¿å°”æ³•`
 ï¼šæ¯”ä¾‹å› å­ã€‚
*`åè§`
 ï¼šæŒ‡å®šæ˜¯å¦
 `åè§`
 åº”è¯¥è®­ç»ƒå‚æ•°ã€‚
 `æ— `
 è¡¨ç¤ºæ²¡æœ‰ä¸€ä¸ª
 `åè§`
 å‚æ•°å°†è¢«è®­ç»ƒã€‚


å½“æ‰€æœ‰é…ç½®å®Œæˆå¹¶ä¸”åŸºæœ¬æ¨¡å‹è¢«åŒ…è£…åï¼Œ
 `æ‰“å°å¯è®­ç»ƒå‚æ•°`
 è¾…åŠ©å‡½æ•°è®©æˆ‘ä»¬æ¢ç´¢
å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚å› ä¸ºæˆ‘ä»¬å¯¹è¡¨æ¼”æ„Ÿå…´è¶£
 **å‚æ•°é«˜æ•ˆçš„å¾®è°ƒ**
 ,
æˆ‘ä»¬åº”è¯¥æœŸæœ›çœ‹åˆ°æ›´å°‘æ•°é‡çš„å¯è®­ç»ƒå‚æ•°
 `åŠ³æ‹‰æ¨¡å‹`
 ä¸åŸæ¥ç›¸æ¯”
 `æ¨¡å‹`
 è¿™é‡Œç¡®å®æ˜¯è¿™ç§æƒ…å†µã€‚


æ‚¨è¿˜å¯ä»¥æ‰‹åŠ¨éªŒè¯å“ªäº›æ¨¡å—å¯ä»¥åœ¨
 `åŠ³æ‹‰æ¨¡å‹`
 ã€‚



```
for name, param in lora_model.named_parameters():
    if param.requires_grad:
        print(name, param.shape)
```


è¿™è¯å®äº†åªæœ‰ LoRA å‚æ•°é™„åŠ åˆ°æ³¨æ„åŠ›å—å’Œ
 `è§£ç å¤´`
 å‚æ•°æ˜¯å¯è®­ç»ƒçš„ã€‚


## è®­ç»ƒæ¨¡å‹



é¦–å…ˆå®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°
 ã€Šè®­ç»ƒè®ºæ®ã€‹
 ã€‚ä½†æ˜¯æ‚¨å¯ä»¥æ›´æ”¹å¤§å¤šæ•°å‚æ•°çš„å€¼
ä½ æ¯”è¾ƒå–œæ¬¢ã€‚ç¡®ä¿è®¾ç½®
 `remove_unused_columns=False`
 ï¼Œå¦åˆ™å›¾åƒåˆ—å°†è¢«åˆ é™¤ï¼Œè¿™é‡Œæ˜¯å¿…éœ€çš„ã€‚
å”¯ä¸€çš„å…¶ä»–å¿…éœ€å‚æ•°æ˜¯
 `è¾“å‡ºç›®å½•`
 å®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚
åœ¨æ¯ä¸ªçºªå…ƒç»“æŸæ—¶ï¼Œ
 ã€Œæ•™ç»ƒã€
 å°†è¯„ä¼° IoU æŒ‡æ ‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚


è¯·æ³¨æ„ï¼Œæ­¤ç¤ºä¾‹æ—¨åœ¨å¼•å¯¼æ‚¨å®Œæˆä½¿ç”¨ PEFT è¿›è¡Œè¯­ä¹‰åˆ†å‰²æ—¶çš„å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬æ²¡æœ‰
æ‰§è¡Œå¹¿æ³›çš„è¶…å‚æ•°è°ƒæ•´ä»¥è·å¾—æœ€ä½³ç»“æœã€‚



```
model_name = checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model\_name}-scene-parse-150-lora",
    learning_rate=5e-4,
    num_train_epochs=50,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=2,
    save_total_limit=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_steps=5,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
)
```


å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™
 ã€Œæ•™ç»ƒã€
 ä»¥åŠæ¨¡å‹ã€æ•°æ®é›†å’Œ
 `è®¡ç®—æŒ‡æ ‡`
 åŠŸèƒ½ã€‚
ç§°å‘¼
 `ç«è½¦()`
 å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚



```
trainer = Trainer(
    model=lora_model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)

trainer.train()
```


## ä¿å­˜æ¨¡å‹å¹¶è¿è¡Œæ¨ç†



ä½¿ç”¨
 `save_pretrained()`
 çš„æ–¹æ³•
 `åŠ³æ‹‰æ¨¡å‹`
 æ¥ä¿å­˜
 *ä»…é™ LoRA å‚æ•°*
 æœ¬åœ°ã€‚
æˆ–è€…ï¼Œä½¿ç”¨
 `push_to_hub()`
 æ–¹æ³•å°†è¿™äº›å‚æ•°ç›´æ¥ä¸Šä¼ åˆ° Hugging Face Hub
ï¼ˆå¦‚å›¾æ‰€ç¤º
 [ä½¿ç”¨ LoRA è¿›è¡Œå›¾åƒåˆ†ç±»](image_classification_lora)
 ä»»åŠ¡æŒ‡å—ï¼‰ã€‚



```
model_id = "segformer-scene-parse-150-lora"
lora_model.save_pretrained(model_id)
```


æˆ‘ä»¬å¯ä»¥çœ‹åˆ° LoRA-only å‚æ•°åªæ˜¯
 **å¤§å°ä¸º 2.2 MB**
 ï¼è¿™æå¤§åœ°æé«˜äº†ä½¿ç”¨éå¸¸å¤§çš„æ¨¡å‹æ—¶çš„å¯ç§»æ¤æ€§ã€‚



```
!ls -lh {model_id}
total 2.2M
-rw-r--r-- 1 root root  369 Feb  8 03:09 adapter_config.json
-rw-r--r-- 1 root root 2.2M Feb  8 03:09 adapter_model.bin
```


ç°åœ¨è®©æˆ‘ä»¬å‡†å¤‡ä¸€ä¸ª
 `æ¨ç†æ¨¡å‹`
 å¹¶è¿è¡Œæ¨ç†ã€‚



```
from peft import PeftConfig

config = PeftConfig.from_pretrained(model_id)
model = AutoModelForSemanticSegmentation.from_pretrained(
    checkpoint, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True
)

inference_model = PeftModel.from_pretrained(model, model_id)
```


è·å–å›¾åƒï¼š



```
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png"
image = Image.open(requests.get(url, stream=True).raw)
image
```


![æˆ¿é—´ç…§ç‰‡](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png)


 é¢„å¤„ç†å›¾åƒä»¥å‡†å¤‡æ¨ç†ã€‚



```
encoding = image_processor(image.convert("RGB"), return_tensors="pt")
```


ä½¿ç”¨ç¼–ç å›¾åƒè¿è¡Œæ¨ç†ã€‚



```
with torch.no_grad():
    outputs = inference_model(pixel_values=encoding.pixel_values)
    logits = outputs.logits

upsampled_logits = nn.functional.interpolate(
    logits,
    size=image.size[::-1],
    mode="bilinear",
    align_corners=False,
)

pred_seg = upsampled_logits.argmax(dim=1)[0]
```


æ¥ä¸‹æ¥ï¼Œå¯è§†åŒ–ç»“æœã€‚ä¸ºæ­¤æˆ‘ä»¬éœ€è¦ä¸€ä¸ªè°ƒè‰²æ¿ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ ade\_palette()ã€‚ç”±äºå®ƒæ˜¯ä¸€ä¸ªé•¿æ•°ç»„ï¼Œæ‰€ä»¥
æˆ‘ä»¬æœªå°†å…¶åŒ…å«åœ¨æœ¬æŒ‡å—ä¸­ï¼Œè¯·ä»ä»¥ä¸‹ä½ç½®å¤åˆ¶
 [TensorFlowæ¨¡å‹èŠ±å›­å­˜å‚¨åº“](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)
 ã€‚



```
import matplotlib.pyplot as plt

color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
palette = np.array(ade_palette())

for label, color in enumerate(palette):
    color_seg[pred_seg == label, :] = color
color_seg = color_seg[..., ::-1]  # convert to BGR

img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
img = img.astype(np.uint8)

plt.figure(figsize=(15, 10))
plt.imshow(img)
plt.show()
```


æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œç»“æœè¿œéå®Œç¾ï¼Œä½†æ˜¯ï¼Œæ­¤ç¤ºä¾‹æ—¨åœ¨è¯´æ˜ä»¥ä¸‹å†…å®¹çš„ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹ï¼š
ä½¿ç”¨ LoRa æŠ€æœ¯å¾®è°ƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œå¹¶ä¸æ—¨åœ¨è¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³
ç»“æœã€‚æ‚¨åœ¨æ­¤å¤„çœ‹åˆ°çš„ç»“æœä¸åœ¨ç›¸åŒè®¾ç½®ä¸Šæ‰§è¡Œå®Œå…¨å¾®è°ƒæ—¶è·å¾—çš„ç»“æœç›¸åŒï¼ˆç›¸åŒ
æ¨¡å‹å˜ä½“ã€ç›¸åŒçš„æ•°æ®é›†ã€ç›¸åŒçš„è®­ç»ƒè®¡åˆ’ç­‰ï¼‰ï¼Œä½† LoRA å…è®¸ç”¨æ€»æ•°çš„ä¸€å°éƒ¨åˆ†æ¥å®ç°å®ƒä»¬
å¯åœ¨æ›´çŸ­çš„æ—¶é—´å†…è®­ç»ƒå‚æ•°ã€‚


å¦‚æœæ‚¨å¸Œæœ›ä½¿ç”¨æ­¤ç¤ºä¾‹å¹¶æ”¹è¿›ç»“æœï¼Œæ‚¨å¯ä»¥å°è¯•ä»¥ä¸‹ä¸€äº›æ“ä½œï¼š


* å¢åŠ è®­ç»ƒæ ·æœ¬æ•°é‡ã€‚
* å°è¯•æ›´å¤§çš„ SegFormer æ¨¡å‹å˜ä½“ï¼ˆæ¢ç´¢å¯ç”¨çš„æ¨¡å‹å˜ä½“
 [æ‹¥æŠ±è„¸éƒ¨ä¸­å¿ƒ](https://huggingface.co/models?search=segformer)
 ï¼‰ã€‚
* å°è¯•ä½¿ç”¨ä¸åŒçš„å‚æ•°å€¼
 `LoraConfig`
 ã€‚
* è°ƒæ•´å­¦ä¹ ç‡å’Œæ‰¹é‡å¤§å°ã€‚