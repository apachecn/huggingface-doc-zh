# æ¡ä»¶ç”Ÿæˆçš„å‰ç¼€è°ƒæ•´

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/task_guides/seq2seq-prefix-tuning>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/task_guides/seq2seq-prefix-tuning>


![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/assets/colab-badge.svg)


![åœ¨ Studio Lab ä¸­æ‰“å¼€](https://studiolab.sagemaker.aws/studiolab.svg)


å‰ç¼€è°ƒæ•´æ˜¯ä¸€ç§é™„åŠ æ–¹æ³•ï¼Œå…¶ä¸­ä»…å°†ä¸€ç³»åˆ—è¿ç»­çš„ç‰¹å®šäºä»»åŠ¡çš„å‘é‡é™„åŠ åˆ°è¾“å…¥çš„å¼€å¤´ï¼Œæˆ–è€…
 *å­—é¦–*
 ã€‚ä»…ä¼˜åŒ–å‰ç¼€å‚æ•°å¹¶å°†å…¶æ·»åŠ åˆ°æ¨¡å‹æ¯ä¸€å±‚çš„éšè—çŠ¶æ€ä¸­ã€‚è¾“å…¥åºåˆ—çš„æ ‡è®°ä»ç„¶å¯ä»¥å…³æ³¨å‰ç¼€ï¼š
 *è™šæ‹Ÿä»£å¸*
 ã€‚å› æ­¤ï¼Œå‰ç¼€è°ƒä¼˜å­˜å‚¨çš„å‚æ•°æ¯”å®Œå…¨å¾®è°ƒçš„æ¨¡å‹å°‘ 1000 å€ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹æ¥å®Œæˆè®¸å¤šä»»åŠ¡ã€‚


ğŸ’¡é˜…è¯»
 [å‰ç¼€è°ƒä¼˜ï¼šä¼˜åŒ–ç”Ÿæˆè¿ç»­æç¤º](https://arxiv.org/abs/2101.00190)
 äº†è§£æœ‰å…³å‰ç¼€è°ƒæ•´çš„æ›´å¤šä¿¡æ¯ã€‚


æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åº”ç”¨å‰ç¼€è°ƒæ•´æ¥è®­ç»ƒ
 [`t5-large`](https://huggingface.co/t5-large)
 æ¨¡å‹ä¸Šçš„
 `sentences_allagree`
 çš„å­é›†
 [financial\_phrasebank](https://huggingface.co/datasets/financial_phrasebank)
 æ•°æ®é›†ã€‚


åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ï¼š



```
!pip install -q peft transformers datasets
```


## è®¾ç½®



é¦–å…ˆå®šä¹‰æ¨¡å‹å’Œåˆ†è¯å™¨ã€æ–‡æœ¬å’Œæ ‡ç­¾åˆ—ä»¥åŠä¸€äº›è¶…å‚æ•°ï¼Œä»¥ä¾¿ä»¥åæ›´å®¹æ˜“æ›´å¿«åœ°å¼€å§‹è®­ç»ƒã€‚è®¾ç½®ç¯å¢ƒå˜é‡
 `TOKENIZERS_PARALLELSIM`
 åˆ°
 `å‡`
 ç¦ç”¨é»˜è®¤æƒ…å†µä¸‹å¹¶è¡Œå¤„ç†æ•°æ®çš„åŸºäº Rust çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥åœ¨ Python ä¸­ä½¿ç”¨å¤šé‡å¤„ç†ã€‚



```
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup
from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType
from datasets import load_dataset
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch
import os

os.environ["TOKENIZERS\_PARALLELISM"] = "false"
os.environ["CUDA\_VISIBLE\_DEVICES"] = "3"

device = "cuda"
model_name_or_path = "t5-large"
tokenizer_name_or_path = "t5-large"

text_column = "sentence"
label_column = "text\_label"
max_length = 128
lr = 1e-2
num_epochs = 5
batch_size = 8
```


## åŠ è½½æ•°æ®é›†



å¯¹äºæœ¬æŒ‡å—ï¼Œæ‚¨å°†æ¥å—ä»¥ä¸‹åŸ¹è®­ï¼š
 `sentences_allagree`
 çš„å­é›†
 [`financial_phrasebank`](https://huggingface.co/datasets/financial_phrasebank)
 æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«æŒ‰æƒ…ç»ªåˆ†ç±»çš„è´¢ç»æ–°é—»ã€‚


ä½¿ç”¨ğŸ¤—
 [æ•°æ®é›†](https://huggingface.co/docs/datasets/index)
`train_test_split`
 å‡½æ•°åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯åˆ†å‰²å¹¶è½¬æ¢
 `æ ‡ç­¾`
 æ›´å…·å¯è¯»æ€§çš„ä»·å€¼
 `æ–‡æœ¬æ ‡ç­¾`
 ã€‚æ‰€æœ‰æ›´æ”¹éƒ½å¯ä»¥åº”ç”¨
 `åœ°å›¾`
 åŠŸèƒ½ï¼š



```
from datasets import load_dataset

dataset = load_dataset("financial\_phrasebank", "sentences\_allagree")
dataset = dataset["train"].train_test_split(test_size=0.1)
dataset["validation"] = dataset["test"]
del dataset["test"]

classes = dataset["train"].features["label"].names
dataset = dataset.map(
    lambda x: {"text\_label": [classes[label] for label in x["label"]]},
    batched=True,
    num_proc=1,
)

dataset["train"][0]
{"sentence": "Profit before taxes was EUR 4.0 mn , down from EUR 4.9 mn .", "label": 0, "text\_label": "negative"}
```


## é¢„å¤„ç†æ•°æ®é›†



åˆå§‹åŒ–ä¸€ä¸ªåˆ†è¯å™¨ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å¡«å……å’Œæˆªæ–­
 `æ¨¡å‹è¾“å…¥`
 å’Œ
 `æ ‡ç­¾`
 :



```
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)


def preprocess\_function(examples):
    inputs = examples[text_column]
    targets = examples[label_column]
    model_inputs = tokenizer(inputs, max_length=max_length, padding="max\_length", truncation=True, return_tensors="pt")
    labels = tokenizer(targets, max_length=2, padding="max\_length", truncation=True, return_tensors="pt")
    labels = labels["input\_ids"]
    labels[labels == tokenizer.pad_token_id] = -100
    model_inputs["labels"] = labels
    return model_inputs
```


ä½¿ç”¨
 `åœ°å›¾`
 å‡½æ•°æ¥åº”ç”¨
 `é¢„å¤„ç†å‡½æ•°`
 åˆ°æ•°æ®é›†ã€‚æ‚¨å¯ä»¥åˆ é™¤æœªå¤„ç†çš„åˆ—ï¼Œå› ä¸ºæ¨¡å‹ä¸å†éœ€è¦å®ƒä»¬ï¼š



```
processed_datasets = dataset.map(
    preprocess_function,
    batched=True,
    num_proc=1,
    remove_columns=dataset["train"].column_names,
    load_from_cache_file=False,
    desc="Running tokenizer on dataset",
)
```


åˆ›å»ºä¸€ä¸ª
 [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
 æ¥è‡ª
 `ç«è½¦`
 å’Œ
 `è¯„ä¼°`
 æ•°æ®é›†ã€‚æ”¾
 `pin_memory=True`
 å¦‚æœæ•°æ®é›†ä¸­çš„æ ·æœ¬ä½äº CPU ä¸Šï¼Œåˆ™å¯ä»¥åœ¨è®­ç»ƒæœŸé—´åŠ å¿«æ•°æ®ä¼ è¾“åˆ° GPU çš„é€Ÿåº¦ã€‚



```
train_dataset = processed_datasets["train"]
eval_dataset = processed_datasets["validation"]

train_dataloader = DataLoader(
    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True
)
eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)
```


## ç«è½¦æ¨¡å‹



ç°åœ¨æ‚¨å¯ä»¥è®¾ç½®æ¨¡å‹å¹¶ç¡®ä¿å®ƒå·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒã€‚æŒ‡å®šä»»åŠ¡åœ¨
 [PrefixTuningConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.PrefixTuningConfig)
 ï¼Œåˆ›å»ºåŸºç¡€
 `t5-å¤§å·`
 æ¨¡å‹æ¥è‡ª
 [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM)
 ï¼Œç„¶åå°†æ¨¡å‹å’Œé…ç½®åŒ…è£…åœ¨
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚è¯·éšæ„æ‰“å°
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 çš„å‚æ•°å¹¶ä¸å®Œå…¨è®­ç»ƒæ‰€æœ‰æ¨¡å‹å‚æ•°è¿›è¡Œæ¯”è¾ƒï¼Œçœ‹çœ‹æ•ˆç‡æé«˜äº†å¤šå°‘ï¼



```
peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
"trainable params: 983040 || all params: 738651136 || trainable%: 0.13308583065659835"
```


è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š



```
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
lr_scheduler = get_linear_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=(len(train_dataloader) * num_epochs),
)
```


å°†æ¨¡å‹ç§»è‡³ GPUï¼Œç„¶åç¼–å†™è®­ç»ƒå¾ªç¯æ¥å¼€å§‹ï¼



```
model = model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for step, batch in enumerate(tqdm(train_dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        total_loss += loss.detach().float()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()

    model.eval()
    eval_loss = 0
    eval_preds = []
    for step, batch in enumerate(tqdm(eval_dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        loss = outputs.loss
        eval_loss += loss.detach().float()
        eval_preds.extend(
            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)
        )

    eval_epoch_loss = eval_loss / len(eval_dataloader)
    eval_ppl = torch.exp(eval_epoch_loss)
    train_epoch_loss = total_loss / len(train_dataloader)
    train_ppl = torch.exp(train_epoch_loss)
    print(f"{epoch=}: {train\_ppl=} {train\_epoch\_loss=} {eval\_ppl=} {eval\_epoch\_loss=}")
```


è®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°å¦‚ä½•ï¼š



```
correct = 0
total = 0
for pred, true in zip(eval_preds, dataset["validation"]["text\_label"]):
    if pred.strip() == true.strip():
        correct += 1
    total += 1
accuracy = correct / total * 100
print(f"{accuracy=} % on the evaluation dataset")
print(f"{eval\_preds[:10]=}")
print(f"{dataset['validation']['text\_label'][:10]=}")
"accuracy=97.3568281938326 % on the evaluation dataset"
"eval\_preds[:10]=['neutral', 'positive', 'neutral', 'positive', 'neutral', 'negative', 'negative', 'neutral', 'neutral', 'neutral']"
"dataset['validation']['text\_label'][:10]=['neutral', 'positive', 'neutral', 'positive', 'neutral', 'negative', 'negative', 'neutral', 'neutral', 'neutral']"
```


åªéœ€å‡ åˆ†é’Ÿå³å¯è¾¾åˆ° 97% çš„å‡†ç¡®ç‡ï¼›ä¸é”™ï¼


## åˆ†äº«æ¨¡å‹



å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨å¯ä»¥åœ¨ Hub ä¸Šå­˜å‚¨å’Œå…±äº«æ‚¨çš„æ¨¡å‹ã€‚ç™»å½•æ‚¨çš„ Hugging Face å¸æˆ·å¹¶åœ¨å‡ºç°æç¤ºæ—¶è¾“å…¥æ‚¨çš„ä»¤ç‰Œï¼š



```
from huggingface_hub import notebook_login

notebook_login()
```


ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ¨¡å‹ä¸Šä¼ åˆ° Hub ä¸Šçš„ç‰¹å®šæ¨¡å‹å­˜å‚¨åº“
 [push\_to\_hub](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
 åŠŸèƒ½ï¼š



```
peft_model_id = "your-name/t5-large\_PREFIX\_TUNING\_SEQ2SEQ"
model.push_to_hub("your-name/t5-large\_PREFIX\_TUNING\_SEQ2SEQ", use_auth_token=True)
```


å¦‚æœæ‚¨æ£€æŸ¥å­˜å‚¨åº“ä¸­çš„æ¨¡å‹æ–‡ä»¶å¤§å°ï¼Œæ‚¨ä¼šå‘ç°å®ƒåªæœ‰ 3.93MBï¼ ğŸ¤


## æ¨ç†



æ¨¡å‹ä¸Šä¼ åˆ° Hub åï¼Œä»»ä½•äººéƒ½å¯ä»¥è½»æ¾ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†ã€‚åŠ è½½é…ç½®å’Œæ¨¡å‹ï¼š



```
from peft import PeftModel, PeftConfig

peft_model_id = "stevhliu/t5-large\_PREFIX\_TUNING\_SEQ2SEQ"

config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(model, peft_model_id)
```


è·å–å¹¶æ ‡è®°ä¸€äº›æœ‰å…³è´¢ç»æ–°é—»çš„æ–‡æœ¬ï¼š



```
inputs = tokenizer(
    "The Lithuanian beer market made up 14.41 million liters in January , a rise of 0.8 percent from the year-earlier figure , the Lithuanian Brewers ' Association reporting citing the results from its members .",
    return_tensors="pt",
)
```


å°†æ¨¡å‹æ”¾åœ¨ GPU ä¸Šå¹¶
 *äº§ç”Ÿ*
 é¢„æµ‹çš„æ–‡æœ¬æƒ…ç»ªï¼š



```
model.to(device)

with torch.no_grad():
    inputs = {k: v.to(device) for k, v in inputs.items()}
    outputs = model.generate(input_ids=inputs["input\_ids"], max_new_tokens=10)
    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))
["positive"]
```