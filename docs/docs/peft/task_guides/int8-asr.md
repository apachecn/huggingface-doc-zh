# è‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„int8è®­ç»ƒ

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/task_guides/int8-asr>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/task_guides/int8-asr>


é‡åŒ–ä¼šé™ä½æµ®ç‚¹æ•°æ®ç±»å‹çš„ç²¾åº¦ï¼Œä»è€Œå‡å°‘å­˜å‚¨æ¨¡å‹æƒé‡æ‰€éœ€çš„å†…å­˜ã€‚ä½†æ˜¯ï¼Œé‡åŒ–ä¼šé™ä½æ¨ç†æ€§èƒ½ï¼Œå› ä¸ºé™ä½ç²¾åº¦æ—¶ä¼šä¸¢å¤±ä¿¡æ¯ã€‚ 8 ä½æˆ–
 `int8`
 é‡åŒ–ä»…ä½¿ç”¨å››åˆ†ä¹‹ä¸€ç²¾åº¦ï¼Œä½†å®ƒä¸ä¼šé™ä½æ€§èƒ½ï¼Œå› ä¸ºå®ƒä¸åªæ˜¯ä¸¢å¼ƒä½æˆ–æ•°æ®ã€‚åè€Œï¼Œ
 `int8`
 é‡åŒ–
 *å›åˆ*
 ä»ä¸€ç§æ•°æ®ç±»å‹è½¬æ¢ä¸ºå¦ä¸€ç§æ•°æ®ç±»å‹ã€‚


ğŸ’¡ é˜…è¯»
 [LLM.int8()ï¼šå¤§è§„æ¨¡ Transformer çš„ 8 ä½çŸ©é˜µä¹˜æ³•](https://arxiv.org/abs/2208.07339)
 è®ºæ–‡äº†è§£æ›´å¤šï¼Œæˆ–è€…ä½ å¯ä»¥çœ‹çœ‹ç›¸åº”çš„
 [åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)
 è¿›è¡Œæ›´æ¸©å’Œçš„ä»‹ç»ã€‚


æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•è®­ç»ƒ
 [`openai/whisper-large-v2`](https://huggingface.co/openai/whisper-large-v2)
 ä½¿ç”¨ä»¥ä¸‹ç»„åˆçš„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ« (ASR) æ¨¡å‹
 `int8`
 é‡åŒ–å’Œ LoRAã€‚æ‚¨å°†ä»ä»¥ä¸‹ä½ç½®è®­ç»ƒ Whisper è¿›è¡Œé©¬æ‹‰åœ°è¯­å¤šè¯­è¨€ ASRï¼š
 [é€šç”¨è¯­éŸ³ 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)
 æ•°æ®é›†ã€‚


åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ï¼š



```
!pip install -q peft transformers datasets accelerate evaluate jiwer bitsandbytes
```


## è®¾ç½®



è®©æˆ‘ä»¬å…ˆè¿›è¡Œä¸€äº›è®¾ç½®ï¼Œä»¥ä¾¿æ‚¨ç¨åå¯ä»¥æ›´å¿«åœ°å¼€å§‹è®­ç»ƒã€‚è®¾ç½®
 `CUDA_VISIBLE_DEVICES`
 åˆ°
 `0`
 ä½¿ç”¨æœºå™¨ä¸Šçš„ç¬¬ä¸€ä¸ª GPUã€‚ç„¶åï¼Œæ‚¨å¯ä»¥æŒ‡å®šæ¨¡å‹åç§°ï¼ˆHub æ¨¡å‹å­˜å‚¨åº“ ID æˆ–åŒ…å«æ¨¡å‹çš„ç›®å½•çš„è·¯å¾„ï¼‰ã€è¦è®­ç»ƒçš„è¯­è¨€å’Œè¯­è¨€ç¼©å†™ã€ä»»åŠ¡ç±»å‹å’Œæ•°æ®é›†åç§°ï¼š



```
import os

os.environ["CUDA\_VISIBLE\_DEVICES"] = "0"
model_name_or_path = "openai/whisper-large-v2"
language = "Marathi"
language_abbr = "mr"
task = "transcribe"
dataset_name = "mozilla-foundation/common\_voice\_11\_0"
```


å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨è¿˜å¯ä»¥ç™»å½•æ‚¨çš„ Hugging Face å¸æˆ·ï¼Œåœ¨ Hub ä¸Šä¿å­˜å¹¶åˆ†äº«ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ï¼š



```
from huggingface_hub import notebook_login

notebook_login()
```


## åŠ è½½æ•°æ®é›†å’ŒæŒ‡æ ‡



è¿™
 [é€šç”¨è¯­éŸ³ 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)
 æ•°æ®é›†åŒ…å«è®¸å¤šå°æ—¶çš„å¤šç§ä¸åŒè¯­è¨€çš„å½•éŸ³è¯­éŸ³ã€‚æœ¬æŒ‡å—ä½¿ç”¨
 [é©¬æ‹‰åœ°è¯­](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/mr/train)
 è¯­è¨€ä½œä¸ºç¤ºä¾‹ï¼Œä½†è¯·éšæ„ä½¿ç”¨æ‚¨æ„Ÿå…´è¶£çš„ä»»ä½•å…¶ä»–è¯­è¨€ã€‚


åˆå§‹åŒ–ä¸€ä¸ª
 `æ•°æ®é›†å­—å…¸`
 ç»“æ„ï¼Œå¹¶åŠ è½½
 `ç«è½¦`
 ï¼ˆåŠ è½½ä¸¤ä¸ª
 `è®­ç»ƒ+éªŒè¯`
 åˆ†æˆ
 `ç«è½¦`
 ï¼‰ å’Œ
 `æµ‹è¯•`
 ä»æ•°æ®é›†åˆ†å‰²æˆï¼š



```
from datasets import load_dataset
from datasets import load_dataset, DatasetDict

common_voice = DatasetDict()

common_voice["train"] = load_dataset(dataset_name, language_abbr, split="train+validation", use_auth_token=True)
common_voice["test"] = load_dataset(dataset_name, language_abbr, split="test", use_auth_token=True)
common_voice["train"][0]
```


## é¢„å¤„ç†æ•°æ®é›†



è®©æˆ‘ä»¬å‡†å¤‡è®­ç»ƒæ•°æ®é›†ã€‚åŠ è½½ç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨å’Œå¤„ç†å™¨ã€‚æ‚¨è¿˜åº”è¯¥å°†è¯­è¨€å’Œä»»åŠ¡ä¼ é€’ç»™åˆ†è¯å™¨å’Œå¤„ç†å™¨ï¼Œä»¥ä¾¿å®ƒä»¬çŸ¥é“å¦‚ä½•å¤„ç†è¾“å…¥ï¼š



```
from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor

feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, language=language, task=task)
processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)
```


æ‚¨å°†åªæ¥å—ä»¥ä¸‹æ–¹é¢çš„åŸ¹è®­ï¼š
 `å¥å­`
 å’Œ
 `éŸ³é¢‘`
 åˆ—ï¼Œå› æ­¤æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ é™¤å…¶ä½™å…ƒæ•°æ®
 `åˆ é™¤_åˆ—`
 ï¼š



```
common_voice = common_voice.remove_columns(
    ["accent", "age", "client\_id", "down\_votes", "gender", "locale", "path", "segment", "up\_votes"]
)
common_voice["train"][0]
{
    "audio": {
        "path": "/root/.cache/huggingface/datasets/downloads/extracted/f7e1ef6a2d14f20194999aad5040c5d4bb3ead1377de3e1bbc6e9dba34d18a8a/common\_voice\_mr\_30585613.mp3",
        "array": array(
            [1.13686838e-13, -1.42108547e-13, -1.98951966e-13, ..., 4.83472422e-06, 3.54798703e-06, 1.63231743e-06]
        ),
        "sampling\_rate": 48000,
    },
    "sentence": "à¤†à¤ˆà¤šà¥‡ à¤†à¤œà¤¾à¤°à¤ªà¤£ à¤µà¤¾à¤¢à¤¤ à¤šà¤¾à¤²à¤²à¥‡, à¤¤à¤¸à¤¤à¤¶à¥€ à¤®à¤¥à¥€à¤¹à¥€ à¤¨à¥€à¤Ÿ à¤–à¤¾à¤¤à¤ªà¥€à¤¤à¤¨à¤¾à¤¶à¥€ à¤à¤¾à¤²à¥€.",
}
```


å¦‚æœä½ çœ‹ä¸€ä¸‹
 `é‡‡æ ·ç‡`
 ï¼Œæ‚¨ä¼šçœ‹åˆ°éŸ³é¢‘æ˜¯ä»¥ 48kHz é‡‡æ ·çš„ã€‚ Whisper æ¨¡å‹åœ¨ 16kHZ çš„éŸ³é¢‘è¾“å…¥ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¿™æ„å‘³ç€æ‚¨éœ€è¦å¯¹éŸ³é¢‘è¾“å…¥è¿›è¡Œä¸‹é‡‡æ ·ä»¥åŒ¹é…æ¨¡å‹çš„é¢„è®­ç»ƒå†…å®¹ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯¹éŸ³é¢‘è¿›è¡Œä¸‹é‡‡æ ·
 `cast_column`
 æ–¹æ³•ä¸Šçš„
 `éŸ³é¢‘`
 åˆ—ï¼Œå¹¶è®¾ç½®
 `é‡‡æ ·ç‡`
 è‡³ 16kHzã€‚ä¸‹æ¬¡è°ƒç”¨æ—¶ï¼ŒéŸ³é¢‘è¾“å…¥ä¼šç«‹å³é‡æ–°é‡‡æ ·ï¼š



```
from datasets import Audio

common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000))
common_voice["train"][0]
{
    "audio": {
        "path": "/root/.cache/huggingface/datasets/downloads/extracted/f7e1ef6a2d14f20194999aad5040c5d4bb3ead1377de3e1bbc6e9dba34d18a8a/common\_voice\_mr\_30585613.mp3",
        "array": array(
            [-3.06954462e-12, -3.63797881e-12, -4.54747351e-12, ..., -7.74800901e-06, -1.74738125e-06, 4.36312439e-06]
        ),
        "sampling\_rate": 16000,
    },
    "sentence": "à¤†à¤ˆà¤šà¥‡ à¤†à¤œà¤¾à¤°à¤ªà¤£ à¤µà¤¾à¤¢à¤¤ à¤šà¤¾à¤²à¤²à¥‡, à¤¤à¤¸à¤¤à¤¶à¥€ à¤®à¤¥à¥€à¤¹à¥€ à¤¨à¥€à¤Ÿ à¤–à¤¾à¤¤à¤ªà¥€à¤¤à¤¨à¤¾à¤¶à¥€ à¤à¤¾à¤²à¥€.",
}
```


æ¸…ç†æ•°æ®é›†åï¼Œæ‚¨å¯ä»¥ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥ç”Ÿæˆæ­£ç¡®çš„æ¨¡å‹è¾“å…¥ã€‚è¯¥å‡½æ•°åº”è¯¥ï¼š


1. é€šè¿‡åŠ è½½å°†éŸ³é¢‘è¾“å…¥é‡æ–°é‡‡æ ·åˆ° 16kHZ
 `éŸ³é¢‘`
 æŸ±å­ã€‚
2. è®¡ç®—éŸ³é¢‘çš„è¾“å…¥ç‰¹å¾
 `æ•°ç»„`
 ä½¿ç”¨ç‰¹å¾æå–å™¨ã€‚
3. ä»£å¸åŒ–
 `å¥å­`
 åˆ—åˆ°è¾“å…¥æ ‡ç­¾ã€‚



```
def prepare\_dataset(batch):
    audio = batch["audio"]
    batch["input\_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling\_rate"]).input_features[0]
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch
```


åº”ç”¨
 `å‡†å¤‡æ•°æ®é›†`
 å‡½æ•°åˆ°æ•°æ®é›†
 `åœ°å›¾`
 åŠŸèƒ½ï¼Œå¹¶è®¾ç½®
 `num_proc`
 è®ºè¯
 `2`
 å¯ç”¨å¤šå¤„ç†ï¼ˆå¦‚æœ
 `åœ°å›¾`
 æŒ‚èµ·ï¼Œç„¶åè®¾ç½®
 `num_proc=1`
 ï¼‰ï¼š



```
common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names["train"], num_proc=2)
```


æœ€åï¼Œåˆ›å»ºä¸€ä¸ª
 `æ•°æ®æ•´ç†å™¨`
 ç±»å°†æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„æ ‡ç­¾å¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œå¹¶å°†å¡«å……æ›¿æ¢ä¸º
 `-100`
 æ‰€ä»¥å®ƒä»¬è¢«æŸå¤±å‡½æ•°å¿½ç•¥äº†ã€‚ç„¶ååˆå§‹åŒ–æ•°æ®æ•´ç†å™¨çš„å®ä¾‹ï¼š



```
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union


@æ•°æ®ç±»
ç±» DataCollatâ€‹â€‹orSpeechSeq2SeqWithPaddingï¼š
    å¤„ç†å™¨ï¼šä»»ä½•

    def \_\_call\_\_(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input\_features": feature["input\_features"]} ç”¨äºç‰¹å¾ä¸­çš„ç‰¹å¾]
        æ‰¹å¤„ç†= self.processor.feature_extractor.padï¼ˆinput_featuresï¼Œreturn_tensors =â€œptâ€ï¼‰

        label_features = [{"input\_ids": feature["labels"]} ç”¨äºç‰¹å¾ä¸­çš„ç‰¹å¾]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        æ ‡ç­¾ = labels_batch["input\_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            æ ‡ç­¾ = æ ‡ç­¾[:, 1:]

        æ‰¹æ¬¡[â€œæ ‡ç­¾â€] = æ ‡ç­¾

        é€€è´§æ‰¹æ¬¡


data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
```


## ç«è½¦



ç°åœ¨æ•°æ®é›†å·²å‡†å¤‡å°±ç»ªï¼Œæ‚¨å¯ä»¥å°†æ³¨æ„åŠ›è½¬å‘æ¨¡å‹ã€‚é¦–å…ˆåŠ è½½é¢„è®­ç»ƒçš„
 `openai/whisper-large-v2`
 æ¨¡å‹æ¥è‡ª
 [AutoModelForSpeechSeq2Seq](https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/auto#transformers.AutoModelForSpeechSeq2Seq)
 ï¼Œå¹¶ç¡®ä¿è®¾ç½®
 `load_in_8bit`
 è®ºè¯
 'çœŸå®'
 å¯ç”¨
 `int8`
 é‡åŒ–ã€‚è¿™
 `device_map=è‡ªåŠ¨`
 å‚æ•°è‡ªåŠ¨ç¡®å®šå¦‚ä½•åŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ï¼š



```
from transformers import AutoModelForSpeechSeq2Seq

model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path, load_in_8bit=True, device_map="auto")
```


ä½ åº”è¯¥é…ç½®
 `forced_decoder_ids=æ— `
 å› ä¸ºåœ¨é‡‡æ ·ä¹‹å‰ä¸ä½¿ç”¨ä»»ä½•æ ‡è®°ï¼Œå¹¶ä¸”æ‚¨ä¹Ÿä¸éœ€è¦åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æŠ‘åˆ¶ä»»ä½•æ ‡è®°ï¼š



```
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
```


ä¸ºæ¨¡å‹åšå¥½å‡†å¤‡
 `int8`
 é‡åŒ–ï¼Œä½¿ç”¨æ•ˆç”¨å‡½æ•°
 [`prepare_model_for_int8_training`](https://github.com/huggingface/peft/blob/34027fe813756897767b9a6f19ae7f1c4c7b418c/src/peft/utils/other.py#L35)
 å¤„ç†ä»¥ä¸‹äº‹é¡¹ï¼š


* æŠ•å°„æ‰€æœ‰é
 `int8`
 æ¨¡å—è¾¾åˆ°å…¨ç²¾åº¦ï¼ˆ
 `fp32`
 ï¼‰ä¸ºäº†ç¨³å®šæ€§
* åœ¨è¾“å…¥åµŒå…¥å±‚æ·»åŠ ä¸€ä¸ªå‰å‘é’©å­æ¥è®¡ç®—è¾“å…¥éšè—çŠ¶æ€çš„æ¢¯åº¦
* å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å®ç°æ›´é«˜æ•ˆçš„å†…å­˜è®­ç»ƒ



```
from peft import prepare_model_for_int8_training

model = prepare_model_for_int8_training(model)
```


æˆ‘ä»¬è¿˜å°† LoRA åº”ç”¨åˆ°è®­ç»ƒä¸­ï¼Œä½¿å…¶æ›´åŠ é«˜æ•ˆã€‚åŠ è½½ä¸€ä¸ª
 [LoraConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraConfig)
 å¹¶é…ç½®ä»¥ä¸‹å‚æ•°ï¼š


* `r`
 ï¼Œä½ç§©çŸ©é˜µçš„ç»´æ•°
* `lora_alpha`
 , æƒé‡çŸ©é˜µçš„æ¯”ä¾‹å› å­
* `ç›®æ ‡æ¨¡å—`
 ï¼Œåº”ç”¨ LoRA çš„æ³¨æ„åŠ›çŸ©é˜µçš„åç§°ï¼ˆ
 `q_proj`
 å’Œ
 `v_proj`
 ï¼Œæˆ–æœ¬ä¾‹ä¸­çš„æŸ¥è¯¢å’Œå€¼ï¼‰
* `lora_dropout`
 , LoRA å±‚çš„ä¸¢å¤±æ¦‚ç‡
*`åè§`
 ï¼Œ è®¾ç½®
 `æ— `


ğŸ’¡ æƒé‡çŸ©é˜µæŒ‰æ¯”ä¾‹ç¼©æ”¾
 `lora_alpha/r`
 ï¼Œä»¥åŠæ›´é«˜çš„
 `lora_alpha`
 value ä¸º LoRA æ¿€æ´»åˆ†é…æ›´å¤šæƒé‡ã€‚ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®å°†åå·®è®¾ç½®ä¸º
 `æ— `
 é¦–å…ˆï¼Œç„¶å
 `ä»…åŠ³æ‹‰`
 ï¼Œåœ¨å°è¯•ä¹‹å‰
 `å…¨éƒ¨`
 ã€‚



```
from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model

config = LoraConfig(r=32, lora_alpha=64, target_modules=["q\_proj", "v\_proj"], lora_dropout=0.05, bias="none")
```


è®¾ç½®å®Œæˆå
 [LoraConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraConfig)
 ï¼Œå°†å…¶å’ŒåŸºæœ¬æ¨¡å‹åŒ…è£¹èµ·æ¥
 `get_peft_model()`
 å‡½æ•°æ¥åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚æ‰“å°å‡ºå¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œçœ‹çœ‹ LoRA ä¸å®Œå…¨è®­ç»ƒæ¨¡å‹ç›¸æ¯”æ•ˆç‡æé«˜äº†å¤šå°‘ï¼



```
model = get_peft_model(model, config)
model.print_trainable_parameters()
"trainable params: 15728640 || all params: 1559033600 || trainable%: 1.0088711365810203"
```


ç°åœ¨æ‚¨å·²å‡†å¤‡å¥½åœ¨ä¸­å®šä¹‰ä¸€äº›è®­ç»ƒè¶…å‚æ•°
 [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)
 ç±»ï¼Œä¾‹å¦‚æ¨¡å‹çš„ä¿å­˜ä½ç½®ã€æ‰¹é‡å¤§å°ã€å­¦ä¹ ç‡å’Œè®­ç»ƒå‘¨æœŸæ•°ã€‚è¿™
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ä¸åŸºæœ¬æ¨¡å‹æ²¡æœ‰ç›¸åŒçš„ç­¾åï¼Œå› æ­¤æ‚¨éœ€è¦æ˜¾å¼è®¾ç½®
 `remove_unused_columns=False`
 å’Œ
 `label_names=["æ ‡ç­¾"]`
 ã€‚



```
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="your-name/int8-whisper-large-v2-asr",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=1,
    learning_rate=1e-3,
    warmup_steps=50,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    fp16=True,
    per_device_eval_batch_size=8,
    generation_max_length=128,
    logging_steps=25,
    remove_unused_columns=False,
    label_names=["labels"],
)
```


ç¼–å†™è‡ªå®šä¹‰ä¹Ÿæ˜¯ä¸€ä¸ªå¥½ä¸»æ„
 [TrainerCallback](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/callback#transformers.TrainerCallback)
 åœ¨è®­ç»ƒæœŸé—´ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼š



```
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


class SavePeftModelCallback(TrainerCallback):
    def on\_save(
 self,
 args: TrainingArguments,
 state: TrainerState,
 control: TrainerControl,
 \*\*kwargs,
 ):
        checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX\_CHECKPOINT\_DIR}-{state.global\_step}")

        peft_model_path = os.path.join(checkpoint_folder, "adapter\_model")
        kwargs["model"].save_pretrained(peft_model_path)

        pytorch_model_path = os.path.join(checkpoint_folder, "pytorch\_model.bin")
        if os.path.exists(pytorch_model_path):
            os.remove(pytorch_model_path)
        return control
```


é€šè¿‡
 `Seq2SeqTrainingArguments`
 ã€æ¨¡å‹ã€æ•°æ®é›†ã€æ•°æ®æ•´ç†å™¨ã€åˆ†è¯å™¨å’Œå›è°ƒ
 [Seq2SeqTrainer](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Seq2SeqTrainer)
 ã€‚æ‚¨å¯ä»¥é€‰æ‹©è®¾ç½®
 `model.config.use_cache = False`
 æ¶ˆé™¤ä»»ä½•è­¦å‘Šã€‚ä¸€åˆ‡å‡†å¤‡å°±ç»ªåï¼Œè‡´ç”µ
 [ç«è½¦](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Trainer.train)
 å¼€å§‹è®­ç»ƒï¼



```
from transformers import Seq2SeqTrainer, TrainerCallback, Seq2SeqTrainingArguments, TrainerState, TrainerControl

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice["train"],
    eval_dataset=common_voice["test"],
    data_collator=data_collator,
    tokenizer=processor.feature_extractor,
    callbacks=[SavePeftModelCallback],
)
model.config.use_cache = False
trainer.train()
```


## è¯„ä»·



[å•è¯é”™è¯¯ç‡](https://huggingface.co/spaces/evaluate-metric/wer)
 (WER) æ˜¯è¯„ä¼° ASR æ¨¡å‹çš„å¸¸ç”¨æŒ‡æ ‡ã€‚ä» ğŸ¤— è¯„ä¼°åŠ è½½ WER æŒ‡æ ‡ï¼š



```
import evaluate

metric = evaluate.load("wer")
```


ç¼–å†™ä¸€ä¸ªå¾ªç¯æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚é¦–å…ˆå°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç„¶åç¼–å†™å¾ªç¯
 [`torch.cuda.amp.autocast()`](https://pytorch.org/docs/stable/amp.html)
 å› ä¸º
 `int8`
 è®­ç»ƒéœ€è¦è‡ªåŠ¨æ–½æ³•ã€‚ç„¶åï¼Œå°†ä¸€æ‰¹ç¤ºä¾‹ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚è·å–è§£ç åçš„é¢„æµ‹å’Œæ ‡ç­¾ï¼Œå¹¶åœ¨è°ƒç”¨ä¹‹å‰å°†å®ƒä»¬æ‰¹é‡æ·»åŠ åˆ° WER æŒ‡æ ‡ä¸­
 `è®¡ç®—`
 å¾—åˆ°æœ€ç»ˆçš„WERåˆ†æ•°ï¼š



```
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
import gc

eval_dataloader = DataLoader(common_voice["test"], batch_size=8, collate_fn=data_collator)

model.eval()
for step, batch in enumerate(tqdm(eval_dataloader)):
    with torch.cuda.amp.autocast():
        with torch.no_grad():
            generated_tokens = (
                model.generate(
                    input_features=batch["input\_features"].to("cuda"),
                    decoder_input_ids=batch["labels"][:, :4].to("cuda"),
                    max_new_tokens=255,
                )
                .cpu()
                .numpy()
            )
            labels = batch["labels"].cpu().numpy()
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
            metric.add_batch(
                predictions=decoded_preds,
                references=decoded_labels,
            )
    del generated_tokens, labels, batch
    gc.collect()
wer = 100 * metric.compute()
print(f"{wer=}")
```


## åˆ†äº«æ¨¡å‹



ä¸€æ—¦æ‚¨å¯¹ç»“æœæ„Ÿåˆ°æ»¡æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼š
 [push\_to\_hub](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
 æ–¹æ³•ï¼š



```
model.push_to_hub("your-name/int8-whisper-large-v2-asr")
```


## æ¨ç†



ç°åœ¨è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æ¨¡å‹å§ï¼


å®ä¾‹åŒ–æ¨¡å‹é…ç½®
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ï¼Œä»è¿™é‡Œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨é…ç½®æ¥åŠ è½½åŸºç¡€å’Œ
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€åˆ†è¯å™¨ã€å¤„ç†å™¨å’Œç‰¹å¾æå–å™¨ã€‚è¯·è®°ä½å®šä¹‰
 `è¯­è¨€`
 å’Œ
 `ä»»åŠ¡`
 åœ¨åˆ†è¯å™¨ã€å¤„ç†å™¨å’Œ
 `forced_decoder_ids`
 ï¼š



```
from peft import PeftModel, PeftConfig

peft_model_id = "smangrul/openai-whisper-large-v2-LORA-colab"
language = "Marathi"
task = "transcribe"
peft_config = PeftConfig.from_pretrained(peft_model_id)
model = WhisperForConditionalGeneration.from_pretrained(
    peft_config.base_model_name_or_path, load_in_8bit=True, device_map="auto"
)
model = PeftModel.from_pretrained(model, peft_model_id)
tokenizer = WhisperTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)
processor = WhisperProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)
feature_extractor = processor.feature_extractor
forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)
```


åŠ è½½éŸ³é¢‘æ ·æœ¬ï¼ˆæ‚¨å¯ä»¥åœ¨
 [æ•°æ®é›†é¢„è§ˆ](https://huggingface.co/datasets/stevhliu/dummy)
 ) è¿›è¡Œè½¬å½•ï¼Œå¹¶ä¸”
 [AutomaticSpeechRecognitionPipeline](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)
 ï¼š



```
from transformers import AutomaticSpeechRecognitionPipeline

audio = "https://huggingface.co/datasets/stevhliu/dummy/resolve/main/mrt\_01523\_00028548203.wav"
pipeline = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)
```


ç„¶åä½¿ç”¨å¸¦æœ‰ autocast çš„ç®¡é“ä½œä¸ºéŸ³é¢‘æ ·æœ¬çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼š



```
with torch.cuda.amp.autocast():
    text = pipe(audio, generate_kwargs={"forced\_decoder\_ids": forced_decoder_ids}, max_new_tokens=255)["text"]
text
"à¤®à¥€ à¤¤à¥à¤®à¤šà¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤•à¤¾à¤¹à¥€ à¤•à¤°à¥‚ à¤¶à¤•à¤¤à¥‹ à¤•à¤¾?"
```