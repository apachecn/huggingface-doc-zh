# LoRA ç”¨äºè¯­ä¹‰ç›¸ä¼¼æ€§ä»»åŠ¡

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/task_guides/semantic-similarity-lora>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/task_guides/semantic-similarity-lora>


ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§é‡å‚æ•°åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å…·æœ‰ä½ç§©è¡¨ç¤ºçš„å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚æƒé‡çŸ©é˜µè¢«åˆ†è§£ä¸ºç»è¿‡è®­ç»ƒå’Œæ›´æ–°çš„ä½ç§©çŸ©é˜µã€‚æ‰€æœ‰é¢„è®­ç»ƒçš„æ¨¡å‹å‚æ•°ä¿æŒå†»ç»“ã€‚è®­ç»ƒåï¼Œä½ç§©çŸ©é˜µè¢«æ·»åŠ å›åŸå§‹æƒé‡ã€‚è¿™ä½¿å¾—å­˜å‚¨å’Œè®­ç»ƒ LoRA æ¨¡å‹æ›´åŠ é«˜æ•ˆï¼Œå› ä¸ºå‚æ•°æ˜æ˜¾å‡å°‘ã€‚


ğŸ’¡é˜…è¯»
 [LoRAï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„ä½ç§©é€‚åº”](https://arxiv.org/abs/2106.09685)
 äº†è§£æœ‰å…³ LoRA çš„æ›´å¤šä¿¡æ¯ã€‚


åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ LoRA
 [è„šæœ¬](https://github.com/huggingface/peft/tree/main/examples/lora_dreambooth)
 å¾®è°ƒä¸€ä¸ª
 [`intfloat/e5-large-v2`](https://huggingface.co/intfloat/e5-large-v2)
 æ¨¡å‹ä¸Šçš„
 [`smangrul/amazon_esci`](https://huggingface.co/datasets/smangrul/amazon_esci)
 è¯­ä¹‰ç›¸ä¼¼æ€§ä»»åŠ¡çš„æ•°æ®é›†ã€‚è¯·éšæ„æ¢ç´¢è¯¥è„šæœ¬ï¼Œä»¥æ›´è¯¦ç»†åœ°äº†è§£å…¶å·¥ä½œåŸç†ï¼


## è®¾ç½®



é¦–å…ˆå®‰è£… ğŸ¤— PEFT
 [æ¥æº](https://github.com/huggingface/peft)
 ï¼Œç„¶åå¯¼èˆªåˆ°åŒ…å«ç”¨äºä½¿ç”¨ LoRA å¾®è°ƒ DreamBooth çš„è®­ç»ƒè„šæœ¬çš„ç›®å½•ï¼š



```
cd peft/examples/feature_extraction
```


ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€æœ‰å¿…éœ€çš„åº“ï¼š



```
pip install -r requirements.txt
```


æ¥ä¸‹æ¥ï¼Œå¯¼å…¥æ‰€æœ‰å¿…éœ€çš„åº“ï¼š


* ğŸ¤— ç”¨äºåŠ è½½çš„å˜å‹å™¨
 `intfloat/e5-large-v2`
 æ¨¡å‹å’Œåˆ†è¯å™¨
* ğŸ¤— åŠ é€Ÿè®­ç»ƒå¾ªç¯
* ğŸ¤— ç”¨äºåŠ è½½å’Œå‡†å¤‡çš„æ•°æ®é›†
 `smangrul/amazon_esci`
 ç”¨äºè®­ç»ƒå’Œæ¨ç†çš„æ•°æ®é›†
* ğŸ¤— Evaluate ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½
* ğŸ¤— PEFT ç”¨äºè®¾ç½® LoRA é…ç½®å¹¶åˆ›å»º PEFT æ¨¡å‹
* ğŸ¤— Huggingface\_hub ç”¨äºå°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¸Šä¼ åˆ° HF hub
* hnswlib ç”¨äºåˆ›å»ºæœç´¢ç´¢å¼•å¹¶è¿›è¡Œå¿«é€Ÿè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢


å‡è®¾å·²ç»å®‰è£…äº†æ”¯æŒ CUDA çš„ PyTorchã€‚


## ç«è½¦



å¯åŠ¨è®­ç»ƒè„šæœ¬
 â€˜åŠ é€Ÿå‘å°„â€™
 å¹¶ä¼ é€’ä½ çš„è¶…å‚æ•°
 `--use_peft`
 å¯ç”¨ LoRA çš„å‚æ•°ã€‚


æœ¬æŒ‡å—ä½¿ç”¨ä»¥ä¸‹å†…å®¹
 [LoraConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraConfig)
 ï¼š



```
peft_config = LoraConfig(
            r=8,
            lora_alpha=16,
            bias="none",
            task_type=TaskType.FEATURE_EXTRACTION,
            target_modules=["key", "query", "value"],
        )
```


ä»¥ä¸‹æ˜¯åœ¨å…·æœ‰æ ‡å‡† RAM çš„ V100 GPU ä¸Šçš„ Colab ä¸­è¿è¡Œæ—¶ï¼Œå®Œæ•´çš„è„šæœ¬å‚æ•°é›†å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š



```
accelerate launch --mixed_precision="fp16" peft_lora_embedding_semantic_search.py --dataset_name="smangrul/amazon\_esci" --max_length=70 --model_name_or_path="intfloat/e5-large-v2" --per_device_train_batch_size=64 --per_device_eval_batch_size=128 --learning_rate=5e-4 --weight_decay=0.0 --num_train_epochs 3 --gradient_accumulation_steps=1 --output_dir="results/peft\_lora\_e5\_ecommerce\_semantic\_search\_colab" --seed=42 --push_to_hub --hub_model_id="smangrul/peft\_lora\_e5\_ecommerce\_semantic\_search\_colab" --with_tracking --report_to="wandb" --use_peft --checkpointing_steps "epoch"
```


## è¯­ä¹‰ç›¸ä¼¼åº¦æ•°æ®é›†



æˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®é›†æ˜¯æ•°æ®é›†çš„ä¸€å°éƒ¨åˆ†
 [esci-data](https://github.com/amazon-science/esci-data.git)
 æ•°æ®é›†ï¼ˆå¯ä»¥åœ¨ Hub ä¸Šæ‰¾åˆ°
 [smangrul/amazon\_esci](https://huggingface.co/datasets/smangrul/amazon_esci)
 ï¼‰ã€‚
æ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªå…ƒç»„
 `ï¼ˆæŸ¥è¯¢ã€äº§å“æ ‡é¢˜ã€ç›¸å…³æ€§æ ‡ç­¾ï¼‰`
 åœ¨å“ªé‡Œ
 `ç›¸å…³æ€§æ ‡ç­¾`
 æ˜¯
 `1`
 å¦‚æœäº§å“ç¬¦åˆæ„å›¾
 `æŸ¥è¯¢`
 ï¼Œå¦åˆ™å°±æ˜¯
 `0`
 ã€‚


æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æ„å»ºä¸€ä¸ªåµŒå…¥æ¨¡å‹ï¼Œå¯ä»¥åœ¨ç»™å®šäº§å“æŸ¥è¯¢çš„æƒ…å†µä¸‹æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„äº§å“ã€‚
è¿™é€šå¸¸æ˜¯æ„å»ºäº§å“æœç´¢å¼•æ“ä»¥æ£€ç´¢ç»™å®šæŸ¥è¯¢çš„æ‰€æœ‰æ½œåœ¨ç›¸å…³äº§å“çš„ç¬¬ä¸€é˜¶æ®µã€‚
é€šå¸¸ï¼Œè¿™æ¶‰åŠä½¿ç”¨åŒç¼–ç å™¨æ¨¡å‹æ¥äº¤å‰è¿æ¥æŸ¥è¯¢å’Œæ•°ç™¾ä¸‡ä¸ªå¯èƒ½ä¼šè¿…é€Ÿçˆ†ç‚¸çš„äº§å“ã€‚
ç›¸åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Transformer æ¨¡å‹æ¥æ£€ç´¢ç»™å®šæŸ¥è¯¢çš„å‰ K ä¸ªæœ€æ¥è¿‘çš„ç›¸ä¼¼äº§å“ï¼š
å°†æŸ¥è¯¢å’Œäº§å“åµŒå…¥åˆ°åŒä¸€æ½œåœ¨åµŒå…¥ç©ºé—´ä¸­ã€‚
æ•°ä»¥ç™¾ä¸‡è®¡çš„äº§å“è¢«ç¦»çº¿åµŒå…¥ä»¥åˆ›å»ºæœç´¢ç´¢å¼•ã€‚
åœ¨è¿è¡Œæ—¶ï¼Œæ¨¡å‹ä»…åµŒå…¥æŸ¥è¯¢ï¼Œå¹¶ä½¿ç”¨æœç´¢ç´¢å¼•ä»æœç´¢ç´¢å¼•ä¸­æ£€ç´¢äº§å“
   

 å¿«é€Ÿè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢åº“ï¼Œä¾‹å¦‚
 [FAISS](https://github.com/facebookresearch/faiss)
 æˆ–è€…
 [HNSWlib](https://github.com/nmslib/hnswlib)
 ã€‚


ä¸‹ä¸€é˜¶æ®µæ¶‰åŠå¯¹æ£€ç´¢åˆ°çš„äº§å“åˆ—è¡¨é‡æ–°æ’åºä»¥è¿”å›æœ€ç›¸å…³çš„äº§å“ï¼›
æ­¤é˜¶æ®µå¯ä»¥åˆ©ç”¨åŸºäºäº¤å‰ç¼–ç å™¨çš„æ¨¡å‹ä½œä¸ºæŸ¥è¯¢å’Œæœ‰é™çš„ä¸€ç»„æ£€ç´¢äº§å“ä¹‹é—´çš„äº¤å‰è¿æ¥ã€‚
ä¸‹å›¾æ¥è‡ª
 [çœŸæ£’è¯­ä¹‰æœç´¢](https://github.com/rom1504/awesome-semantic-search)
 æ¦‚è¿°äº†ä¸€ä¸ªç²—ç•¥çš„è¯­ä¹‰æœç´¢ç®¡é“ï¼š


![è¯­ä¹‰æœç´¢ç®¡é“](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/semantic_search_pipeline.png)


 å¯¹äºæœ¬ä»»åŠ¡æŒ‡å—ï¼Œæˆ‘ä»¬å°†æ¢ç´¢è®­ç»ƒåµŒå…¥æ¨¡å‹ä»¥é¢„æµ‹è¯­ä¹‰ç›¸ä¼¼äº§å“çš„ç¬¬ä¸€é˜¶æ®µ
ç»™å‡ºäº§å“æŸ¥è¯¢ã€‚


## è®­ç»ƒè„šæœ¬æ·±å…¥æ¢è®¨



æˆ‘ä»¬å¾®è°ƒ
 [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2)
 å…¶ä¸­æœ€é‡è¦çš„æ˜¯
 [MTEB åŸºå‡†æµ‹è¯•](https://huggingface.co/spaces/mteb/leaderboard)
 ä½¿ç”¨ PEFT-LoRAã€‚


`å¥å­åµŒå…¥çš„è‡ªåŠ¨æ¨¡å‹`
 è¿”å›æŸ¥è¯¢å’Œäº§å“åµŒå…¥ï¼Œä»¥åŠ
 `å‡å€¼æ± `
 å‡½æ•°åœ¨åºåˆ—ç»´åº¦ä¸Šå¯¹å®ƒä»¬è¿›è¡Œæ± åŒ–å¹¶å¯¹å®ƒä»¬è¿›è¡Œæ ‡å‡†åŒ–ï¼š



```
class AutoModelForSentenceEmbedding(nn.Module):
    def \_\_init\_\_(self, model\_name, tokenizer, normalize=True):
        super(AutoModelForSentenceEmbedding, self).__init__()

        self.model = AutoModel.from_pretrained(model_name)  
        self.normalize = normalize
        self.tokenizer = tokenizer

    def forward(self, \*\*kwargs):
        model_output = self.model(**kwargs)
        embeddings = self.mean_pooling(model_output, kwargs["attention\_mask"])
        if self.normalize:
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

        return embeddings

    def mean\_pooling(self, model\_output, attention\_mask):
        token_embeddings = model_output[0]  # First element of model\_output contains all token embeddings
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def \_\_getattr\_\_(self, name: str):
        """Forward missing attributes to the wrapped module."""
        try:
            return super().__getattr__(name)  # defer to nn.Module's logic
        except AttributeError:
            return getattr(self.model, name)


def get\_cosine\_embeddings(query\_embs, Product\_embs):
    è¿”å› torch.sum(query_embs * Product_embs, axis=1)


def get\_loss(cosine\_score, labels):
    return torch.mean(torch.square(labels * (1 - cosine_score) + torch.clamp((1 - labels) * cosine_score, min=0.0)))
```


è¿™
 `get_cosine_embeddings`
 å‡½æ•°è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å’Œ
 `è·å–æŸå¤±`
 å‡½æ•°è®¡ç®—æŸå¤±ã€‚è¯¥æŸå¤±ä½¿æ¨¡å‹èƒ½å¤Ÿäº†è§£åˆ°ä½™å¼¦åˆ†æ•°ä¸º
 `1`
 æŸ¥è¯¢å’Œäº§å“å¯¹æ˜¯ç›¸å…³çš„ï¼Œä½™å¼¦åˆ†æ•°ä¸º
 `0`
 æˆ–ä»¥ä¸‹æ— å…³ç´§è¦ã€‚


å®šä¹‰
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ä½¿ç”¨æ‚¨çš„ LoRA è¶…å‚æ•°ï¼Œå¹¶åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚æˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Accelerate æ¥å¤„ç†æ‰€æœ‰è®¾å¤‡ç®¡ç†ã€æ··åˆç²¾åº¦è®­ç»ƒã€æ¢¯åº¦ç´¯ç§¯ã€WandB è·Ÿè¸ªä»¥åŠä¿å­˜/åŠ è½½å®ç”¨ç¨‹åºã€‚


## ç»“æœ



ä¸‹è¡¨æ¯”è¾ƒäº†è®­ç»ƒæ—¶é—´ã€Colab ä¸­é€‚åˆçš„æ‰¹é‡å¤§å°ä»¥åŠ PEFT æ¨¡å‹å’Œå®Œå…¨å¾®è°ƒæ¨¡å‹ä¹‹é—´çš„æœ€ä½³ ROC-AUC åˆ†æ•°ï¼š


| 	 Training Type	  | 	 Training time per epoch (Hrs)	  | 	 Batch Size that fits	  | 	 ROC-AUC score (higher is better)	  |
| --- | --- | --- | --- |
| 	 Pre-Trained e5-large-v2	  | 	 -	  | 	 -	  | 	 0.68	  |
| 	 PEFT	  | 	 1.73	  | 	 64	  | 	 0.787	  |
| 	 Full Fine-Tuning	  | 	 2.33	  | 	 32	  | 	 0.7969	  |


PEFT-LoRA æ¨¡å‹ç«è½¦
 **1.35X**
 æ›´å¿«å¹¶ä¸”æ›´é€‚åˆ
 **2X**
 ä¸å®Œå…¨å¾®è°ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ‰¹é‡å¤§å°æœ‰æ‰€ä¸‹é™ï¼Œå¹¶ä¸” PEFT-LoRA çš„æ€§èƒ½ä¸å®Œå…¨å¾®è°ƒçš„æ¨¡å‹ç›¸å½“ï¼Œç›¸å¯¹ä¸‹é™äº†
 **-1.24%**
 åœ¨ ROC-AUC ä¸­ã€‚æ­£å¦‚ä¸­æåˆ°çš„ï¼Œè¿™ä¸ªå·®è·å¯èƒ½å¯ä»¥ç”¨æ›´å¤§çš„æ¨¡å‹æ¥å¼¥è¡¥
 [å‚æ•°é«˜æ•ˆæç¤ºè°ƒæ•´çš„è§„æ¨¡åŠ›é‡](https://huggingface.co/papers/2104.08691)
 ã€‚


## æ¨ç†



æˆ‘ä»¬èµ°å§ï¼ç°åœ¨æˆ‘ä»¬æœ‰äº†æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºç›®å½•ä¸­çš„æ‰€æœ‰äº§å“åˆ›å»ºæœç´¢ç´¢å¼•ã€‚
è¯·å‚é˜…
 `peft_lora_embedding_semantic_similarity_inference.ipynb`
 è·å–å®Œæ•´çš„æ¨ç†ä»£ç ã€‚


1.è·å–æˆ‘ä»¬å¯ä»¥è°ƒç”¨çš„äº§å“çš„idåˆ—è¡¨
 `ids_to_products_dict`
 ï¼š



```
{0: 'RamPro 10" All Purpose Utility Air Tires/Wheels with a 5/8" Diameter Hole with Double Sealed Bearings (Pack of 2)',
 1: 'MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tractor Tire with Yellow Rim, (3" Centered Hub, 3/4" Bushings )',
 2: 'NEIKO 20601A 14.5 inch Steel Tire Spoon Lever Iron Tool Kit | Professional Tire Changing Tool for Motorcycle, Dirt Bike, Lawn Mower | 3 pcs Tire Spoons | 3 Rim Protector | Valve Tool | 6 Valve Cores',
 3: '2PK 13x5.00-6 13x5.00x6 13x5x6 13x5-6 2PLY Turf Mower Tractor Tire with Gray Rim',
 4: '(Set of 2) 15x6.00-6 Husqvarna/Poulan Tire Wheel Assy .75" Bearing',
 5: 'MaxAuto 2 Pcs 16x6.50-8 Lawn Mower Tire for Garden Tractors Ridings, 4PR, Tubeless',
 6: 'Dr.Roc Tire Spoon Lever Dirt Bike Lawn Mower Motorcycle Tire Changing Tools with Durable Bag 3 Tire Irons 2 Rim Protectors 1 Valve Stems Set TR412 TR413',
 7: 'MARASTAR 21446-2PK 15x6.00-6" Front Tire Assembly Replacement-Craftsman Mower, Pack of 2',
 8: '15x6.00-6" Front Tire Assembly Replacement for 100 and 300 Series John Deere Riding Mowers - 2 pack',
 9: 'Honda HRR Wheel Kit (2 Front 44710-VL0-L02ZB, 2 Back 42710-VE2-M02ZE)',
 10: 'Honda 42710-VE2-M02ZE (Replaces 42710-VE2-M01ZE) Lawn Mower Rear Wheel Set of 2' ...
```


2.ä½¿ç”¨ç»è¿‡åŸ¹è®­çš„
 [smangrul/peft\_lora\_e5\_ecommerce\_semantic\_search\_colab](https://huggingface.co/smangrul/peft_lora_e5_ecommerce_semantic_search_colab)
 è·å¾—äº§å“åµŒå…¥çš„æ¨¡å‹ï¼š



```
# base model
model = AutoModelForSentenceEmbedding(model_name_or_path, tokenizer)

# peft config and wrapping
model = PeftModel.from_pretrained(model, peft_model_id)

device = "cuda"
model.to(device)
model.eval()
model = model.merge_and_unload()

import numpy as np
num_products= len(dataset)
d = 1024

product_embeddings_array = np.zeros((num_products, d))
for step, batch in enumerate(tqdm(dataloader)):
    with torch.no_grad():
        with torch.amp.autocast(dtype=torch.bfloat16, device_type="cuda"):
            product_embs = model(**{k:v.to(device) for k, v in batch.items()}).detach().float().cpu()
    start_index = step*batch_size
    end_index = start_index+batch_size if  (start_index+batch_size) < num_products else num_products
    product_embeddings_array[start_index:end_index] = product_embs
    del product_embs, batch
```


3. ä½¿ç”¨ HNSWlib åˆ›å»ºæœç´¢ç´¢å¼•ï¼š



```
def construct\_search\_index(dim, num\_elements, data):
    # Declaring index
    search_index = hnswlib.Index(space = 'ip', dim = dim) # possible options are l2, cosine or ip
    
    # Initializing index - the maximum number of elements should be known beforehand
    search_index.init_index(max_elements = num_elements, ef_construction = 200, M = 100)

    # Element insertion (can be called several times):
    ids = np.arange(num_elements)
    search_index.add_items(data, ids)

    return search_index

product_search_index = construct_search_index(d, num_products, product_embeddings_array)
```


4. è·å–æŸ¥è¯¢åµŒå…¥å’Œæœ€è¿‘é‚»ï¼š



```
def get\_query\_embeddings(query, model, tokenizer, device):
    inputs = tokenizer(query, padding="max\_length", max_length=70, truncation=True, return_tensors="pt")
    model.eval()
    with torch.no_grad():
        query_embs = model(**{k:v.to(device) for k, v in inputs.items()}).detach().cpu()
    return query_embs[0]

    
def get\_nearest\_neighbours(k, search\_index, query\_embeddings, ids\_to\_products\_dict, threshold=0.7):
    # Controlling the recall by setting ef:
    search_index.set_ef(100) # ef should always be > k

    # Query dataset, k - number of the closest elements (returns 2 numpy arrays)
    labels, distances = search_index.knn_query(query_embeddings, k = k)
    
    return [(ids_to_products_dict[label], (1-distance)) for label, distance in zip(labels[0], distances[0]) if (1-distance)>=threshold]
```


5. è®©æˆ‘ä»¬ç”¨æŸ¥è¯¢æ¥æµ‹è¯•ä¸€ä¸‹
 ã€Šæ·±åº¦å­¦ä¹ ä¹¦ç±ã€‹
 ï¼š



```
query = "deep learning books"
k = 10
query_embeddings = get_query_embeddings(query, model, tokenizer, device)
search_results = get_nearest_neighbours(k, product_search_index, query_embeddings, ids_to_products_dict, threshold=0.7)

print(f"{query=}") 
for product, cosine_sim_score in search_results:
    print(f"cosine\_sim\_score={round(cosine\_sim\_score,2)} {product=}")
```


è¾“å‡ºï¼š



```
query='deep learning books'
cosine_sim_score=0.95 product='Deep Learning (The MIT Press Essential Knowledge series)'
cosine_sim_score=0.93 product='Practical Deep Learning: A Python-Based Introduction'
cosine_sim_score=0.9 product='Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems'
cosine_sim_score=0.9 product='Machine Learning: A Hands-On, Project-Based Introduction to Machine Learning for Absolute Beginners: Mastering Engineering ML Systems using Scikit-Learn and TensorFlow'
cosine_sim_score=0.9 product='Mastering Machine Learning on AWS: Advanced machine learning in Python using SageMaker, Apache Spark, and TensorFlow'
cosine_sim_score=0.9 product='The Hundred-Page Machine Learning Book'
cosine_sim_score=0.89 product='Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems'
cosine_sim_score=0.89 product='Machine Learning: A Journey from Beginner to Advanced Including Deep Learning, Scikit-learn and Tensorflow'
cosine_sim_score=0.88 product='Mastering Machine Learning with scikit-learn'
cosine_sim_score=0.88 product='Mastering Machine Learning with scikit-learn - Second Edition: Apply effective learning algorithms to real-world problems using scikit-learn'
```


å°½ç®¡æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„ä¹¦ç±è¢«æ£€ç´¢
 `æœºå™¨å­¦ä¹ `
 æœªåŒ…å«åœ¨æŸ¥è¯¢ä¸­ã€‚è¿™æ„å‘³ç€è¯¥æ¨¡å‹å·²ç»äº†è§£åˆ°è¿™äº›ä¹¦ç±åœ¨è¯­ä¹‰ä¸Šä¸åŸºäºäºšé©¬é€Šä¸Šå®¢æˆ·çš„è´­ä¹°è¡Œä¸ºçš„æŸ¥è¯¢ç›¸å…³ã€‚


ç†æƒ³æƒ…å†µä¸‹ï¼Œæ¥ä¸‹æ¥çš„æ­¥éª¤å°†æ¶‰åŠä½¿ç”¨ ONNX/TensorRT æ¥ä¼˜åŒ–æ¨¡å‹å¹¶ä½¿ç”¨ Triton æœåŠ¡å™¨æ¥æ‰˜ç®¡å®ƒã€‚çœ‹çœ‹ğŸ¤—
 [æœ€ä½³](https://huggingface.co/docs/optimum/index)
 è¿›è¡Œç›¸å…³ä¼˜åŒ–ä»¥å®ç°é«˜æ•ˆæœåŠ¡ï¼