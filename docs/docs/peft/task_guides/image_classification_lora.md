# ä½¿ç”¨ LoRA è¿›è¡Œå›¾åƒåˆ†ç±»

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/task_guides/image_classification_lora>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/task_guides/image_classification_lora>


æœ¬æŒ‡å—æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LoRAï¼ˆä¸€ç§ä½ç§©è¿‘ä¼¼æŠ€æœ¯ï¼‰æ¥å¾®è°ƒå›¾åƒåˆ†ç±»æ¨¡å‹ã€‚
é€šè¿‡ä½¿ç”¨ğŸ¤— PEFT çš„ LoRAï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å‡å°‘åˆ°åªæœ‰åŸå§‹å‚æ•°çš„ 0.77%ã€‚


LoRA é€šè¿‡å‘æ¨¡å‹çš„ç‰¹å®šå—ï¼ˆä¾‹å¦‚æ³¨æ„åŠ›ï¼‰æ·»åŠ ä½ç§©â€œæ›´æ–°çŸ©é˜µâ€æ¥å®ç°è¿™ç§å‡å°‘
å—ã€‚å¾®è°ƒæ—¶ï¼Œä»…è®­ç»ƒè¿™äº›çŸ©é˜µï¼Œè€ŒåŸå§‹æ¨¡å‹å‚æ•°ä¿æŒä¸å˜ã€‚
åœ¨æ¨ç†æ—¶ï¼Œæ›´æ–°çŸ©é˜µä¸åŸå§‹æ¨¡å‹å‚æ•°åˆå¹¶ä»¥äº§ç”Ÿæœ€ç»ˆçš„åˆ†ç±»ç»“æœã€‚


æœ‰å…³ LoRA çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…
 [LoRA åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2106.09685)
 ã€‚


## å®‰è£…ä¾èµ–é¡¹



å®‰è£…æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„åº“ï¼š



```
!pip install transformers accelerate evaluate datasets peft -q
```


æ£€æŸ¥æ‰€æœ‰å¿…éœ€åº“çš„ç‰ˆæœ¬ä»¥ç¡®ä¿æ‚¨æ˜¯æœ€æ–°çš„ï¼š



```
import transformers
import accelerate
import peft

print(f"Transformers version: {transformers.\_\_version\_\_}")
print(f"Accelerate version: {accelerate.\_\_version\_\_}")
print(f"PEFT version: {peft.\_\_version\_\_}")
"Transformers version: 4.27.4"
"Accelerate version: 0.18.0"
"PEFT version: 0.2.0"
```


## éªŒè¯ä»¥å…±äº«æ‚¨çš„æ¨¡å‹



è¦åœ¨åŸ¹è®­ç»“æŸæ—¶ä¸ç¤¾åŒºåˆ†äº«å¾®è°ƒåçš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨æ‚¨çš„ ğŸ¤— ä»¤ç‰Œè¿›è¡Œèº«ä»½éªŒè¯ã€‚
æ‚¨å¯ä»¥ä»æ‚¨çš„
 [è´¦æˆ·è®¾ç½®](https://huggingface.co/settings/token)
 ã€‚



```
from huggingface_hub import notebook_login

notebook_login()
```


## é€‰æ‹©æ¨¡å‹æ£€æŸ¥ç‚¹è¿›è¡Œå¾®è°ƒ



ä»æ”¯æŒçš„ä»»ä½•æ¨¡å‹æ¶æ„ä¸­é€‰æ‹©ä¸€ä¸ªæ¨¡å‹æ£€æŸ¥ç‚¹
 [å›¾åƒåˆ†ç±»](https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads)
 ã€‚å¦‚æœ‰ç–‘é—®ï¼Œè¯·å‚é˜…
è¿™
 [å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](https://huggingface.co/docs/transformers/v4.27.2/en/tasks/image_classification)
 åœ¨
ğŸ¤— å˜å½¢é‡‘åˆšæ–‡æ¡£ã€‚



```
model_checkpoint = "google/vit-base-patch16-224-in21k"
```


## åŠ è½½æ•°æ®é›†



ä¸ºäº†ç¼©çŸ­æ­¤ç¤ºä¾‹çš„è¿è¡Œæ—¶é—´ï¼Œæˆ‘ä»¬åªåŠ è½½è®­ç»ƒé›†ä¸­çš„å‰ 5000 ä¸ªå®ä¾‹
 [Food-101 æ•°æ®é›†](https://huggingface.co/datasets/food101)
 :



```
from datasets import load_dataset

dataset = load_dataset("food101", split="train[:5000]")
```


## æ•°æ®é›†å‡†å¤‡



è¦å‡†å¤‡ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„æ•°æ®é›†ï¼Œè¯·åˆ›å»º
 `æ ‡ç­¾2id`
 å’Œ
 `id2æ ‡ç­¾`
 å­—å…¸ã€‚è¿™äº›éƒ½ä¼šè¿›æ¥
åœ¨æ‰§è¡Œæ¨ç†å’Œå…ƒæ•°æ®ä¿¡æ¯æ—¶å¾ˆæ–¹ä¾¿ï¼š



```
labels = dataset.features["label"].names
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = i
    id2label[i] = label

id2label[2]
"baklava"
```


æ¥ä¸‹æ¥ï¼ŒåŠ è½½æ‚¨æ­£åœ¨å¾®è°ƒçš„æ¨¡å‹çš„å›¾åƒå¤„ç†å™¨ï¼š



```
from transformers import AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)
```


è¿™
 `å›¾åƒå¤„ç†å™¨`
 åŒ…å«æœ‰å…³åº”è°ƒæ•´è®­ç»ƒå’Œè¯„ä¼°å›¾åƒå¤§å°çš„æœ‰ç”¨ä¿¡æ¯
ä»¥åŠç”¨äºæ ‡å‡†åŒ–åƒç´ å€¼çš„å€¼ã€‚ä½¿ç”¨
 `å›¾åƒå¤„ç†å™¨`
 , å‡†å¤‡è½¬æ¢
æ•°æ®é›†çš„å‡½æ•°ã€‚è¿™äº›åŠŸèƒ½å°†åŒ…æ‹¬æ•°æ®å¢å¼ºå’Œåƒç´ ç¼©æ”¾ï¼š



```
from torchvision.transforms import (
    CenterCrop,
    Compose,
    Normalize,
    RandomHorizontalFlip,
    RandomResizedCrop,
    Resize,
    ToTensor,
)

normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
train_transforms = Compose(
    [
        RandomResizedCrop(image_processor.size["height"]),
        RandomHorizontalFlip(),
        ToTensor(),
        normalize,
    ]
)

val_transforms = Compose(
    [
        Resize(image_processor.size["height"]),
        CenterCrop(image_processor.size["height"]),
        ToTensor(),
        normalize,
    ]
)


def é¢„å¤„ç†\_trainï¼ˆç¤ºä¾‹\_batchï¼‰ï¼š
    """è·¨æ‰¹æ¬¡åº”ç”¨è®­ç»ƒ\_å˜æ¢ã€‚"""
    example_batch["pixel\_values"] = [train_transforms(image.convert("RGB")) for image in example_batch["image"]]
    è¿”å› example_batch


def preprocess\_val(example\_batch):
    """Apply val\_transforms across a batch."""
    example_batch["pixel\_values"] = [val_transforms(image.convert("RGB")) for image in example_batch["image"]]
    return example_batch
```


å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼š



```
splits = dataset.train_test_split(test_size=0.1)
train_ds = splits["train"]
val_ds = splits["test"]
```


æœ€åï¼Œç›¸åº”åœ°è®¾ç½®æ•°æ®é›†çš„è½¬æ¢å‡½æ•°ï¼š



```
train_ds.set_transform(preprocess_train)
val_ds.set_transform(preprocess_val)
```


## åŠ è½½å¹¶å‡†å¤‡æ¨¡å‹



åœ¨åŠ è½½æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥æ£€æŸ¥æ¨¡å‹çš„å‚æ•°æ€»æ•°
å…¶ä¸­æœ‰å¤šå°‘æ˜¯å¯è®­ç»ƒçš„ã€‚



```
def print\_trainable\_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable\_params} || all params: {all\_param} || trainable%: {100 \* trainable\_params / all\_param:.2f}"
    )
```


æ­£ç¡®åˆå§‹åŒ–åŸå§‹æ¨¡å‹éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒå°†ç”¨ä½œåˆ›å»ºæ¨¡å‹çš„åŸºç¡€
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ä½ ä¼š
å®é™…ä¸Šå¾®è°ƒã€‚æŒ‡å®š
 `æ ‡ç­¾2id`
 å’Œ
 `id2æ ‡ç­¾`
 ä»¥ä¾¿
 [AutoModelForImageClassification](https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/auto#transformers.AutoModelForImageClassification)
 å¯ä»¥é™„åŠ ä¸€ä¸ªåˆ†ç±»
å‰å¾€é€‚åˆè¯¥æ•°æ®é›†çš„åº•å±‚æ¨¡å‹ã€‚æ‚¨åº”è¯¥çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºï¼š



```
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']
```



```
from transformers import AutoModelForImageClassification, TrainingArguments, Trainer

model = AutoModelForImageClassification.from_pretrained(
    model_checkpoint,
    label2id=label2id,
    id2label=id2label,
    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
)
```


åœ¨åˆ›å»ºä¹‹å‰
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ï¼Œæ‚¨å¯ä»¥æ£€æŸ¥åŸå§‹æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼š



```
print_trainable_parameters(model)
"trainable params: 85876325 || all params: 85876325 || trainable%: 100.00"
```


æ¥ä¸‹æ¥ï¼Œä½¿ç”¨
 `get_peft_model`
 åŒ…è£…åŸºæœ¬æ¨¡å‹ï¼Œä»¥ä¾¿å°†â€œæ›´æ–°â€çŸ©é˜µæ·»åŠ åˆ°ç›¸åº”çš„ä½ç½®ã€‚



```
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["query", "value"],
    lora_dropout=0.1,
    bias="none",
    modules_to_save=["classifier"],
)
lora_model = get_peft_model(model, config)
print_trainable_parameters(lora_model)
"trainable params: 667493 || all params: 86466149 || trainable%: 0.77"
```


è®©æˆ‘ä»¬æ¥è§£å¼€è¿™é‡Œå‘ç”Ÿçš„äº‹æƒ…ã€‚
è¦ä½¿ç”¨LoRAï¼Œæ‚¨éœ€è¦åœ¨ä¸­æŒ‡å®šç›®æ ‡æ¨¡å—
 `LoraConfig`
 ä»¥ä¾¿
 `get_peft_model()`
 çŸ¥é“å“ªäº›æ¨¡å—
æˆ‘ä»¬çš„æ¨¡å‹å†…éƒ¨éœ€è¦ç”¨ LoRA çŸ©é˜µè¿›è¡Œä¿®æ”¹ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åªå¯¹å®šä½æŸ¥è¯¢æ„Ÿå…´è¶£ï¼Œå¹¶ä¸”
åŸºç¡€æ¨¡å‹çš„æ³¨æ„åŠ›å—çš„å€¼çŸ©é˜µã€‚ç”±äºè¿™äº›çŸ©é˜µå¯¹åº”çš„å‚æ•°æ˜¯â€œå‘½åâ€çš„
åˆ†åˆ«æ˜¯â€œqueryâ€å’Œâ€œvalueâ€ï¼Œæˆ‘ä»¬åœ¨
 `ç›®æ ‡æ¨¡å—`
 çš„è®ºè¯
 `LoraConfig`
 ã€‚


æˆ‘ä»¬è¿˜æŒ‡å®š
 `è¦ä¿å­˜çš„æ¨¡å—`
 ã€‚åŒ…è£…åŸºæœ¬æ¨¡å‹å
 `get_peft_model()`
 éšç€
 `é…ç½®`
 ï¼Œæˆ‘ä»¬å¾—åˆ°
ä¸€ç§æ–°æ¨¡å‹ï¼Œå…¶ä¸­ä»… LoRA å‚æ•°å¯è®­ç»ƒï¼ˆæ‰€è°“çš„â€œæ›´æ–°çŸ©é˜µâ€ï¼‰ï¼Œè€Œé¢„è®­ç»ƒå‚æ•°
è¢«å†·å†»ä¿å­˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨å¾®è°ƒåŸºç¡€æ¨¡å‹æ—¶ä¹Ÿè®­ç»ƒåˆ†ç±»å™¨å‚æ•°
è‡ªå®šä¹‰æ•°æ®é›†ã€‚ä¸ºäº†ç¡®ä¿åˆ†ç±»å™¨å‚æ•°ä¹Ÿç»è¿‡è®­ç»ƒï¼Œæˆ‘ä»¬æŒ‡å®š
 `è¦ä¿å­˜çš„æ¨¡å—`
 ã€‚è¿™ä¹Ÿæ˜¯
ç¡®ä¿åœ¨ä½¿ç”¨ç±»ä¼¼å®ç”¨ç¨‹åºæ—¶ï¼Œè¿™äº›æ¨¡å—ä¸ LoRA å¯è®­ç»ƒå‚æ•°ä¸€èµ·åºåˆ—åŒ–
 `save_pretrained()`
 å’Œ
 `push_to_hub()`
 ã€‚


å…¶ä»–å‚æ•°çš„å«ä¹‰å¦‚ä¸‹ï¼š


* `r`
 ï¼šLoRA æ›´æ–°çŸ©é˜µä½¿ç”¨çš„ç»´åº¦ã€‚
* `é˜¿å°”æ³•`
 ï¼šæ¯”ä¾‹å› å­ã€‚
*`åè§`
 ï¼šæŒ‡å®šæ˜¯å¦
 `åè§`
 åº”è¯¥è®­ç»ƒå‚æ•°ã€‚
 `æ— `
 è¡¨ç¤ºæ²¡æœ‰ä¸€ä¸ª
 `åè§`
 å‚æ•°å°†è¢«è®­ç»ƒã€‚


`r`
 å’Œ
 `é˜¿å°”æ³•`
 ä½¿ç”¨ LoRA æ—¶å…±åŒæ§åˆ¶æœ€ç»ˆå¯è®­ç»ƒå‚æ•°çš„æ€»æ•°ï¼Œä¸ºæ‚¨æä¾›çµæ´»æ€§
å¹³è¡¡æœ€ç»ˆæ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚


é€šè¿‡æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œæ‚¨å¯ä»¥äº†è§£æˆ‘ä»¬å®é™…æ­£åœ¨è®­ç»ƒçš„å‚æ•°æ•°é‡ã€‚æ—¢ç„¶ç›®æ ‡æ˜¯
ä¸ºäº†å®ç°å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼Œæ‚¨åº”è¯¥æœŸæœ›åœ¨
 `åŠ³æ‹‰æ¨¡å‹`
 ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œè¿™é‡Œç¡®å®æ˜¯è¿™æ ·ã€‚


## å®šä¹‰è®­ç»ƒå‚æ•°



å¯¹äºæ¨¡å‹å¾®è°ƒï¼Œä½¿ç”¨
 [Trainer](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Trainer)
 ã€‚å®ƒæ¥å—
æ‚¨å¯ä»¥ä½¿ç”¨å‡ ä¸ªå‚æ•°æ¥åŒ…è£…
 [TrainingArguments](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.TrainingArguments)
 ã€‚



```
from transformers import TrainingArguments, Trainer


model_name = model_checkpoint.split("/")[-1]
batch_size = 128

args = TrainingArguments(
    f"{model\_name}-finetuned-lora-food101",
    remove_unused_columns=False,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-3,
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=batch_size,
    fp16=True,
    num_train_epochs=5,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    push_to_hub=True,
    label_names=["labels"],
)
```


ä¸é PEFT æ–¹æ³•ç›¸æ¯”ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œå› ä¸ºéœ€è¦è®­ç»ƒçš„å‚æ•°æ›´å°‘ã€‚
æ‚¨è¿˜å¯ä»¥è®¾ç½®æ¯”æ­£å¸¸æƒ…å†µæ›´å¤§çš„å­¦ä¹ ç‡ï¼ˆä¾‹å¦‚ 1e-5ï¼‰ã€‚


è¿™è¿˜å¯èƒ½å‡å°‘è¿›è¡Œæ˜‚è´µçš„è¶…å‚æ•°è°ƒæ•´å®éªŒçš„éœ€è¦ã€‚


## å‡†å¤‡è¯„ä¼°æŒ‡æ ‡




```
import numpy as np
import evaluate

metric = evaluate.load("accuracy")


def compute\_metrics(eval\_pred):
    """Computes accuracy on a batch of predictions"""
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)
```


è¿™
 `è®¡ç®—æŒ‡æ ‡`
 å‡½æ•°é‡‡ç”¨å‘½åå…ƒç»„ä½œä¸ºè¾“å…¥ï¼š
 `é¢„æµ‹`
 ï¼Œå®ƒä»¬æ˜¯ Numpy æ•°ç»„å½¢å¼çš„æ¨¡å‹çš„ logitsï¼Œ
å’Œ
 `label_ids`
 ï¼Œå®ƒä»¬æ˜¯ Numpy æ•°ç»„çš„çœŸå®æ ‡ç­¾ã€‚


## å®šä¹‰æ’åºè§„åˆ™å‡½æ•°



ä½¿ç”¨æ’åºå‡½æ•°
 [Trainer](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Trainer)
 æ”¶é›†ä¸€æ‰¹åŸ¹è®­å’Œè¯„ä¼°ç¤ºä¾‹å¹¶å‡†å¤‡å¥½
åº•å±‚æ¨¡å‹å¯æ¥å—çš„æ ¼å¼ã€‚



```
import torch


def collate\_fn(examples):
    pixel_values = torch.stack([example["pixel\_values"] for example in examples])
    labels = torch.tensor([example["label"] for example in examples])
    return {"pixel\_values": pixel_values, "labels": labels}
```


## è®­ç»ƒå’Œè¯„ä¼°



å°†æ‰€æœ‰å†…å®¹æ”¾åœ¨ä¸€èµ· - æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€æ•°æ®ã€æ•´ç†å‡½æ•°ç­‰ã€‚ç„¶åï¼Œå¼€å§‹è®­ç»ƒï¼



```
trainer = Trainer(
    lora_model,
    args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=image_processor,
    compute_metrics=compute_metrics,
    data_collator=collate_fn,
)
train_results = trainer.train()
```


åœ¨çŸ­çŸ­å‡ åˆ†é’Ÿå†…ï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹å³ä½¿åœ¨è¿™ä¹ˆå°çš„æ•°æ®ä¸Šä¹Ÿæ˜¾ç¤ºå‡º 96% çš„éªŒè¯å‡†ç¡®ç‡
è®­ç»ƒæ•°æ®é›†çš„å­é›†ã€‚



```
trainer.evaluate(val_ds)
{
    "eval\_loss": 0.14475855231285095,
    "eval\_accuracy": 0.96,
    "eval\_runtime": 3.5725,
    "eval\_samples\_per\_second": 139.958,
    "eval\_steps\_per\_second": 1.12,
    "epoch": 5.0,
}
```


## åˆ†äº«æ‚¨çš„æ¨¡å‹å¹¶è¿è¡Œæ¨ç†



å¾®è°ƒå®Œæˆåï¼Œä¸ç¤¾åŒºå…±äº« LoRA å‚æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š



```
repo_name = f"sayakpaul/{model\_name}-finetuned-lora-food101"
lora_model.push_to_hub(repo_name)
```


æ‰“ç”µè¯æ—¶
 [push\_to\_hub](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
 äº
 `åŠ³æ‹‰æ¨¡å‹`
 ï¼Œä»… LoRA å‚æ•°ä»¥åŠä¸­æŒ‡å®šçš„ä»»ä½•æ¨¡å—
 `è¦ä¿å­˜çš„æ¨¡å—`
 è¢«ä¿å­˜ã€‚çœ‹çœ‹
 [è®­ç»ƒå¥½çš„LoRAå‚æ•°](https://huggingface.co/sayakpaul/vit-base-patch16-224-in21k-finetuned-lora-food101/blob/main/adapter_model.bin)
 ã€‚
æ‚¨ä¼šå‘ç°å®ƒåªæœ‰ 2.6 MBï¼è¿™æå¤§åœ°æœ‰åŠ©äºå¯ç§»æ¤æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨éå¸¸å¤§çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶ï¼ˆä¾‹å¦‚
 [ç»½æ”¾](https://huggingface.co/bigscience/bloom)
 ï¼‰ã€‚


æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åŠ è½½ LoRA æ›´æ–°å‚æ•°ä»¥åŠæˆ‘ä»¬çš„æ¨ç†åŸºç¡€æ¨¡å‹ã€‚å½“æ‚¨åŒ…è£…åŸºç¡€æ¨¡å‹æ—¶
å’Œ
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ï¼Œä¿®æ”¹å®Œæˆ
 *åˆ°ä½*
 ã€‚ä¸ºäº†å‡è½»å°±åœ°ä¿®æ”¹å¯èƒ½å¼•èµ·çš„ä»»ä½•æ‹…å¿§ï¼Œ
åƒä¹‹å‰ä¸€æ ·åˆå§‹åŒ–åŸºæœ¬æ¨¡å‹å¹¶æ„å»ºæ¨ç†æ¨¡å‹ã€‚



```
from peft import PeftConfig, PeftModel


config = PeftConfig.from_pretrained(repo_name)
model = AutoModelForImageClassification.from_pretrained(
    config.base_model_name_or_path,
    label2id=label2id,
    id2label=id2label,
    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
)
# Load the LoRA model
inference_model = PeftModel.from_pretrained(model, repo_name)
```


ç°åœ¨è®©æˆ‘ä»¬è·å–ä¸€ä¸ªç¤ºä¾‹å›¾åƒè¿›è¡Œæ¨ç†ã€‚



```
from PIL import Image
import requests

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg"
image = Image.open(requests.get(url, stream=True).raw)
image
```


![è´å¥ˆç‰¹é¥¼å›¾åƒ](https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg)


 é¦–å…ˆï¼Œå®ä¾‹åŒ–ä¸€ä¸ª
 `å›¾åƒå¤„ç†å™¨`
 æ¥è‡ªåº•å±‚æ¨¡å‹å­˜å‚¨åº“ã€‚



```
image_processor = AutoImageProcessor.from_pretrained(repo_name)
```


ç„¶åï¼Œå‡†å¤‡ç”¨äºæ¨ç†çš„ç¤ºä¾‹ã€‚



```
encoding = image_processor(image.convert("RGB"), return_tensors="pt")
```


æœ€åï¼Œè¿è¡Œæ¨ç†ï¼



```
with torch.no_grad():
    outputs = inference_model(**encoding)
    logits = outputs.logits

predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", inference_model.config.id2label[predicted_class_idx])
"Predicted class: beignets"
```