# å¿«é€Ÿå¯¼è§ˆ

> è¯‘è€…ï¼š[ç³–é†‹é±¼](https://github.com/now-101)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/quicktour>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/quicktour>


ğŸ¤— PEFTæä¾›äº†ç”¨äºå¾®è°ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°é«˜æ•ˆæ–¹æ³•ã€‚ä¼ ç»ŸèŒƒå¼æ˜¯é’ˆå¯¹æ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œä½†ç”±äºå½“ä»Šæ¨¡å‹ä¸­å‚æ•°æ•°é‡åºå¤§ï¼Œè¿™ç§æ–¹æ³•å˜å¾—å¼‚å¸¸æ˜‚è´µå’Œä¸åˆ‡å®é™…ã€‚ç›¸åï¼Œæ›´é«˜æ•ˆçš„æ–¹æ³•æ˜¯è®­ç»ƒå°‘é‡æç¤ºå‚æ•°æˆ–ä½¿ç”¨é‡æ–°å‚æ•°åŒ–æ–¹æ³•ï¼Œå¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œä»¥å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚

æœ¬å¿«é€Ÿå¯¼è§ˆå°†å±•ç¤ºğŸ¤— PEFTçš„ä¸»è¦åŠŸèƒ½ï¼Œä»¥åŠæ‚¨å¦‚ä½•åœ¨é€šå¸¸æ— æ³•ä½¿ç”¨å¤§æ¨¡å‹çš„æ¶ˆè´¹ç±»è®¾å¤‡ä¸Šè¿›è¡Œè®­ç»ƒæˆ–æ¨ç†ã€‚


## è®­ç»ƒ
æ¯ç§PEFTæ–¹æ³•éƒ½ç”±ä¸€ä¸ªPeftConfigç±»å®šä¹‰ï¼Œè¯¥ç±»å­˜å‚¨æ„å»ºPeftModelæ‰€éœ€çš„æ‰€æœ‰é‡è¦å‚æ•°ã€‚ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨LoRAè¿›è¡Œè®­ç»ƒï¼ŒåŠ è½½å¹¶åˆ›å»ºä¸€ä¸ªLoraConfigç±»ï¼Œå¹¶æŒ‡å®šä»¥ä¸‹å‚æ•°ï¼š


- task_typeï¼šè¦è¿›è¡Œè®­ç»ƒçš„ä»»åŠ¡ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸ºåºåˆ—åˆ°åºåˆ—è¯­è¨€å»ºæ¨¡ï¼‰
- inference_modeï¼šæ˜¯å¦å°†æ¨¡å‹ç”¨äºæ¨æ–­
- rï¼šä½ç§©çŸ©é˜µçš„ç»´åº¦
- lora_alphaï¼šä½ç§©çŸ©é˜µçš„ç¼©æ”¾å› å­
- lora_dropoutï¼šLoRAå±‚çš„ä¸¢å¼ƒæ¦‚ç‡

```python
from peft import LoraConfig, TaskType

peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)
```

> è¯·æŸ¥çœ‹LoraConfigå‚è€ƒæ–‡æ¡£ï¼Œäº†è§£æ‚¨å¯ä»¥è°ƒæ•´çš„å…¶ä»–å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œä¾‹å¦‚è¦è°ƒæ•´çš„æ¨¡å—æˆ–åç½®ç±»å‹ã€‚  

è®¾ç½®å¥½LoraConfigåï¼Œä½¿ç”¨get_peft_model()å‡½æ•°åˆ›å»ºä¸€ä¸ªPeftModelã€‚è¯¥å‡½æ•°æ¥å—ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆæ‚¨å¯ä»¥ä»Transformersåº“ä¸­åŠ è½½ï¼‰ï¼Œä»¥åŠåŒ…å«å¦‚ä½•é…ç½®æ¨¡å‹ä»¥åœ¨LoRAä¸‹è¿›è¡Œè®­ç»ƒçš„å‚æ•°çš„LoraConfigã€‚

åŠ è½½æ‚¨æƒ³è¦å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ã€‚
```python
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/mt0-large")
```  
ä½¿ç”¨get_peft_model()å‡½æ•°å°†åŸºç¡€æ¨¡å‹å’Œpeft_configå°è£…èµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ªPeftModelã€‚ä¸ºäº†äº†è§£æ‚¨æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œå¯ä»¥ä½¿ç”¨print_trainable_parametersæ–¹æ³•ã€‚

```python
from peft import get_peft_model

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

"output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"
```
åœ¨ bigscience/mt0-large çš„ 1.2B å‚æ•°ä¸­ï¼Œæ‚¨åªè®­ç»ƒäº†å…¶ä¸­çš„ 0.19%ï¼

å°±æ˜¯è¿™æ ·ğŸ‰ï¼ ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Transformers Trainerã€Accelerate æˆ–ä»»ä½•è‡ªå®šä¹‰ PyTorch è®­ç»ƒå¾ªç¯æ¥è®­ç»ƒæ¨¡å‹ã€‚

ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨ Trainer ç±»è¿›è¡Œè®­ç»ƒï¼Œè¯·ä½¿ç”¨ä¸€äº›è®­ç»ƒè¶…å‚æ•°è®¾ç½® TrainingArguments ç±»ã€‚

```python
training_args = TrainingArguments(
    output_dir="your-name/bigscience/mt0-large-lora",
    learning_rate=1e-3,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)
```
å°†æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€æ•°æ®é›†ã€æ ‡è®°å™¨å’Œä»»ä½•å…¶ä»–å¿…è¦ç»„ä»¶ä¼ é€’ç»™Trainerï¼Œå¹¶è°ƒç”¨trainå¼€å§‹è®­ç»ƒã€‚
```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```
## ä¿å­˜æ¨¡å‹
åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨save_pretrainedå‡½æ•°å°†æ¨¡å‹ä¿å­˜åˆ°ä¸€ä¸ªç›®å½•ä¸­ã€‚
```python
model.save_pretrained("output_dir")
```
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨push_to_hubå‡½æ•°å°†æ¨¡å‹ä¿å­˜åˆ°Hubï¼ˆè¯·ç¡®ä¿é¦–å…ˆç™»å½•åˆ°æ‚¨çš„Hugging Faceè´¦æˆ·ï¼‰ã€‚
```python
from huggingface_hub import notebook_login

notebook_login()
model.push_to_hub("your-name/bigscience/mt0-large-lora")
```
è¿™ä¸¤ç§æ–¹æ³•åªä¿å­˜ç»è¿‡è®­ç»ƒçš„é¢å¤–PEFTæƒé‡ï¼Œè¿™æ„å‘³ç€å­˜å‚¨ã€ä¼ è¾“å’ŒåŠ è½½éƒ½éå¸¸é«˜æ•ˆã€‚ä¾‹å¦‚ï¼Œè¿™ä¸ªä½¿ç”¨LoRAè®­ç»ƒçš„facebook/opt-350mæ¨¡å‹åªåŒ…å«ä¸¤ä¸ªæ–‡ä»¶ï¼šadapter_config.jsonå’Œadapter_model.safetensorsã€‚adapter_model.safetensorsæ–‡ä»¶åªæœ‰6.3MBï¼

![opt-350m-lora](./attachment/opt-350m-lora.png "å­˜å‚¨åœ¨Hubä¸Šçš„opt-350mæ¨¡å‹çš„é€‚é…å™¨æƒé‡åªæœ‰çº¦6MBï¼Œè€Œå®Œæ•´æ¨¡å‹æƒé‡çš„å¤§å°å¯èƒ½è¾¾åˆ°çº¦700MBã€‚")


## æ¨ç†
> æŸ¥çœ‹AutoPeftModel APIå‚è€ƒï¼Œè·å–å¯ç”¨çš„æ‰€æœ‰AutoPeftModelç±»çš„å®Œæ•´åˆ—è¡¨ã€‚

ä½¿ç”¨AutoPeftModelç±»å’Œfrom_pretrainedæ–¹æ³•å¯ä»¥è½»æ¾åŠ è½½ä»»ä½•ç»è¿‡PEFTè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚

```python
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer
import torch

model = AutoPeftModelForCausalLM.from_pretrained("ybelkada/opt-350m-lora")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

model = model.to("cuda")
model.eval()
inputs = tokenizer("Preheat the oven to 350 degrees and place the cookie dough", return_tensors="pt")

outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=50)
print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])

"Preheat the oven to 350 degrees and place the cookie dough in the center of the oven. In a large bowl, combine the flour, baking powder, baking soda, salt, and cinnamon. In a separate bowl, combine the egg yolks, sugar, and vanilla."
```
å¯¹äºå…¶ä»–ä¸è¢«AutoPeftModelForç±»æ˜ç¡®æ”¯æŒçš„ä»»åŠ¡ï¼Œæ¯”å¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œä½ ä»ç„¶å¯ä»¥ä½¿ç”¨åŸºæœ¬çš„AutoPeftModelç±»åŠ è½½é€‚ç”¨äºè¯¥ä»»åŠ¡çš„æ¨¡å‹ã€‚

```python
from peft import AutoPeftModel

model = AutoPeftModel.from_pretrained("smangrul/openai-whisper-large-v2-LORA-colab")
```

## ä¸‹ä¸€æ­¥

ç°åœ¨ä½ å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨PEFTæ–¹æ³•ä¹‹ä¸€è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬é¼“åŠ±ä½ å°è¯•ä¸€äº›å…¶ä»–æ–¹æ³•ï¼Œæ¯”å¦‚æç¤ºå¾®è°ƒã€‚æ­¥éª¤ä¸å¿«é€Ÿå…¥é—¨ä¸­å±•ç¤ºçš„æ­¥éª¤éå¸¸ç›¸ä¼¼ï¼š

- 1ã€ä¸ºPEFTæ–¹æ³•å‡†å¤‡ä¸€ä¸ªPeftConfig
ä½¿ç”¨get_peft_model()æ–¹æ³•æ ¹æ®é…ç½®å’ŒåŸºç¡€æ¨¡å‹åˆ›å»ºä¸€ä¸ªPeftModel
- 2ã€ç„¶åä½ å¯ä»¥æŒ‰ç…§è‡ªå·±çš„å–œå¥½è¿›è¡Œè®­ç»ƒï¼è¦åŠ è½½ç”¨äºæ¨æ–­çš„PEFTæ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨AutoPeftModelç±»ã€‚

å¦‚æœä½ æœ‰å…´è¶£ä½¿ç”¨å…¶ä»–PEFTæ–¹æ³•é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æŸ¥çœ‹ä»»åŠ¡æŒ‡å—ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²ã€å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€DreamBoothã€æ ‡è®°åˆ†ç±»ç­‰ã€‚