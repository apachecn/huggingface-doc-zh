# å¿«é€Ÿæ¸¸è§ˆ

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/quicktour>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/quicktour>


ğŸ¤— PEFT åŒ…å«ç”¨äºè®­ç»ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚ä¼ ç»Ÿçš„èŒƒä¾‹æ˜¯é’ˆå¯¹æ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œä½†ç”±äºå½“ä»Šæ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡å·¨å¤§ï¼Œè¿™å˜å¾—éå¸¸æ˜‚è´µä¸”ä¸åˆ‡å®é™…ã€‚ç›¸åï¼Œè®­ç»ƒè¾ƒå°‘æ•°é‡çš„æç¤ºå‚æ•°æˆ–ä½¿ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰ç­‰é‡æ–°å‚æ•°åŒ–æ–¹æ³•æ¥å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ä¼šæ›´æœ‰æ•ˆã€‚


æœ¬å¿«é€Ÿæµè§ˆå°†å‘æ‚¨å±•ç¤º ğŸ¤— PEFT çš„ä¸»è¦åŠŸèƒ½ï¼Œå¹¶å¸®åŠ©æ‚¨è®­ç»ƒé€šå¸¸åœ¨æ¶ˆè´¹è®¾å¤‡ä¸Šæ— æ³•è®¿é—®çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ã€‚æ‚¨å°†çœ‹åˆ°å¦‚ä½•è®­ç»ƒ 1.2B å‚æ•°
 [`bigscience/mt0-large`](https://huggingface.co/bigscience/mt0-large)
 ä½¿ç”¨ LoRA è¿›è¡Œæ¨¡å‹ç”Ÿæˆåˆ†ç±»æ ‡ç­¾å¹¶å°†å…¶ç”¨äºæ¨ç†ã€‚


## ä½©å¤«ç‰¹é…ç½®



æ¯ä¸ª ğŸ¤— PEFT æ–¹æ³•éƒ½ç”±ä¸€ä¸ªå®šä¹‰
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 å­˜å‚¨ç”¨äºæ„å»ºçš„æ‰€æœ‰é‡è¦å‚æ•°çš„ç±»
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚


å› ä¸ºæ‚¨è¦ä½¿ç”¨ LoRAï¼Œæ‰€ä»¥æ‚¨éœ€è¦åŠ è½½å¹¶åˆ›å»ºä¸€ä¸ª
 [LoraConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraConfig)
 ç­çº§ã€‚ä¹‹å†…
 `LoraConfig`
 ï¼ŒæŒ‡å®šä»¥ä¸‹å‚æ•°ï¼š


* è¿™
 `ä»»åŠ¡ç±»å‹`
 ï¼Œæˆ–æœ¬ä¾‹ä¸­çš„åºåˆ—åˆ°åºåˆ—è¯­è¨€å»ºæ¨¡
* `æ¨ç†æ¨¡å¼`
 ï¼Œæ— è®ºæ‚¨æ˜¯å¦ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
* `r`
 ï¼Œä½ç§©çŸ©é˜µçš„ç»´æ•°
* `lora_alpha`
 ï¼Œä½ç§©çŸ©é˜µçš„ç¼©æ”¾å› å­
* `lora_dropout`
 ï¼ŒLoRA å±‚çš„ä¸¢å¤±æ¦‚ç‡



```
from peft import LoraConfig, TaskType

peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)
```


ğŸ’¡å‚è§
 [LoraConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraConfig)
 æœ‰å…³æ‚¨å¯ä»¥è°ƒæ•´çš„å…¶ä»–å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒã€‚


## ä½©å¤«ç‰¹æ¨¡å‹



A
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 æ˜¯ç”±
 `get_peft_model()`
 åŠŸèƒ½ã€‚å®ƒéœ€è¦ä¸€ä¸ªåŸºæœ¬æ¨¡å‹ - æ‚¨å¯ä»¥ä» ğŸ¤— Transformers åº“åŠ è½½ - ä»¥åŠ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 åŒ…å«å¦‚ä½•ä¸ºç‰¹å®š ğŸ¤— PEFT æ–¹æ³•é…ç½®æ¨¡å‹çš„è¯´æ˜ã€‚


é¦–å…ˆåŠ è½½æ‚¨æƒ³è¦å¾®è°ƒçš„åŸºæœ¬æ¨¡å‹ã€‚



```
from transformers import AutoModelForSeq2SeqLM

model_name_or_path = "bigscience/mt0-large"
tokenizer_name_or_path = "bigscience/mt0-large"
model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
```


åŒ…è£¹ä½ çš„åŸºç¡€æ¨¡å‹å¹¶
 `peft_config`
 ä¸
 `get_peft_model`
 å‡½æ•°æ¥åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚è¦äº†è§£æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œè¯·ä½¿ç”¨
 `æ‰“å°å¯è®­ç»ƒå‚æ•°`
 æ–¹æ³•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨åªè®­ç»ƒäº†æ¨¡å‹å‚æ•°çš„ 0.19%ï¼ ğŸ¤



```
from peft import get_peft_model

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
"output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"
```


å°±æ˜¯è¿™æ ·ğŸ‰ï¼ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ ğŸ¤— Transformers è®­ç»ƒæ¨¡å‹
 [åŸ¹è®­å¸ˆ](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/trainer#transformers.Trainer)
 ã€ğŸ¤— åŠ é€Ÿæˆ–ä»»ä½•è‡ªå®šä¹‰ PyTorch è®­ç»ƒå¾ªç¯ã€‚


## ä¿å­˜å¹¶åŠ è½½æ¨¡å‹



æ¨¡å‹å®Œæˆè®­ç»ƒåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ¨¡å‹ä¿å­˜åˆ°ç›®å½•ä¸­
 [ä¿å­˜\_pretrained](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
 åŠŸèƒ½ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ¨¡å‹ä¿å­˜åˆ° Hubï¼ˆè¯·ç¡®ä¿å…ˆç™»å½•æ‚¨çš„ Hugging Face å¸æˆ·ï¼‰ï¼š
 [push\_to\_hub](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
 åŠŸèƒ½ã€‚



```
model.save_pretrained("output\_dir")

# if pushing to Hub
from huggingface_hub import notebook_login

notebook_login()
model.push_to_hub("my\_awesome\_peft\_model")
```


è¿™åªä¼šä¿å­˜è®­ç»ƒåçš„å¢é‡ ğŸ¤— PEFT æƒé‡ï¼Œè¿™æ„å‘³ç€å­˜å‚¨ã€ä¼ è¾“å’ŒåŠ è½½éå¸¸é«˜æ•ˆã€‚ä¾‹å¦‚ï¼Œè¿™ä¸ª
 [`bigscience/T0_3B`](https://huggingface.co/smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM)
 ä½¿ç”¨ LoRA è®­ç»ƒçš„æ¨¡å‹
 [`twitter_complaints`](https://huggingface.co/datasets/ought/raft/viewer/twitter_complaints/train)
 RAFT çš„å­é›†
 [æ•°æ®é›†](https://huggingface.co/datasets/ought/raft)
 ä»…åŒ…å«ä¸¤ä¸ªæ–‡ä»¶ï¼š
 `adapter_config.json`
 å’Œ
 `adapter_model.bin`
 ã€‚åä¸€ä¸ªæ–‡ä»¶åªæœ‰ 19MBï¼


ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è½»æ¾åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†
 [æ¥è‡ª\_pretrained](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
 åŠŸèƒ½ï¼š



```
  from transformers import AutoModelForSeq2SeqLM
+ from peft import PeftModel, PeftConfig

+ peft\_model\_id = "smangrul/twitter\_complaints\_bigscience\_T0\_3B\_LORA\_SEQ\_2\_SEQ\_LM"
+ config = PeftConfig.from\_pretrained(peft\_model\_id)
  model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
+ model = PeftModel.from\_pretrained(model, peft\_model\_id)
  tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

  model = model.to(device)
  model.eval()
  inputs = tokenizer("Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :", return_tensors="pt")

  with torch.no_grad():
      outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=10)
      print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])
  'complaint'
```


## è½»æ¾åŠ è½½è‡ªåŠ¨ç±»



å¦‚æœæ‚¨å·²åœ¨æœ¬åœ°æˆ–é›†çº¿å™¨ä¸Šä¿å­˜é€‚é…å™¨ï¼Œåˆ™å¯ä»¥åˆ©ç”¨
 `AutoPeftModelForxxx`
 ç±»å¹¶ä½¿ç”¨ä¸€è¡Œä»£ç åŠ è½½ä»»ä½• PEFT æ¨¡å‹ï¼š



```
- from peft import PeftConfig, PeftModel
- from transformers import AutoModelForCausalLM
+ from peft import AutoPeftModelForCausalLM

- peft\_config = PeftConfig.from\_pretrained("ybelkada/opt-350m-lora") 
- base\_model\_path = peft\_config.base\_model\_name\_or\_path
- transformers\_model = AutoModelForCausalLM.from\_pretrained(base\_model\_path)
- peft\_model = PeftModel.from\_pretrained(transformers\_model, peft\_config)
+ peft\_model = AutoPeftModelForCausalLM.from\_pretrained("ybelkada/opt-350m-lora")
```


ç›®å‰ï¼Œæ”¯æŒçš„æ±½è½¦ç±»åˆ«æœ‰ï¼š
 `CausalLM çš„ AutoPeft æ¨¡å‹`
 ,
 `AutoPeftModelForSequenceClassification`
 ,
 `AutoPeftModelForSeq2SeqLM`
 ,
 `AutoPeftModelForTokenClassification`
 ,
 `AutoPeftModelForQuestionAnswering`
 å’Œ
 `ç”¨äºç‰¹å¾æå–çš„ AutoPeft æ¨¡å‹`
 ã€‚å¯¹äºå…¶ä»–ä»»åŠ¡ï¼ˆä¾‹å¦‚ Whisperã€StableDiffusionï¼‰ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åŠ è½½æ¨¡å‹ï¼š



```
- from peft import PeftModel, PeftConfig, AutoPeftModel
+ from peft import AutoPeftModel
- from transformers import WhisperForConditionalGeneration

- model\_id = "smangrul/openai-whisper-large-v2-LORA-colab"

peft_model_id = "smangrul/openai-whisper-large-v2-LORA-colab"
- peft\_config = PeftConfig.from\_pretrained(peft\_model\_id)
- model = WhisperForConditionalGeneration.from\_pretrained(
- peft\_config.base\_model\_name\_or\_path, load\_in\_8bit=True, device\_map="auto"
- )
- model = PeftModel.from\_pretrained(model, peft\_model\_id)
+ model = AutoPeftModel.from\_pretrained(peft\_model\_id)
```


## ä¸‹ä¸€æ­¥



ç°åœ¨æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨å…¶ä¸­ä¸€ç§ ğŸ¤— PEFT æ–¹æ³•è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨å°è¯•å…¶ä»–ä¸€äº›æ–¹æ³•ï¼Œä¾‹å¦‚æç¤ºè°ƒæ•´ã€‚è¿™äº›æ­¥éª¤ä¸æœ¬å¿«é€Ÿå…¥é—¨ä¸­æ˜¾ç¤ºçš„æ­¥éª¤éå¸¸ç›¸ä¼¼ï¼›å‡†å¤‡ä¸€ä¸ª
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 å¯¹äº ğŸ¤— PEFT æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨
 `get_peft_model`
 åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ä»é…ç½®å’ŒåŸºæœ¬æ¨¡å‹ã€‚ç„¶åä½ å°±å¯ä»¥éšå¿ƒæ‰€æ¬²åœ°è®­ç»ƒå®ƒäº†ï¼


å¦‚æœæ‚¨æœ‰å…´è¶£ä½¿ç”¨ ğŸ¤— PEFT æ–¹æ³•è®­ç»ƒæ¨¡å‹ä»¥æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼ˆä¾‹å¦‚è¯­ä¹‰åˆ†å‰²ã€å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€DreamBooth å’Œæ ‡è®°åˆ†ç±»ï¼‰ï¼Œè¯·éšæ„æŸ¥çœ‹ä»»åŠ¡æŒ‡å—ã€‚