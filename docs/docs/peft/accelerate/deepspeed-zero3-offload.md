# æ·±é€Ÿ

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/accelerate/deepspeed-zero3-offload>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload>


[DeepSpeed](https://www.deepspeed.ai/)
 æ˜¯ä¸€ä¸ªä¸“ä¸ºå…·æœ‰æ•°åäº¿å‚æ•°çš„å¤§å‹æ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒçš„é€Ÿåº¦å’Œè§„æ¨¡è€Œè®¾è®¡çš„åº“ã€‚å…¶æ ¸å¿ƒæ˜¯é›¶å†—ä½™ä¼˜åŒ–å™¨ (ZeRO)ï¼Œå®ƒå°†ä¼˜åŒ–å™¨çŠ¶æ€ (ZeRO-1)ã€æ¢¯åº¦ (ZeRO-2) å’Œå‚æ•° (ZeRO-3) è·¨æ•°æ®å¹¶è¡Œè¿›ç¨‹è¿›è¡Œåˆ†ç‰‡ã€‚è¿™å¤§å¤§å‡å°‘äº†å†…å­˜ä½¿ç”¨é‡ï¼Œä½¿æ‚¨èƒ½å¤Ÿå°†è®­ç»ƒæ‰©å±•åˆ°åäº¿ä¸ªå‚æ•°æ¨¡å‹ã€‚ä¸ºäº†é‡Šæ”¾æ›´é«˜çš„å†…å­˜æ•ˆç‡ï¼ŒZeRO-Offload åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ©ç”¨ CPU èµ„æºæ¥å‡å°‘ GPU è®¡ç®—å’Œå†…å­˜ã€‚


ğŸ¤— Accelerate æ”¯æŒè¿™ä¸¤ä¸ªåŠŸèƒ½ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬ä¸ ğŸ¤— PEFT ä¸€èµ·ä½¿ç”¨ã€‚æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„ DeepSpeed
 [è®­ç»ƒè„šæœ¬](https://github.com/huggingface/peft/blob/main/examples/conditional_ Generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py)
 ã€‚æ‚¨å°†é…ç½®è„šæœ¬æ¥è®­ç»ƒå¤§å‹æ¨¡å‹ï¼Œä»¥ä½¿ç”¨ ZeRO-3 å’Œ ZeRO-Offload è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚


ğŸ’¡ ä¸ºäº†å¸®åŠ©æ‚¨å…¥é—¨ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ç¤ºä¾‹åŸ¹è®­è„šæœ¬
 [å› æœè¯­è¨€å»ºæ¨¡](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_ds_zero3_offload.py)
 å’Œ
 [æ¡ä»¶ç”Ÿæˆ](https://github.com/huggingface/peft/blob/main/examples/conditional_ Generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py)
 ã€‚æ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„åº”ç”¨ç¨‹åºè°ƒæ•´è¿™äº›è„šæœ¬ï¼Œå¦‚æœæ‚¨çš„ä»»åŠ¡ä¸è„šæœ¬ä¸­çš„ä»»åŠ¡ç±»ä¼¼ï¼Œç”šè‡³å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒä»¬ã€‚


## é…ç½®



é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤
 [åˆ›å»º DeepSpeed é…ç½®æ–‡ä»¶](https://huggingface.co/docs/accelerate/quicktour#launching-your-distributed-script)
 ä¸ ğŸ¤— åŠ é€Ÿã€‚è¿™
 `--config_file`
 flag å…è®¸æ‚¨å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç‰¹å®šä½ç½®ï¼Œå¦åˆ™å°†ä¿å­˜ä¸º
 `default_config.yaml`
 ğŸ¤— åŠ é€Ÿç¼“å­˜ä¸­çš„æ–‡ä»¶ã€‚


é…ç½®æ–‡ä»¶ç”¨äºåœ¨å¯åŠ¨è®­ç»ƒè„šæœ¬æ—¶è®¾ç½®é»˜è®¤é€‰é¡¹ã€‚



```
accelerate config --config_file ds_zero3_cpu.yaml
```


ç³»ç»Ÿä¼šè¯¢é—®æ‚¨ä¸€äº›æœ‰å…³æ‚¨çš„è®¾ç½®çš„é—®é¢˜ï¼Œå¹¶é…ç½®ä»¥ä¸‹å‚æ•°ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ ZeRO-3 å’Œ ZeRO-Offloadï¼Œå› æ­¤è¯·ç¡®ä¿é€‰æ‹©è¿™äº›é€‰é¡¹ã€‚



```
`zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning
`gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them.
`gradient_clipping`: Enable gradient clipping with value.
`offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2.
`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3.
`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3.
`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3.
`mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training. 
```


ä¸€ä¸ªä¾‹å­
 [é…ç½®æ–‡ä»¶](https://github.com/huggingface/peft/blob/main/examples/conditional_ Generation/accelerate_ds_zero3_cpu_offload_config.yaml)
 å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºã€‚æœ€éœ€è¦æ³¨æ„çš„æ˜¯
 `é›¶é˜¶æ®µ`
 è¢«è®¾å®šä¸º
 `3`
 ï¼Œ å’Œ
 `offload_optimizer_device`
 å’Œ
 `offload_param_device`
 è¢«è®¾ç½®ä¸º
 `CPU`
 ã€‚



```
compute\_environment: LOCAL\_MACHINE
deepspeed\_config:
  gradient\_accumulation\_steps: 1
  gradient\_clipping: 1.0
  offload\_optimizer\_device: cpu
  offload\_param\_device: cpu
  zero3\_init\_flag: true
  zero3\_save\_16bit\_model: true
  zero\_stage: 3
distributed\_type: DEEPSPEED
downcast\_bf16: 'no'
dynamo\_backend: 'NO'
fsdp\_config: {}
machine\_rank: 0
main\_training\_function: main
megatron\_lm\_config: {}
mixed\_precision: 'no'
num\_machines: 1
num\_processes: 1
rdzv\_backend: static
same\_network: true
use\_cpu: false
```


## é‡è¦éƒ¨åˆ†



è®©æˆ‘ä»¬æ›´æ·±å…¥åœ°ç ”ç©¶ä¸€ä¸‹è„šæœ¬ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥çœ‹åˆ°å‘ç”Ÿäº†ä»€ä¹ˆï¼Œå¹¶äº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚


å†…
 [`main`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_ Generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py#L103)
 å‡½æ•°ï¼Œè¯¥è„šæœ¬åˆ›å»ºä¸€ä¸ª
 `åŠ é€Ÿå™¨`
 ç±»æ¥åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒçš„æ‰€æœ‰å¿…è¦è¦æ±‚ã€‚


ğŸ’¡ éšæ„æ›´æ”¹é‡Œé¢çš„æ¨¡å‹å’Œæ•°æ®é›†
 `ä¸»è¦`
 åŠŸèƒ½ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†æ ¼å¼ä¸è„šæœ¬ä¸­çš„æ ¼å¼ä¸åŒï¼Œæ‚¨å¯èƒ½è¿˜éœ€è¦ç¼–å†™è‡ªå·±çš„é¢„å¤„ç†å‡½æ•°ã€‚


è¯¥è„šæœ¬è¿˜ä¸ºæ‚¨æ­£åœ¨ä½¿ç”¨çš„ ğŸ¤— PEFT æ–¹æ³•åˆ›å»ºé…ç½®ï¼Œåœ¨æœ¬ä¾‹ä¸­ä¸º LoRAã€‚è¿™
 [LoraConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraConfig)
 æŒ‡å®šä»»åŠ¡ç±»å‹å’Œé‡è¦å‚æ•°ï¼Œä¾‹å¦‚ä½ç§©çŸ©é˜µçš„ç»´åº¦ã€çŸ©é˜µç¼©æ”¾å› å­å’Œ LoRA å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸åŒçš„ ğŸ¤— PEFT æ–¹æ³•ï¼Œè¯·ç¡®ä¿æ›¿æ¢
 `LoraConfig`
 ä¸é€‚å½“çš„
 [ç±»](../package_reference/tuners)
 ã€‚



```
 def main():
+ accelerator = Accelerator()
     model_name_or_path = "facebook/bart-large"
     dataset_name = "twitter_complaints"
+ peft\_config = LoraConfig(
         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
     )
```


åœ¨æ•´ä¸ªè„šæœ¬ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°
 `main_process_first`
 å’Œ
 `ç­‰å¾…å¤§å®¶`
 å¸®åŠ©æ§åˆ¶å’ŒåŒæ­¥è¿›ç¨‹æ‰§è¡Œçš„å‡½æ•°ã€‚


è¿™
 `get_peft_model()`
 å‡½æ•°é‡‡ç”¨ä¸€ä¸ªåŸºæœ¬æ¨¡å‹ï¼Œå¹¶ä¸”
 `peft_config`
 æ‚¨ä¹‹å‰å‡†å¤‡åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ï¼š



```
  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+ model = get\_peft\_model(model, peft\_config)
```


å°†æ‰€æœ‰ç›¸å…³è®­ç»ƒå¯¹è±¡ä¼ é€’ç»™ğŸ¤— Accelerate
 `å‡†å¤‡`
 è¿™ç¡®ä¿ä¸€åˆ‡éƒ½å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼š



```
model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(
    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler
)
```


ä¸‹ä¸€æ®µä»£ç æ£€æŸ¥æ˜¯å¦åœ¨ä¸­ä½¿ç”¨äº† DeepSpeed æ’ä»¶
 `åŠ é€Ÿå™¨`
 ï¼Œå¦‚æœæ’ä»¶å­˜åœ¨ï¼Œé‚£ä¹ˆ
 `åŠ é€Ÿå™¨`
 ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„ ZeRO-3ï¼š



```
is_ds_zero_3 = False
if getattr(accelerator.state, "deepspeed\_plugin", None):
    is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3
```


åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œé€šå¸¸
 `loss.backward()`
 è¢« ğŸ¤— Accelerate å–ä»£
 â€˜è½åâ€™
 å®ƒä½¿ç”¨æ­£ç¡®çš„
 `å‘å()`
 æ ¹æ®æ‚¨çš„é…ç½®çš„æ–¹æ³•ï¼š



```
  for epoch in range(num_epochs):
      with TorchTracemalloc() as tracemalloc:
          model.train()
          total_loss = 0
          for step, batch in enumerate(tqdm(train_dataloader)):
              outputs = model(**batch)
              loss = outputs.loss
              total_loss += loss.detach().float()
+ accelerator.backward(loss)
              optimizer.step()
              lr_scheduler.step()
              optimizer.zero_grad()
```


å°±è¿™äº›ï¼è„šæœ¬çš„å…¶ä½™éƒ¨åˆ†å¤„ç†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ï¼Œç”šè‡³å°†å…¶æ¨é€åˆ°ä¸­å¿ƒã€‚


## ç«è½¦



è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚ä¹‹å‰ï¼Œæ‚¨å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°
 `ds_zero3_cpu.yaml`
 ï¼Œå› æ­¤æ‚¨éœ€è¦ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†è·¯å¾„ä¼ é€’ç»™å¯åŠ¨å™¨
 `--config_file`
 åƒè¿™æ ·çš„è®ºç‚¹ï¼š



```
accelerate launch --config_file ds_zero3_cpu.yaml examples/peft_lora_seq2seq_accelerate_ds_zero3_offload.py
```


æ‚¨å°†çœ‹åˆ°ä¸€äº›è·Ÿè¸ªè®­ç»ƒæœŸé—´å†…å­˜ä½¿ç”¨æƒ…å†µçš„è¾“å‡ºæ—¥å¿—ï¼Œä¸€æ—¦å®Œæˆï¼Œè„šæœ¬å°±ä¼šè¿”å›å‡†ç¡®æ€§å¹¶å°†é¢„æµ‹ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼š



```
GPU Memory before entering the train : 1916
GPU Memory consumed at the end of the train (end-begin): 66
GPU Peak Memory consumed during the train (max-begin): 7488
GPU Total Peak Memory consumed during the train (max): 9404
CPU Memory before entering the train : 19411
CPU Memory consumed at the end of the train (end-begin): 0
CPU Peak Memory consumed during the train (max-begin): 0
CPU Total Peak Memory consumed during the train (max): 19411
epoch=4: train_ppl=tensor(1.0705, device='cuda:0') train_epoch_loss=tensor(0.0681, device='cuda:0')
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:27<00:00,  3.92s/it]
GPU Memory before entering the eval : 1982
GPU Memory consumed at the end of the eval (end-begin): -66
GPU Peak Memory consumed during the eval (max-begin): 672
GPU Total Peak Memory consumed during the eval (max): 2654
CPU Memory before entering the eval : 19411
CPU Memory consumed at the end of the eval (end-begin): 0
CPU Peak Memory consumed during the eval (max-begin): 0
CPU Total Peak Memory consumed during the eval (max): 19411
accuracy=100.0
eval_preds[:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']
dataset['train'][label_column][:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']
```