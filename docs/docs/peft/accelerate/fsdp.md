# å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/accelerate/fsdp>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/accelerate/fsdp>


[å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ](https://pytorch.org/docs/stable/fsdp.html)
 (FSDP) æ˜¯ä¸ºé«˜è¾¾ 1T å‚æ•°çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒè€Œå¼€å‘çš„ã€‚ FSDP é€šè¿‡è·¨æ•°æ®å¹¶è¡Œè¿›ç¨‹å¯¹æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†ç‰‡æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”å®ƒè¿˜å¯ä»¥å°†åˆ†ç‰‡çš„æ¨¡å‹å‚æ•°å¸è½½åˆ° CPUã€‚ FSDP æä¾›çš„å†…å­˜æ•ˆç‡å…è®¸æ‚¨å°†è®­ç»ƒæ‰©å±•åˆ°æ›´å¤§çš„æ‰¹æ¬¡æˆ–æ¨¡å‹å¤§å°ã€‚


ç›®å‰ï¼ŒFSDP ä¸ä¼šå‡å°‘ GPU å†…å­˜ä½¿ç”¨é‡ï¼Œå¹¶ä¸”å…·æœ‰ CPU å¸è½½åŠŸèƒ½çš„ FSDP åœ¨è®­ç»ƒæœŸé—´å®é™…ä¸Šä¼šæ¶ˆè€— 1.65 å€çš„ GPU å†…å­˜ã€‚ä½ å¯ä»¥è¿½è¸ªè¿™ä¸ª PyTorch
 [é—®é¢˜](https://github.com/pytorch/pytorch/issues/91165)
 å¦‚æœ‰ä»»ä½•æ›´æ–°ã€‚


ğŸ¤— Accelerate æ”¯æŒ FSDPï¼Œæ‚¨å¯ä»¥å°†å…¶ä¸ ğŸ¤— PEFT ä¸€èµ·ä½¿ç”¨ã€‚æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„ FSDP
 [è®­ç»ƒè„šæœ¬](https://github.com/huggingface/peft/blob/main/examples/conditional_ Generation/peft_lora_seq2seq_accelerate_fsdp.py)
 ã€‚æ‚¨å°†é…ç½®è„šæœ¬æ¥è®­ç»ƒå¤§å‹æ¨¡å‹ä»¥è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚


## é…ç½®



é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤
 [åˆ›å»º FSDP é…ç½®æ–‡ä»¶](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp)
 ä¸ ğŸ¤— åŠ é€Ÿã€‚ä½¿ç”¨
 `--config_file`
 æ ‡å¿—å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç‰¹å®šä½ç½®ï¼Œå¦åˆ™ä¿å­˜ä¸º
 `default_config.yaml`
 ğŸ¤— åŠ é€Ÿç¼“å­˜ä¸­çš„æ–‡ä»¶ã€‚


é…ç½®æ–‡ä»¶ç”¨äºåœ¨å¯åŠ¨è®­ç»ƒè„šæœ¬æ—¶è®¾ç½®é»˜è®¤é€‰é¡¹ã€‚



```
accelerate config --config_file fsdp_config.yaml
```


ç³»ç»Ÿä¼šè¯¢é—®æ‚¨ä¸€äº›æœ‰å…³æ‚¨çš„è®¾ç½®çš„é—®é¢˜ï¼Œå¹¶é…ç½®ä»¥ä¸‹å‚æ•°ã€‚å¯¹äºæ­¤ç¤ºä¾‹ï¼Œè¯·ç¡®ä¿å®Œå…¨åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œâ€‹â€‹åˆ©ç”¨ CPU è¿›è¡Œå¸è½½ï¼Œå¹¶æ ¹æ® Transformer å±‚ç±»åç§°åŒ…è£…æ¨¡å‹å±‚ã€‚



```
`Sharding Strategy`: [1] FULL_SHARD (shards optimizer states, gradients and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD
`Offload Params`: Decides Whether to offload parameters and gradients to CPU
`Auto Wrap Policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3] NO_WRAP 
`Transformer Layer Class to Wrap`: When using `TRANSFORMER_BASED_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g, 
`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`...
`Min Num Params`: minimum number of parameters when using `SIZE_BASED_WRAP`
`Backward Prefetch`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH
`State Dict Type`: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT  
```


ä¾‹å¦‚ï¼Œæ‚¨çš„ FSDP é…ç½®æ–‡ä»¶å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š



```
command\_file: null
commands: null
compute\_environment: LOCAL\_MACHINE
deepspeed\_config: {}
distributed\_type: FSDP
downcast\_bf16: 'no'
dynamo\_backend: 'NO'
fsdp\_config:
  fsdp\_auto\_wrap\_policy: TRANSFORMER\_BASED\_WRAP
  fsdp\_backward\_prefetch\_policy: BACKWARD\_PRE
  fsdp\_offload\_params: true
  fsdp\_sharding\_strategy: 1
  fsdp\_state\_dict\_type: FULL\_STATE\_DICT
  fsdp\_transformer\_layer\_cls\_to\_wrap: T5Block
gpu\_ids: null
machine\_rank: 0
main\_process\_ip: null
main\_process\_port: null
main\_training\_function: main
megatron\_lm\_config: {}
mixed\_precision: 'no'
num\_machines: 1
num\_processes: 2
rdzv\_backend: static
same\_network: true
tpu\_name: null
tpu\_zone: null
use\_cpu: false
```


## é‡è¦éƒ¨åˆ†



è®©æˆ‘ä»¬æ›´æ·±å…¥åœ°ç ”ç©¶è®­ç»ƒè„šæœ¬ä»¥äº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚


è¿™
 [`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_ Generation/peft_lora_seq2seq_accelerate_fsdp.py#L14)
 å‡½æ•°é¦–å…ˆåˆå§‹åŒ–ä¸€ä¸ª
 `åŠ é€Ÿå™¨`
 ç±»å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒçš„æ‰€æœ‰å†…å®¹ï¼Œä¾‹å¦‚è‡ªåŠ¨æ£€æµ‹æ‚¨çš„è®­ç»ƒç¯å¢ƒã€‚


ğŸ’¡ éšæ„æ›´æ”¹é‡Œé¢çš„æ¨¡å‹å’Œæ•°æ®é›†
 `ä¸»è¦`
 åŠŸèƒ½ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†æ ¼å¼ä¸è„šæœ¬ä¸­çš„æ ¼å¼ä¸åŒï¼Œæ‚¨å¯èƒ½è¿˜éœ€è¦ç¼–å†™è‡ªå·±çš„é¢„å¤„ç†å‡½æ•°ã€‚


è¯¥è„šæœ¬è¿˜ä¼šåˆ›å»ºä¸æ‚¨æ­£åœ¨ä½¿ç”¨çš„ ğŸ¤— PEFT æ–¹æ³•ç›¸å¯¹åº”çš„é…ç½®ã€‚å¯¹äº LoRAï¼Œæ‚¨å°†ä½¿ç”¨
 [LoraConfig](/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraConfig)
 æŒ‡å®šä»»åŠ¡ç±»å‹ä»¥åŠå…¶ä»–å‡ ä¸ªé‡è¦å‚æ•°ï¼Œä¾‹å¦‚ä½ç§©çŸ©é˜µçš„ç»´åº¦ã€çŸ©é˜µç¼©æ”¾å› å­å’Œ LoRA å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸åŒçš„ ğŸ¤— PEFT æ–¹æ³•ï¼Œè¯·æ›¿æ¢
 `LoraConfig`
 ä¸é€‚å½“çš„
 [ç±»](../package_reference/tuners)
 ã€‚


æ¥ä¸‹æ¥ï¼Œè„šæœ¬åŒ…è£…åŸºæœ¬æ¨¡å‹å¹¶
 `peft_config`
 ä¸
 `get_peft_model()`
 å‡½æ•°æ¥åˆ›å»ºä¸€ä¸ª
 [PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 ã€‚



```
 def main():
+ accelerator = Accelerator()
     model_name_or_path = "t5-base"
     base_path = "temp/data/FinancialPhraseBank-v1.0"
+ peft\_config = LoraConfig(
         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
     )
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+ model = get\_peft\_model(model, peft\_config)
```


åœ¨æ•´ä¸ªè„šæœ¬ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°
 `main_process_first`
 å’Œ
 `ç­‰å¾…å¤§å®¶`
 å¸®åŠ©æ§åˆ¶å’ŒåŒæ­¥è¿›ç¨‹æ‰§è¡Œçš„å‡½æ•°ã€‚


å‡†å¤‡å¥½æ•°æ®é›†å¹¶åŠ è½½æ‰€æœ‰å¿…è¦çš„è®­ç»ƒç»„ä»¶åï¼Œè„šæœ¬ä¼šæ£€æŸ¥æ‚¨æ˜¯å¦æ­£åœ¨ä½¿ç”¨
 `fsdp_æ’ä»¶`
 ã€‚ PyTorch æä¾›äº†ä¸¤ç§åœ¨ FSDP ä¸­åŒ…è£…æ¨¡å‹å±‚çš„æ–¹æ³•ï¼šè‡ªåŠ¨æˆ–æ‰‹åŠ¨ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯è®© FSDP è‡ªåŠ¨é€’å½’åŒ…è£…æ¨¡å‹å±‚ï¼Œè€Œæ— éœ€æ›´æ”¹ä»»ä½•å…¶ä»–ä»£ç ã€‚æ‚¨å¯ä»¥é€‰æ‹©æ ¹æ®å›¾å±‚åç§°æˆ–å¤§å°ï¼ˆå‚æ•°æ•°é‡ï¼‰æ¥åŒ…è£…æ¨¡å‹å›¾å±‚ã€‚åœ¨ FSDP é…ç½®æ–‡ä»¶ä¸­ï¼Œå®ƒä½¿ç”¨
 `TRANSFORMER_BASED_WRAP`
 é€‰é¡¹æ¥åŒ…è£…
 `T5å—`
 å±‚ã€‚



```
if getattr(accelerator.state, "fsdp\_plugin", None) is not None:
    accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)
```


æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ ğŸ¤— Accelerate
 `å‡†å¤‡`
 å‡½æ•°æ¥å‡†å¤‡æ¨¡å‹ã€æ•°æ®é›†ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ä»¥è¿›è¡Œè®­ç»ƒã€‚



```
model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(
    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler
)
```


ä»è¿™é‡Œå¼€å§‹ï¼Œè„šæœ¬çš„å…¶ä½™éƒ¨åˆ†å¤„ç†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°å¹¶å°†æ¨¡å‹å…±äº«åˆ°ä¸­å¿ƒã€‚


## ç«è½¦



è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚ä¹‹å‰ï¼Œæ‚¨å°†é…ç½®æ–‡ä»¶ä¿å­˜åˆ°
 `fsdp_config.yaml`
 ï¼Œå› æ­¤æ‚¨éœ€è¦ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†è·¯å¾„ä¼ é€’ç»™å¯åŠ¨å™¨
 `--config_file`
 åƒè¿™æ ·çš„è®ºç‚¹ï¼š



```
accelerate launch --config_file fsdp_config.yaml examples/peft_lora_seq2seq_accelerate_fsdp.py
```


è®­ç»ƒå®Œæˆåï¼Œè„šæœ¬ä¼šè¿”å›å‡†ç¡®æ€§å¹¶å°†é¢„æµ‹ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚