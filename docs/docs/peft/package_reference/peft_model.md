# æ¥·æ¨¡

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/package_reference/peft_model>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/package_reference/peft_model>


[PeftModel](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel)
 æ˜¯ç”¨äºæŒ‡å®šè¦åº”ç”¨ PEFT æ–¹æ³•çš„åŸºæœ¬ Transformer æ¨¡å‹å’Œé…ç½®çš„åŸºæœ¬æ¨¡å‹ç±»ã€‚åŸºåœ°
 `ä½©å¤«ç‰¹æ¨¡å‹`
 åŒ…å«ä» Hub åŠ è½½å’Œä¿å­˜æ¨¡å‹çš„æ–¹æ³•ï¼Œå¹¶æ”¯æŒ
 [PromptEncoder](/docs/peft/v0.6.2/en/package_reference/tuners#peft.PromptEncoder)
 ä»¥ä¾¿åŠæ—¶å­¦ä¹ ã€‚


## ä½©å¤«ç‰¹æ¨¡å‹




### 


ç­çº§
 

 ä½©å¤«ç‰¹ã€‚
 

 ä½©å¤«ç‰¹æ¨¡å‹


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L83)


ï¼ˆ


 æ¨¡å‹
 
 ï¼šé¢„è®­ç»ƒæ¨¡å‹


pft\_config
 
 : ä½©å¤«ç‰¹é…ç½®


é€‚é…å™¨\_åç§°
 
 : str = 'é»˜è®¤'


ï¼‰


 å‚æ•°


* **æ¨¡å‹**
 ï¼ˆ
 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel)
 ) â€” ç”¨äº Peft çš„åŸºç¡€å˜å‹å™¨æ¨¡å‹ã€‚
* **peft\_config**
 ï¼ˆ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ) â€” Peft æ¨¡å‹çš„é…ç½®ã€‚
* **é€‚é…å™¨\_åç§°**
 ï¼ˆ
 `str`
 ) â€” é€‚é…å™¨çš„åç§°ï¼Œé»˜è®¤ä¸º
 `â€œé»˜è®¤â€`
 ã€‚


 åŒ…å«å„ç§ Peft æ–¹æ³•çš„åŸºç¡€æ¨¡å‹ã€‚


**å±æ€§**
 ï¼š


* **åŸºç¡€\_æ¨¡å‹**
 ï¼ˆ
 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel)
 ) â€” ç”¨äº Peft çš„åŸºç¡€å˜å‹å™¨æ¨¡å‹ã€‚
* **peft\_config**
 ï¼ˆ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ) â€” Peft æ¨¡å‹çš„é…ç½®ã€‚
* **æ¨¡å—\_è¦\_ä¿å­˜**
 ï¼ˆ
 `åˆ—è¡¨`
 çš„
 `str`
 ) â€” æ—¶è¦ä¿å­˜çš„å­æ¨¡å—åç§°åˆ—è¡¨
ä¿å­˜æ¨¡å‹ã€‚
* **æç¤º\_ç¼–ç å™¨**
 ï¼ˆ
 [PromptEncoder](/docs/peft/v0.6.2/en/package_reference/tuners#peft.PromptEncoder)
 ) â€” ç”¨äº Peft çš„æç¤ºç¼–ç å™¨ if
ä½¿ç”¨
 [PromptLearningConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PromptLearningConfig)
 ã€‚
* **æç¤º\_tokens**
 ï¼ˆ
 `torch.å¼ é‡`
 ) â€” ç”¨äº Peft çš„è™šæ‹Ÿæç¤ºæ ‡è®°ï¼Œå¦‚æœ
ä½¿ç”¨
 [PromptLearningConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PromptLearningConfig)
 ã€‚
* **å˜å‹å™¨\_backbone\_name**
 ï¼ˆ
 `str`
 ) â€” å˜å‹å™¨çš„åç§°
åŸºæœ¬æ¨¡å‹ä¸­çš„ä¸»å¹²ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
 [PromptLearningConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PromptLearningConfig)
 ã€‚
* **è¯\_åµŒå…¥**
 ï¼ˆ
 `torch.nn.Embedding`
 ) â€” Transformer ä¸»å¹²çš„è¯åµŒå…¥
åœ¨åŸºæœ¬æ¨¡å‹ä¸­ï¼Œå¦‚æœä½¿ç”¨
 [PromptLearningConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PromptLearningConfig)
 ã€‚



#### 


åˆ›å»º\_æˆ–\_æ›´æ–°\_æ¨¡å‹\_å¡


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L697)


ï¼ˆ


 è¾“å‡º\_dir
 
 : å­—ç¬¦ä¸²


ï¼‰


æ›´æ–°æˆ–åˆ›å»ºæ¨¡å‹å¡ä»¥åŒ…å«æœ‰å…³ peft çš„ä¿¡æ¯ï¼š


1. æ–°å¢
 `ä½©å¤«ç‰¹`
 åº“æ ‡ç­¾
2.å¢åŠ peftç‰ˆæœ¬
3.æ·»åŠ åŸºç¡€å‹å·ä¿¡æ¯
4. æ·»åŠ é‡åŒ–ä¿¡æ¯ï¼ˆå¦‚æœä½¿ç”¨ï¼‰




#### 


ç¦ç”¨\_é€‚é…å™¨


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L526)


ï¼ˆ
 

 ï¼‰


ç¦ç”¨é€‚é…å™¨æ¨¡å—ã€‚




#### 


å‘å‰


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L512)


ï¼ˆ


 \*å‚æ•°
 
 ï¼š ä»»ä½•


\*\*kwargs
 
 ï¼š ä»»ä½•


ï¼‰


æ¨¡å‹çš„å‰å‘ä¼ é€’ã€‚




#### 


æ¥è‡ª\_é¢„è®­ç»ƒ


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L263)


ï¼ˆ


 æ¨¡å‹
 
 ï¼šé¢„è®­ç»ƒæ¨¡å‹


å‹å·\_id
 
 : è”åˆ[str, os.PathLike]


é€‚é…å™¨\_åç§°
 
 : str = 'é»˜è®¤'


æ˜¯\_å¯è®­ç»ƒçš„
 
 ï¼šå¸ƒå°”=å‡


é…ç½®
 
 : å¯é€‰[PeftConfig] = æ— 


\*\*kwargs
 
 ï¼š ä»»ä½•


ï¼‰


 å‚æ•°


* **æ¨¡å‹**
 ï¼ˆ
 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel)
 )â€”
è¦é€‚åº”çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹åº”ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œåˆå§‹åŒ–
 [æ¥è‡ª\_pretrained](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
 ğŸ¤— Transformers åº“ä¸­çš„æ–¹æ³•ã€‚
* **å‹å·\_id**
 ï¼ˆ
 `str`
 æˆ–è€…
 `os.PathLike`
 )â€”
è¦ä½¿ç”¨çš„ PEFT é…ç½®çš„åç§°ã€‚å¯ä»¥æ˜¯ï¼š
 
+ ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œ
`å‹å· ID`
Hugging Face ä¸Šæ¨¡å‹å­˜å‚¨åº“å†…æ‰˜ç®¡çš„ PEFT é…ç½®çš„ç¤ºä¾‹
ä¸­å¿ƒã€‚
+ åŒ…å«ä½¿ç”¨ä¿å­˜çš„ PEFT é…ç½®æ–‡ä»¶çš„ç›®å½•çš„è·¯å¾„
`ä¿å­˜é¢„è®­ç»ƒ`
æ–¹æ³• ï¼ˆ
`./my_peft_config_directory/`
ï¼‰ã€‚
* **é€‚é…å™¨\_åç§°**
 ï¼ˆ
 `str`
 ,
 *é€‰ä¿®çš„*
 ï¼Œé»˜è®¤ä¸º
 `â€œé»˜è®¤â€`
 )â€”
è¦åŠ è½½çš„é€‚é…å™¨çš„åç§°ã€‚è¿™å¯¹äºåŠ è½½å¤šä¸ªé€‚é…å™¨éå¸¸æœ‰ç”¨ã€‚
* **å¯è®­ç»ƒ**
 ï¼ˆ
 `å¸ƒå°”`
 ,
 *é€‰ä¿®çš„*
 ï¼Œé»˜è®¤ä¸º
 â€˜å‡â€™
 )â€”
é€‚é…å™¨æ˜¯å¦åº”è¯¥å¯è®­ç»ƒã€‚å¦‚æœ
 â€˜å‡â€™
 ï¼Œé€‚é…å™¨å°†è¢«å†»ç»“å¹¶ç”¨äº
æ¨ç†
* **é…ç½®**
 ï¼ˆ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ,
 *é€‰ä¿®çš„*
 )â€”
è¦ä½¿ç”¨çš„é…ç½®å¯¹è±¡ï¼Œè€Œä¸æ˜¯è‡ªåŠ¨åŠ è½½çš„é…ç½®ã€‚è¿™ä¸ªé…ç½®
å¯¹è±¡æ˜¯äº’æ–¥çš„
 `å‹å·_id`
 å’Œ
 `å¤¸æ ¼æ–¯`
 ã€‚å½“é…ç½®å·²ç»å®Œæˆæ—¶è¿™å¾ˆæœ‰ç”¨
è°ƒç”¨å‰å·²åŠ è½½
 `from_pretrained`
 ã€‚
å¤¸æ ¼æ–¯â€”â€”ï¼ˆ
 `å¯é€‰çš„`
 ï¼‰ï¼š
ä¼ é€’ç»™ç‰¹å®š PEFT é…ç½®ç±»çš„å…¶ä»–å…³é”®å­—å‚æ•°ã€‚


 ä»é¢„è®­ç»ƒæ¨¡å‹å’ŒåŠ è½½çš„ PEFT æƒé‡å®ä¾‹åŒ– PEFT æ¨¡å‹ã€‚


è¯·æ³¨æ„ï¼Œé€šè¿‡çš„
 `æ¨¡å‹`
 å¯ä»¥å°±åœ°ä¿®æ”¹ã€‚




#### 


è·å–\_base\_model


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L549)


ï¼ˆ
 

 ï¼‰


è¿”å›åŸºæœ¬æ¨¡å‹ã€‚




#### 


è·å–\_nb\_å¯è®­ç»ƒ\_å‚æ•°


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L471)


ï¼ˆ
 

 ï¼‰


è¿”å›æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å’Œæ‰€æœ‰å‚æ•°çš„æ•°é‡ã€‚




#### 


å¾—åˆ°\_æç¤º


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L425)


ï¼ˆ


 æ‰¹é‡\_å¤§å°
 
 ï¼šæ•´æ•°


ä»»åŠ¡\_ids
 
 : å¯é€‰[torch.Tensor] = æ— 


ï¼‰


è¿”å›ç”¨äº Peft çš„è™šæ‹Ÿæç¤ºã€‚ä»…é€‚ç”¨äºä»¥ä¸‹æƒ…å†µ
 `peft_config.peft_type != PeftType.LORA`
 ã€‚




#### 


è·å–\_æç¤º\_åµŒå…¥\_ä»¥\_ä¿å­˜


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L406)


ï¼ˆ


 é€‚é…å™¨\_åç§°
 
 : å­—ç¬¦ä¸²


ï¼‰


è¿”å›ä¿å­˜æ¨¡å‹æ—¶è¦ä¿å­˜çš„æç¤ºåµŒå…¥ã€‚ä»…é€‚ç”¨äºä»¥ä¸‹æƒ…å†µ
 `peft_config.peft_type != PeftType.LORA`
 ã€‚




#### 


æ‰“å°\_å¯è®­ç»ƒ\_å‚æ•°


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L495)


ï¼ˆ
 

 ï¼‰


æ‰“å°æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚




#### 


ä¿å­˜\_é¢„è®­ç»ƒ


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L157)


ï¼ˆ


 ä¿å­˜\_ç›®å½•
 
 : å­—ç¬¦ä¸²


å®‰å…¨\_åºåˆ—åŒ–
 
 ï¼šå¸ƒå°”=å‡


é€‰å®š\_é€‚é…å™¨
 
 : å¯é€‰[åˆ—è¡¨[str]] = æ— 


\*\*kwargs
 
 ï¼š ä»»ä½•


ï¼‰


 å‚æ•°


* **ä¿å­˜\_ç›®å½•**
 ï¼ˆ
 `str`
 )â€”
é€‚é…å™¨æ¨¡å‹å’Œé…ç½®æ–‡ä»¶çš„ä¿å­˜ç›®å½•ï¼ˆå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºï¼‰
å­˜åœ¨ï¼‰ã€‚
* **å®‰å…¨\_åºåˆ—åŒ–**
 ï¼ˆ
 `å¸ƒå°”`
 ,
 *é€‰ä¿®çš„*
 )â€”
æ˜¯å¦ä»¥ safetensors æ ¼å¼ä¿å­˜é€‚é…å™¨æ–‡ä»¶ã€‚
* **å¤¸æ ¼æ–¯**
 ï¼ˆé™„åŠ å…³é”®å­—å‚æ•°ï¼Œ
 *é€‰ä¿®çš„*
 )â€”
ä¼ é€’ç»™çš„é™„åŠ å…³é”®å­—å‚æ•°
 `æ¨é€åˆ°é›†çº¿å™¨`
 æ–¹æ³•ã€‚


 è¯¥å‡½æ•°å°†é€‚é…å™¨æ¨¡å‹å’Œé€‚é…å™¨é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ä¸€ä¸ªç›®å½•ä¸­ï¼Œä»¥ä¾¿äºè°ƒç”¨
ä½¿ç”¨é‡æ–°åŠ è½½
 [PeftModel.from\_pretrained()](/docs/peft/v0.6.2/en/package_reference/peft_model#peft.PeftModel.from_pretrained)
 ç±»æ–¹æ³•ï¼Œä¹Ÿè¢«
 `PefModel.push_to_hub()`
 æ–¹æ³•ã€‚




#### 


è®¾ç½®\_é€‚é…å™¨


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L678)


ï¼ˆ


 é€‚é…å™¨\_åç§°
 
 : å­—ç¬¦ä¸²


ï¼‰


è®¾ç½®æ´»åŠ¨é€‚é…å™¨ã€‚


## åºåˆ—åˆ†ç±»çš„ Peft æ¨¡å‹



A
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡ã€‚



### 


ç­çº§
 

 ä½©å¤«ç‰¹ã€‚
 

 åºåˆ—åˆ†ç±»çš„ Peft æ¨¡å‹


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L746)


ï¼ˆ


 æ¨¡å‹


 pft\_config
 
 : ä½©å¤«ç‰¹é…ç½®


é€‚é…å™¨\_åç§°
 
 ='é»˜è®¤'


ï¼‰


 å‚æ•°


* **æ¨¡å‹**
 ï¼ˆ
 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel)
 ) â€” åŸºç¡€å˜å‹å™¨æ¨¡å‹ã€‚
* **peft\_config**
 ï¼ˆ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ) â€” Peft é…ç½®ã€‚


 ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„ Pft æ¨¡å‹ã€‚


**å±æ€§**
 ï¼š


* **é…ç½®**
 ï¼ˆ
 [PretrainedConfig](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/configuration#transformers.PretrainedConfig)
 ) â€” åŸºç¡€æ¨¡å‹çš„é…ç½®å¯¹è±¡ã€‚
* **cls\_layer\_name**
 ï¼ˆ
 `str`
 ) â€” åˆ†ç±»å±‚çš„åç§°ã€‚


 ä¾‹å­ï¼š



```
>>> from transformers import AutoModelForSequenceClassification
>>> from peft import PeftModelForSequenceClassification, get_peft_config

>>> config = {
...     "peft\_type": "PREFIX\_TUNING",
...     "task\_type": "SEQ\_CLS",
...     "inference\_mode": False,
...     "num\_virtual\_tokens": 20,
...     "token\_dim": 768,
...     "num\_transformer\_submodules": 1,
...     "num\_attention\_heads": 12,
...     "num\_layers": 12,
...     "encoder\_hidden\_size": 768,
...     "prefix\_projection": False,
...     "postprocess\_past\_key\_value\_function": None,
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForSequenceClassification(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117
```


## ä»¤ç‰Œåˆ†ç±»çš„ Pft æ¨¡å‹



A
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ç”¨äºæ ‡è®°åˆ†ç±»ä»»åŠ¡ã€‚



### 


ç­çº§
 

 ä½©å¤«ç‰¹ã€‚
 

 ä»¤ç‰Œåˆ†ç±»çš„ Pft æ¨¡å‹


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L1349)


ï¼ˆ


 æ¨¡å‹


 pft\_config
 
 : PeftConfig = æ— 


é€‚é…å™¨\_åç§°
 
 ='é»˜è®¤'


ï¼‰


 å‚æ•°


* **æ¨¡å‹**
 ï¼ˆ
 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel)
 ) â€” åŸºç¡€å˜å‹å™¨æ¨¡å‹ã€‚
* **peft\_config**
 ï¼ˆ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ) â€” Peft é…ç½®ã€‚


 ç”¨äºä»¤ç‰Œåˆ†ç±»ä»»åŠ¡çš„ Pft æ¨¡å‹ã€‚


**å±æ€§**
 ï¼š


* **é…ç½®**
 ï¼ˆ
 [PretrainedConfig](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/configuration#transformers.PretrainedConfig)
 ) â€” åŸºç¡€æ¨¡å‹çš„é…ç½®å¯¹è±¡ã€‚
* **cls\_layer\_name**
 ï¼ˆ
 `str`
 ) â€” åˆ†ç±»å±‚çš„åç§°ã€‚


 ä¾‹å­ï¼š



```
>>> from transformers import AutoModelForSequenceClassification
>>> from peft import PeftModelForTokenClassification, get_peft_config

>>> config = {
...     "peft\_type": "PREFIX\_TUNING",
...     "task\_type": "TOKEN\_CLS",
...     "inference\_mode": False,
...     "num\_virtual\_tokens": 20,
...     "token\_dim": 768,
...     "num\_transformer\_submodules": 1,
...     "num\_attention\_heads": 12,
...     "num\_layers": 12,
...     "encoder\_hidden\_size": 768,
...     "prefix\_projection": False,
...     "postprocess\_past\_key\_value\_function": None,
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForTokenClassification(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117
```


## å› æœLMçš„Pefæ¨¡å‹



A
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ç”¨äºå› æœè¯­è¨€å»ºæ¨¡ã€‚



### 


ç­çº§
 

 ä½©å¤«ç‰¹ã€‚
 

 å› æœLMçš„Pefæ¨¡å‹


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L935)


ï¼ˆ


 æ¨¡å‹


 pft\_config
 
 : ä½©å¤«ç‰¹é…ç½®


é€‚é…å™¨\_åç§°
 
 ='é»˜è®¤'


ï¼‰


 å‚æ•°


* **æ¨¡å‹**
 ï¼ˆ
 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel)
 ) â€” åŸºç¡€å˜å‹å™¨æ¨¡å‹ã€‚
* **peft\_config**
 ï¼ˆ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ) â€” Peft é…ç½®ã€‚


 ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„ Peft æ¨¡å‹ã€‚


 ä¾‹å­ï¼š



```
>>> from transformers import AutoModelForCausalLM
>>> from peft import PeftModelForCausalLM, get_peft_config

>>> config = {
...     "peft\_type": "PREFIX\_TUNING",
...     "task\_type": "CAUSAL\_LM",
...     "inference\_mode": False,
...     "num\_virtual\_tokens": 20,
...     "token\_dim": 1280,
...     "num\_transformer\_submodules": 1,
...     "num\_attention\_heads": 20,
...     "num\_layers": 36,
...     "encoder\_hidden\_size": 1280,
...     "prefix\_projection": False,
...     "postprocess\_past\_key\_value\_function": None,
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForCausalLM.from_pretrained("gpt2-large")
>>> peft_model = PeftModelForCausalLM(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544
```


## PeftModelForSeq2SeqLM



A
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ç”¨äºåºåˆ—åˆ°åºåˆ—çš„è¯­è¨€å»ºæ¨¡ã€‚



### 


ç­çº§
 

 ä½©å¤«ç‰¹ã€‚
 

 PeftModelForSeq2SeqLM


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L1104)


ï¼ˆ


 æ¨¡å‹


 pft\_config
 
 : ä½©å¤«ç‰¹é…ç½®


é€‚é…å™¨\_åç§°
 
 ='é»˜è®¤'


ï¼‰


 å‚æ•°


* **æ¨¡å‹**
 ï¼ˆ
 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel)
 ) â€” åŸºç¡€å˜å‹å™¨æ¨¡å‹ã€‚
* **peft\_config**
 ï¼ˆ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ) â€” Peft é…ç½®ã€‚


 ç”¨äºåºåˆ—åˆ°åºåˆ—è¯­è¨€å»ºæ¨¡çš„ Peft æ¨¡å‹ã€‚


 ä¾‹å­ï¼š



```
>>> from transformers import AutoModelForSeq2SeqLM
>>> from peft import PeftModelForSeq2SeqLM, get_peft_config

>>> config = {
...     "peft\_type": "LORA",
...     "task\_type": "SEQ\_2\_SEQ\_LM",
...     "inference\_mode": False,
...     "r": 8,
...     "target\_modules": ["q", "v"],
...     "lora\_alpha": 32,
...     "lora\_dropout": 0.1,
...     "fan\_in\_fan\_out": False,
...     "enable\_lora": None,
...     "bias": "none",
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
>>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566
```


## Pefté—®é¢˜è§£ç­”æ¨¡å‹



A
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ç”¨äºå›ç­”é—®é¢˜ã€‚



### 


ç­çº§
 

 ä½©å¤«ç‰¹ã€‚
 

 Pefté—®é¢˜è§£ç­”æ¨¡å‹


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L1522)


ï¼ˆ


 æ¨¡å‹


 pft\_config
 
 : PeftConfig = æ— 


é€‚é…å™¨\_åç§°
 
 ='é»˜è®¤'


ï¼‰


 å‚æ•°


* **æ¨¡å‹**
 ï¼ˆ
 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel)
 ) â€” åŸºç¡€å˜å‹å™¨æ¨¡å‹ã€‚
* **peft\_config**
 ï¼ˆ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ) â€” Peft é…ç½®ã€‚


 ç”¨äºæå–å¼é—®ç­”çš„ Peft æ¨¡å‹ã€‚


**å±æ€§**
 :


* **é…ç½®**
 ï¼ˆ
 [PretrainedConfig](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/configuration#transformers.PretrainedConfig)
 ) â€” åŸºç¡€æ¨¡å‹çš„é…ç½®å¯¹è±¡ã€‚
* **cls\_layer\_name**
 ï¼ˆ
 `str`
 ) â€” åˆ†ç±»å±‚çš„åç§°ã€‚


 ä¾‹å­ï¼š



```
>>> from transformers import AutoModelForQuestionAnswering
>>> from peft import PeftModelForQuestionAnswering, get_peft_config

>>> config = {
...     "peft\_type": "LORA",
...     "task\_type": "QUESTION\_ANS",
...     "inference\_mode": False,
...     "r": 16,
...     "target\_modules": ["query", "value"],
...     "lora\_alpha": 32,
...     "lora\_dropout": 0.05,
...     "fan\_in\_fan\_out": False,
...     "bias": "none",
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForQuestionAnswering(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 592900 || all params: 108312580 || trainable%: 0.5473971721475013
```


## ç‰¹å¾æå–çš„Peftæ¨¡å‹



A
 `ä½©å¤«ç‰¹æ¨¡å‹`
 ç”¨äºä»å˜å‹å™¨æ¨¡å‹ä¸­æå–ç‰¹å¾/åµŒå…¥ã€‚



### 


ç­çº§
 

 ä½©å¤«ç‰¹ã€‚
 

 ç‰¹å¾æå–çš„Peftæ¨¡å‹


[<
 

 æ¥æº
 

 >](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/peft_model.py#L1714)


ï¼ˆ


 æ¨¡å‹


 pft\_config
 
 : PeftConfig = æ— 


é€‚é…å™¨\_åç§°
 
 ='é»˜è®¤'


ï¼‰


 å‚æ•°


* **æ¨¡å‹**
 ï¼ˆ
 [PreTrainedModel](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/model#transformers.PreTrainedModel)
 ) â€” åŸºç¡€å˜å‹å™¨æ¨¡å‹ã€‚
* **peft\_config**
 ï¼ˆ
 [PeftConfig](/docs/peft/v0.6.2/en/package_reference/config#peft.PeftConfig)
 ) â€” Peft é…ç½®ã€‚


 ç”¨äºä» Transformer æ¨¡å‹ä¸­æå–ç‰¹å¾/åµŒå…¥çš„ Peft æ¨¡å‹


**å±æ€§**
 :


* **é…ç½®**
 ï¼ˆ
 [PretrainedConfig](https://huggingface.co/docs/transformers/v4.35.0/en/main_classes/configuration#transformers.PretrainedConfig)
 ) â€” åŸºç¡€æ¨¡å‹çš„é…ç½®å¯¹è±¡ã€‚


 ä¾‹å­ï¼š



```
>>> from transformers import AutoModel
>>> from peft import PeftModelForFeatureExtraction, get_peft_config

>>> config = {
...     "peft\_type": "LORA",
...     "task\_type": "FEATURE\_EXTRACTION",
...     "inference\_mode": False,
...     "r": 16,
...     "target\_modules": ["query", "value"],
...     "lora\_alpha": 32,
...     "lora\_dropout": 0.05,
...     "fan\_in\_fan\_out": False,
...     "bias": "none",
... }
>>> peft_config = get_peft_config(config)
>>> model = AutoModel.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForFeatureExtraction(model, peft_config)
>>> peft_model.print_trainable_parameters()
```