# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹

> è¯‘è€…ï¼š[ç‰‡åˆ»å°å“¥å“¥](https://github.com/jiangzhonglian)
>
> é¡¹ç›®åœ°å€ï¼š<https://huggingface.apachecn.org/docs/peft/developer_guides/custom_models>
>
> åŸå§‹åœ°å€ï¼š<https://huggingface.co/docs/peft/developer_guides/custom_models>


ä¸€äº›å¾®è°ƒæŠ€æœ¯ï¼ˆä¾‹å¦‚æç¤ºè°ƒæ•´ï¼‰ç‰¹å®šäºè¯­è¨€æ¨¡å‹ã€‚è¿™æ„å‘³ç€åœ¨ ğŸ¤— PEFT ä¸­ï¼Œå®ƒæ˜¯
å‡è®¾æ­£åœ¨ä½¿ç”¨ ğŸ¤— Transformers æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…¶ä»–å¾®è°ƒæŠ€æœ¯ - ä¾‹å¦‚
 [LoRA](./conceptual_guides/lora)
 - ä¸é™äºç‰¹å®šå‹å·ã€‚


åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£ LoRA å¦‚ä½•åº”ç”¨äºå¤šå±‚æ„ŸçŸ¥å™¨å’Œè®¡ç®—æœºè§†è§‰æ¨¡å‹
 [è’‚å§†](https://huggingface.co/docs/timm/index)
 å›¾ä¹¦é¦†ã€‚


## å¤šå±‚æ„ŸçŸ¥å™¨



å‡è®¾æˆ‘ä»¬æƒ³è¦ä½¿ç”¨ LoRA å¾®è°ƒå¤šå±‚æ„ŸçŸ¥å™¨ã€‚è¿™æ˜¯å®šä¹‰ï¼š



```
from torch import nn


class MLP(nn.Module):
    def \_\_init\_\_(self, num\_units\_hidden=2000):
        super().__init__()
        self.seq = nn.Sequential(
            nn.Linear(20, num_units_hidden),
            nn.ReLU(),
            nn.Linear(num_units_hidden, num_units_hidden),
            nn.ReLU(),
            nn.Linear(num_units_hidden, 2),
            nn.LogSoftmax(dim=-1),
        )

    def forward(self, X):
        return self.seq(X)
```


è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œå…·æœ‰è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ã€‚


å¯¹äºè¿™ä¸ªç©å…·ç¤ºä¾‹ï¼Œæˆ‘ä»¬é€‰æ‹©äº†å¤§é‡çš„éšè—å•å…ƒæ¥çªå‡ºæ•ˆç‡å¢ç›Š
æ¥è‡ª PEFTï¼Œä½†è¿™äº›æ”¶ç›Šä¸æ›´ç°å®çš„ä¾‹å­ç›¸ç¬¦ã€‚


è¯¥æ¨¡å‹ä¸­æœ‰ä¸€äº›çº¿æ€§å±‚å¯ä»¥ä½¿ç”¨ LoRA è¿›è¡Œè°ƒæ•´ã€‚ä½¿ç”¨å¸¸è§çš„ ğŸ¤— å˜å½¢é‡‘åˆšæ—¶
æ¨¡å‹ä¸­ï¼ŒPEFT ä¼šçŸ¥é“å°† LoRA åº”ç”¨åˆ°å“ªäº›å±‚ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±æˆ‘ä»¬ä½œä¸ºç”¨æˆ·æ¥é€‰æ‹©å±‚ã€‚
è¦ç¡®å®šè¦è°ƒæ•´çš„å±‚çš„åç§°ï¼š



```
print([(n, type(m)) for n, m in MLP().named_modules()])
```


è¿™åº”è¯¥æ‰“å°ï¼š



```
[('', __main__.MLP),
 ('seq', torch.nn.modules.container.Sequential),
 ('seq.0', torch.nn.modules.linear.Linear),
 ('seq.1', torch.nn.modules.activation.ReLU),
 ('seq.2', torch.nn.modules.linear.Linear),
 ('seq.3', torch.nn.modules.activation.ReLU),
 ('seq.4', torch.nn.modules.linear.Linear),
 ('seq.5', torch.nn.modules.activation.LogSoftmax)]
```


å‡è®¾æˆ‘ä»¬è¦å°† LoRA åº”ç”¨äºè¾“å…¥å±‚å’Œéšè—å±‚ï¼Œè¿™äº›æ˜¯
 `'seq.0'`
 å’Œ
 `'seq.2'`
 ã€‚è€Œä¸”ï¼Œ
å‡è®¾æˆ‘ä»¬æƒ³è¦åœ¨æ²¡æœ‰ LoRA çš„æƒ…å†µä¸‹æ›´æ–°è¾“å‡ºå±‚ï¼Œé‚£å°±æ˜¯
 `'seq.4'`
 ã€‚ç›¸åº”çš„é…ç½®å°†
æ˜¯ï¼š



```
from peft import LoraConfig

config = LoraConfig(
    target_modules=["seq.0", "seq.2"],
    modules_to_save=["seq.4"],
)
```


è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ›å»º PEFT æ¨¡å‹å¹¶æ£€æŸ¥è®­ç»ƒå‚æ•°çš„æ¯”ä¾‹ï¼š



```
from peft import get_peft_model

model = MLP()
peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()
# prints trainable params: 56,164 || all params: 4,100,164 || trainable%: 1.369798866581922
```


æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•æˆ‘ä»¬å–œæ¬¢çš„è®­ç»ƒæ¡†æ¶ï¼Œæˆ–è€…ç¼–å†™æˆ‘ä»¬è‡ªå·±çš„æ‹Ÿåˆå¾ªç¯æ¥è®­ç»ƒ
 `peft_model`
 ã€‚


æœ‰å…³å®Œæ•´çš„ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹
 [æœ¬ç¬”è®°æœ¬](https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb)
 ã€‚


## è’‚å§†æ¨¡å‹



è¿™
 [è’‚å§†](https://huggingface.co/docs/timm/index)
 åº“åŒ…å«å¤§é‡é¢„è®­ç»ƒçš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ã€‚
è¿™äº›ä¹Ÿå¯ä»¥é€šè¿‡ PEFT è¿›è¡Œå¾®è°ƒã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™åœ¨å®è·µä¸­æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚


é¦–å…ˆï¼Œç¡®ä¿ Python ç¯å¢ƒä¸­å®‰è£…äº† timmï¼š



```
python -m pip install -U timm
```


æ¥ä¸‹æ¥æˆ‘ä»¬ä¸ºå›¾åƒåˆ†ç±»ä»»åŠ¡åŠ è½½ timm æ¨¡å‹ï¼š



```
import timm

num_classes = ...
model_id = "timm/poolformer\_m36.sail\_in1k"
model = timm.create_model(model_id, pretrained=True, num_classes=num_classes)
```


åŒæ ·ï¼Œæˆ‘ä»¬éœ€è¦å†³å®šå°† LoRA åº”ç”¨åˆ°å“ªäº›å±‚ã€‚ç”±äº LoRA æ”¯æŒ 2D è½¬æ¢å±‚ï¼Œå¹¶ä¸”ç”±äº
è¿™äº›æ˜¯è¯¥æ¨¡å‹çš„ä¸»è¦æ„å»ºå—ï¼Œæˆ‘ä»¬åº”è¯¥å°† LoRA åº”ç”¨äº 2D è½¬æ¢å±‚ã€‚æ¥è¯†åˆ«å§“å
è¿™äº›å±‚ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ‰€æœ‰å±‚çš„åç§°ï¼š



```
print([(n, type(m)) for n, m in MLP().named_modules()])
```


è¿™å°†æ‰“å°ä¸€ä¸ªå¾ˆé•¿çš„åˆ—è¡¨ï¼Œæˆ‘ä»¬åªæ˜¾ç¤ºå‰å‡ ä¸ªï¼š



```
[('', timm.models.metaformer.MetaFormer),
 ('stem', timm.models.metaformer.Stem),
 ('stem.conv', torch.nn.modules.conv.Conv2d),
 ('stem.norm', torch.nn.modules.linear.Identity),
 ('stages', torch.nn.modules.container.Sequential),
 ('stages.0', timm.models.metaformer.MetaFormerStage),
 ('stages.0.downsample', torch.nn.modules.linear.Identity),
 ('stages.0.blocks', torch.nn.modules.container.Sequential),
 ('stages.0.blocks.0', timm.models.metaformer.MetaFormerBlock),
 ('stages.0.blocks.0.norm1', timm.layers.norm.GroupNorm1),
 ('stages.0.blocks.0.token\_mixer', timm.models.metaformer.Pooling),
 ('stages.0.blocks.0.token\_mixer.pool', torch.nn.modules.pooling.AvgPool2d),
 ('stages.0.blocks.0.drop\_path1', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.layer\_scale1', timm.models.metaformer.Scale),
 ('stages.0.blocks.0.res\_scale1', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.norm2', timm.layers.norm.GroupNorm1),
 ('stages.0.blocks.0.mlp', timm.layers.mlp.Mlp),
 ('stages.0.blocks.0.mlp.fc1', torch.nn.modules.conv.Conv2d),
 ('stages.0.blocks.0.mlp.act', torch.nn.modules.activation.GELU),
 ('stages.0.blocks.0.mlp.drop1', torch.nn.modules.dropout.Dropout),
 ('stages.0.blocks.0.mlp.norm', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.mlp.fc2', torch.nn.modules.conv.Conv2d),
 ('stages.0.blocks.0.mlp.drop2', torch.nn.modules.dropout.Dropout),
 ('stages.0.blocks.0.drop\_path2', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.0.layer\_scale2', timm.models.metaformer.Scale),
 ('stages.0.blocks.0.res\_scale2', torch.nn.modules.linear.Identity),
 ('stages.0.blocks.1', timm.models.metaformer.MetaFormerBlock),
 ('stages.0.blocks.1.norm1', timm.layers.norm.GroupNorm1),
 ('stages.0.blocks.1.token\_mixer', timm.models.metaformer.Pooling),
 ('stages.0.blocks.1.token\_mixer.pool', torch.nn.modules.pooling.AvgPool2d),
 ...
 ('head.global\_pool.flatten', torch.nn.modules.linear.Identity),
 ('head.norm', timm.layers.norm.LayerNorm2d),
 ('head.flatten', torch.nn.modules.flatten.Flatten),
 ('head.drop', torch.nn.modules.linear.Identity),
 ('head.fc', torch.nn.modules.linear.Linear)]
 ]
```


ç»è¿‡ä»”ç»†æ£€æŸ¥ï¼Œæˆ‘ä»¬å‘ç° 2D å·ç§¯å±‚çš„åç§°å¦‚ä¸‹
 `â€œstages.0.blocks.0.mlp.fc1â€`
 å’Œ
 `â€œstages.0.blocks.0.mlp.fc2â€`
 ã€‚æˆ‘ä»¬å¦‚ä½•å…·ä½“åŒ¹é…è¿™äº›å›¾å±‚åç§°ï¼Ÿä½ å¯ä»¥å†™ä¸€ä¸ª
 [å¸¸è§„çš„
è¡¨è¾¾å¼](https://docs.python.org/3/library/re.html)
 ä»¥åŒ¹é…å›¾å±‚åç§°ã€‚å¯¹äºæˆ‘ä»¬çš„ä¾‹å­ï¼Œæ­£åˆ™è¡¨è¾¾å¼
 `r".*\.mlp\.fc\d"`
 åº”è¯¥åšè¿™é¡¹å·¥ä½œã€‚


æ­¤å¤–ï¼Œä¸ç¬¬ä¸€ä¸ªç¤ºä¾‹ä¸€æ ·ï¼Œæˆ‘ä»¬åº”è¯¥ç¡®ä¿è¾“å‡ºå±‚ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸ºåˆ†ç±»å¤´ï¼‰æ˜¯
ä¹Ÿæ›´æ–°äº†ã€‚æŸ¥çœ‹ä¸Šé¢æ‰“å°çš„åˆ—è¡¨çš„æœ«å°¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒçš„åç§°æ˜¯
 `'head.fc'`
 ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œ
è¿™æ˜¯æˆ‘ä»¬çš„ LoRA é…ç½®ï¼š



```
config = LoraConfig(target_modules=r".\*\.mlp\.fc\d", modules_to_save=["head.fc"])
```


ç„¶åæˆ‘ä»¬åªéœ€è¦å°†åŸºæœ¬æ¨¡å‹å’Œé…ç½®ä¼ é€’ç»™å³å¯åˆ›å»º PEFT æ¨¡å‹
 `get_peft_model`
 :



```
peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()
# prints trainable params: 1,064,454 || all params: 56,467,974 || trainable%: 1.88505789139876
```


è¿™è¡¨æ˜æˆ‘ä»¬åªéœ€è¦è®­ç»ƒä¸åˆ° 2% çš„å‚æ•°ï¼Œè¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ•ˆç‡å¢ç›Šã€‚


æœ‰å…³å®Œæ•´çš„ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹
 [æœ¬ç¬”è®°æœ¬](https://github.com/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb)
 ã€‚